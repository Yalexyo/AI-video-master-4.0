import os
import json
import logging
from pathlib import Path
from streamlit_app.config.config import get_paths_config

# Logger will be passed from the calling module (e.g., app.py)
# logger = logging.getLogger(__name__) # Avoid creating a new logger instance here

def format_ms_time(ms_value):
    """å°†æ¯«ç§’æ—¶é—´è½¬æ¢ä¸º HH:MM:SS.mmm æ ¼å¼"""
    if ms_value is None:
        return "æ—¶é—´æœªçŸ¥"
    try:
        # Ensure ms_value is an integer or float
        ms_value = float(ms_value)
    except (ValueError, TypeError):
        return "æ—¶é—´æ ¼å¼æ— æ•ˆ"

    s, ms_part = divmod(ms_value, 1000)
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)
    return f"{int(h):02d}:{int(m):02d}:{int(s):02d}.{int(ms_part):03d}"

def save_detailed_segments_metadata(all_videos_analysis_data, root_dir, logger):
    """
    ä¿å­˜æ‰€æœ‰è§†é¢‘ç‰‡æ®µçš„è¯¦ç»†å…ƒæ•°æ®åˆ° JSON æ–‡ä»¶ã€‚

    Args:
        all_videos_analysis_data (list): åŒ…å«æ‰€æœ‰è§†é¢‘åˆ†ææ•°æ®çš„åˆ—è¡¨ã€‚
                                         æ¯ä¸ªè§†é¢‘æ•°æ®å­—å…¸åº”åŒ…å« "video_id" å’Œ "semantic_segments"ã€‚
                                         "semantic_segments" æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯è¯­ä¹‰ç±»å‹ï¼Œå€¼æ˜¯ç‰‡æ®µè¯¦æƒ…åˆ—è¡¨ã€‚
                                         æ¯ä¸ªç‰‡æ®µè¯¦æƒ…å­—å…¸åº”åŒ…å« "segment_path", "asr_matched_text" (æˆ– "text"),
                                         "start_time" (ç§’), "end_time" (ç§’)ã€‚
        root_dir (str): é¡¹ç›®çš„æ ¹ç›®å½•è·¯å¾„ã€‚
        logger (logging.Logger): ç”¨äºæ—¥å¿—è®°å½•çš„Loggerå®ä¾‹ã€‚
    """
    if not all_videos_analysis_data:
        logger.info("æ²¡æœ‰åˆ†ææ•°æ®å¯ä¾›ä¿å­˜å…ƒæ•°æ®ã€‚")
        return False # è¿”å› False è¡¨ç¤ºæ²¡æœ‰æ•°æ®æˆ–æœªæ‰§è¡Œä¿å­˜

    current_run_segments_metadata = [] # å­˜å‚¨å½“å‰è¿è¡Œåˆ†æå¾—å‡ºçš„å…ƒæ•°æ®
    output_root_dir = os.path.join(root_dir, "data", "output")

    for video_data in all_videos_analysis_data:
        original_video_id = video_data.get("video_id", "N/A")
        semantic_segments = video_data.get("semantic_segments", {})
        
        # ä¿®æ­£ï¼šç¡®ä¿ä» video_data ä¸­è·å–ç›®æ ‡äººç¾¤æ—¶ä½¿ç”¨ "target_audiences" é”®
        llm_analyzed_target_audiences = video_data.get("target_audiences", []) # é»˜è®¤ä¸ºç©ºåˆ—è¡¨  
        target_audiences_str = ", ".join(llm_analyzed_target_audiences) if llm_analyzed_target_audiences else "æœªçŸ¥"

        if not semantic_segments:
            logger.debug(f"è§†é¢‘ {original_video_id} æ²¡æœ‰è¯­ä¹‰åˆ†æ®µä¿¡æ¯ï¼Œè·³è¿‡å…ƒæ•°æ®ä¿å­˜ã€‚")
            continue

        for semantic_type, segments_in_module in semantic_segments.items():
            for segment_detail in segments_in_module:
                transcript_text = segment_detail.get('asr_matched_text', segment_detail.get('text', "è½¬å½•å¾…å®š"))
                
                # start_time å’Œ end_time ä» segment_detail ä¸­è·å–çš„æ˜¯ç§’
                start_time_seconds = segment_detail.get('start_time')
                end_time_seconds = segment_detail.get('end_time')

                # è½¬æ¢ä¸ºæ¯«ç§’ä»¥ä¾› format_ms_time ä½¿ç”¨
                start_time_ms = start_time_seconds * 1000 if start_time_seconds is not None else None
                end_time_ms = end_time_seconds * 1000 if end_time_seconds is not None else None
                
                time_info_str = f"{format_ms_time(start_time_ms)} - {format_ms_time(end_time_ms)}"

                segment_full_path = segment_detail.get("segment_path")
                segment_filename = os.path.basename(segment_full_path) if segment_full_path else f"{original_video_id}_seg_UNKNOWN_{semantic_type}.mp4"
                
                if not segment_full_path:
                    logger.warning(f"ç‰‡æ®µè¯¦æƒ…ç¼ºå°‘ segment_path: {segment_detail}, ä½¿ç”¨å ä½æ–‡ä»¶å: {segment_filename}")

                current_run_segments_metadata.append({
                    "type": semantic_type,
                    "filename": segment_filename,
                    "original_video_id": original_video_id,
                    "time_info": time_info_str,
                    "transcript": transcript_text,
                    # ä¿ç•™ start_time_ms å’Œ end_time_ms ä»¥ä¾¿ SRT ç”Ÿæˆæ›´ç²¾ç¡®
                    "start_time_ms": start_time_ms,
                    "end_time_ms": end_time_ms,
                    # ç§»é™¤è§†é¢‘çº§äº§å“ç±»å‹å­—æ®µï¼Œå› ä¸ºç°åœ¨åªåœ¨ç‰‡æ®µçº§åˆ†æäº§å“ç±»å‹
                    "target_audiences": target_audiences_str # ä¿ç•™ç›®æ ‡äººç¾¤å­—æ®µï¼ˆè§†é¢‘çº§åˆ†æï¼‰
                })
    
    if not current_run_segments_metadata:
        logger.info("å½“å‰åˆ†æè¿è¡Œæ²¡æœ‰äº§ç”Ÿæœ‰æ•ˆçš„ç‰‡æ®µå…ƒæ•°æ®å¯ä¿å­˜ã€‚")
        return True # é€»è¾‘ä¸Šæ²¡æœ‰å¤±è´¥ï¼Œåªæ˜¯æ²¡æ–°ä¸œè¥¿ï¼Œä½†å¯èƒ½éœ€è¦ä¿å­˜æ—§æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

    metadata_file_path = os.path.join(output_root_dir, "video_segments_metadata.json")
    final_metadata_to_save = []
    processed_video_ids_in_current_run = {item['original_video_id'] for item in current_run_segments_metadata}

    # 1. å°è¯•åŠ è½½ç°æœ‰å…ƒæ•°æ®
    existing_metadata = []
    if os.path.exists(metadata_file_path):
        try:
            with open(metadata_file_path, 'r', encoding='utf-8') as f:
                existing_metadata = json.load(f)
            logger.info(f"å·²åŠ è½½ç°æœ‰çš„å…ƒæ•°æ®ï¼Œå…± {len(existing_metadata)} æ¡è®°å½•ã€‚")
        except json.JSONDecodeError:
            logger.warning(f"ç°æœ‰çš„å…ƒæ•°æ®æ–‡ä»¶ {metadata_file_path} æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†ä½œä¸ºç©ºæ–‡ä»¶å¤„ç†ã€‚")
        except Exception as e_load:
            logger.error(f"åŠ è½½ç°æœ‰å…ƒæ•°æ®æ–‡ä»¶ {metadata_file_path} å¤±è´¥: {e_load}", exc_info=True)
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä¸ºäº†æ•°æ®å®‰å…¨ï¼Œå¯ä»¥é€‰æ‹©ä¸ç»§ç»­ï¼Œæˆ–è€…åªä¿å­˜å½“å‰è¿è¡Œçš„æ•°æ®
            # è¿™é‡Œé€‰æ‹©ç»§ç»­ï¼Œä½†åªä½¿ç”¨å½“å‰è¿è¡Œçš„æ•°æ®ï¼Œä»¥é¿å…æŸåæ—§æ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ä½†æ— æ³•è¯»å–ï¼‰
            pass # å°† existing_metadata ä¿æŒä¸ºç©ºåˆ—è¡¨

    # 2. åˆå¹¶å…ƒæ•°æ®ï¼šä¿ç•™æœªåœ¨å½“å‰è¿è¡Œä¸­å¤„ç†çš„æ—§è§†é¢‘çš„å…ƒæ•°æ®
    if existing_metadata:
        for old_item in existing_metadata:
            if old_item.get('original_video_id') not in processed_video_ids_in_current_run:
                final_metadata_to_save.append(old_item)
        logger.info(f"ä¿ç•™äº† {len(final_metadata_to_save)} æ¡æ¥è‡ªæœªè¢«æœ¬æ¬¡åˆ†æè¦†ç›–çš„æ—§è§†é¢‘çš„å…ƒæ•°æ®ã€‚")

    # 3. æ·»åŠ /æ›¿æ¢å½“å‰è¿è¡Œåˆ†æçš„è§†é¢‘çš„å…ƒæ•°æ®
    final_metadata_to_save.extend(current_run_segments_metadata)
    logger.info(f"åˆå¹¶åï¼Œæœ€ç»ˆå…ƒæ•°æ®å…± {len(final_metadata_to_save)} æ¡è®°å½•ã€‚")

    if not final_metadata_to_save:
        logger.info("æœ€ç»ˆæ²¡æœ‰å…ƒæ•°æ®å¯ä¿å­˜ï¼ˆå¯èƒ½æ˜¯åˆå§‹è¿è¡Œä¸”æ— æ•°æ®ï¼Œæˆ–åˆå¹¶åä¸ºç©ºï¼‰ã€‚")
        # ç¡®ä¿ç©ºæ–‡ä»¶ä¹Ÿè¢«å†™å…¥ï¼Œä»¥ä¾¿ä¸‹æ¸¸çŸ¥é“æ˜¯ç©ºçš„
        # os.makedirs(output_root_dir, exist_ok=True)
        # with open(metadata_file_path, 'w', encoding='utf-8') as f:
        #     json.dump([], f, ensure_ascii=False, indent=4)
        return True # æ²¡æœ‰å¤±è´¥ï¼Œåªæ˜¯æ²¡å†…å®¹

    try:
        os.makedirs(output_root_dir, exist_ok=True) # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        with open(metadata_file_path, 'w', encoding='utf-8') as f:
            json.dump(final_metadata_to_save, f, ensure_ascii=False, indent=4)
        logger.info(f"å·²å°†åˆå¹¶åçš„è¯¦ç»†ç‰‡æ®µå…ƒæ•°æ®ä¿å­˜åˆ°: {metadata_file_path}")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜è¯¦ç»†ç‰‡æ®µå…ƒæ•°æ®å¤±è´¥: {e}", exc_info=True)
        return False

# --- æ–°å¢ SRT ç”ŸæˆåŠŸèƒ½ ---

def _time_str_to_seconds(time_str: str) -> float:
    """å°† HH:MM:SS.mmm æˆ– HH:MM:SS,mmm æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ€»ç§’æ•°"""
    if not time_str or not isinstance(time_str, str):
        return 0.0
    time_str = time_str.replace(',', '.') # å…¼å®¹é€—å·ä½œä¸ºæ¯«ç§’åˆ†éš”ç¬¦
    parts = time_str.split(':')
    try:
        if len(parts) == 3:
            h = int(parts[0])
            m = int(parts[1])
            s_ms = parts[2].split('.')
            s = int(s_ms[0])
            ms = int(s_ms[1]) if len(s_ms) > 1 else 0
            return h * 3600 + m * 60 + s + ms / 1000.0
        elif len(parts) == 2: # MM:SS.mmm
            m = int(parts[0])
            s_ms = parts[1].split('.')
            s = int(s_ms[0])
            ms = int(s_ms[1]) if len(s_ms) > 1 else 0
            return m * 60 + s + ms / 1000.0
        elif len(parts) == 1: # SS.mmm
            s_ms = parts[0].split('.')
            s = int(s_ms[0])
            ms = int(s_ms[1]) if len(s_ms) > 1 else 0
            return s + ms / 1000.0
    except ValueError:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå¯ä»¥è®°å½•ä¸€ä¸ªè­¦å‘Šæˆ–è¿”å›0
        # logger.warning(f"æ— æ³•è§£ææ—¶é—´å­—ç¬¦ä¸²: {time_str}") # logger éœ€è¦ä¼ é€’è¿›æ¥
        return 0.0
    return 0.0


def _seconds_to_srt_time_format(total_seconds: float) -> str:
    """å°†æ€»ç§’æ•°è½¬æ¢ä¸º SRT çš„ HH:MM:SS,mmm æ ¼å¼"""
    if total_seconds < 0:
        total_seconds = 0
    hours = int(total_seconds // 3600)
    minutes = int((total_seconds % 3600) // 60)
    seconds = int(total_seconds % 60)
    milliseconds = int((total_seconds - int(total_seconds)) * 1000)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

def create_srt_files_for_segments(root_dir, logger):
    """
    æ ¹æ® video_segments_metadata.json ä¸ºæ¯ä¸ªç‰‡æ®µåˆ›å»º SRT å­—å¹•æ–‡ä»¶ã€‚
    SRT æ–‡ä»¶å°†ä¿å­˜åœ¨ data/output/{è¯­ä¹‰ç±»å‹}/{segment_filename_without_ext}.srt
    
    ğŸ†• ä¿®å¤ï¼šSRTæ–‡ä»¶ç°åœ¨ä¿æŒåŸå§‹è§†é¢‘çš„æ—¶é—´åç§»ï¼Œè€Œä¸æ˜¯ä»00:00:00é‡æ–°å¼€å§‹

    Args:
        root_dir (Path or str): é¡¹ç›®çš„æ ¹ç›®å½•ã€‚
        logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹ã€‚
    """
    root_dir = Path(root_dir)
    output_dir = root_dir / "data" / "output"
    metadata_file_path = output_dir / "video_segments_metadata.json"

    if not metadata_file_path.exists():
        logger.warning(f"å…ƒæ•°æ®æ–‡ä»¶ {metadata_file_path} ä¸å­˜åœ¨ï¼Œæ— æ³•ç”Ÿæˆ SRT æ–‡ä»¶ã€‚")
        return

    try:
        with open(metadata_file_path, 'r', encoding='utf-8') as f:
            segments_metadata = json.load(f)
    except Exception as e:
        logger.error(f"åŠ è½½å…ƒæ•°æ®æ–‡ä»¶ {metadata_file_path} å¤±è´¥: {e}", exc_info=True)
        return

    if not segments_metadata:
        logger.info("å…ƒæ•°æ®ä¸ºç©ºï¼Œæ²¡æœ‰ SRT æ–‡ä»¶å¯ç”Ÿæˆã€‚")
        return

    srt_files_created_count = 0
    for segment_meta in segments_metadata:
        try:
            segment_filename = segment_meta.get("filename")
            original_video_id = segment_meta.get("original_video_id")
            time_info_str = segment_meta.get("time_info") # "HH:MM:SS.mmm - HH:MM:SS.mmm"
            transcript_text = segment_meta.get("transcript")
            semantic_type = segment_meta.get("type") # è·å–è¯­ä¹‰ç±»å‹
            start_time_ms = segment_meta.get("start_time_ms")
            end_time_ms = segment_meta.get("end_time_ms")

            if not all([segment_filename, original_video_id, transcript_text, semantic_type]):
                logger.warning(f"ç‰‡æ®µå…ƒæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡ SRT ç”Ÿæˆ: {segment_meta.get('filename', 'æœªçŸ¥æ–‡ä»¶å')}")
                continue

            # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è§†é¢‘ä¸­çš„å®é™…æ—¶é—´ï¼Œè€Œä¸æ˜¯ä»0å¼€å§‹
            if start_time_ms is not None and end_time_ms is not None:
                # ä½¿ç”¨æ¯«ç§’æ—¶é—´æˆ³
                original_start_seconds = float(start_time_ms) / 1000.0
                original_end_seconds = float(end_time_ms) / 1000.0
            elif time_info_str and " - " in time_info_str:
                # ä»æ—¶é—´å­—ç¬¦ä¸²è§£æ
                time_parts = time_info_str.split(" - ")
                if len(time_parts) != 2:
                    logger.warning(f"æ—¶é—´ä¿¡æ¯æ ¼å¼ä¸æ­£ç¡® '{time_info_str}' for {segment_filename}ï¼Œè·³è¿‡ SRT ç”Ÿæˆã€‚")
                    continue
                original_start_seconds = _time_str_to_seconds(time_parts[0])
                original_end_seconds = _time_str_to_seconds(time_parts[1])
            else:
                logger.warning(f"æ— æ³•è·å–æ—¶é—´ä¿¡æ¯ for {segment_filename}ï¼Œè·³è¿‡ SRT ç”Ÿæˆã€‚")
                continue

            # ğŸ†• å…³é”®ä¿®å¤ï¼šSRTæ–‡ä»¶ä¿æŒåŸå§‹è§†é¢‘çš„æ—¶é—´åç§»
            srt_start_time = _seconds_to_srt_time_format(original_start_seconds)
            srt_end_time = _seconds_to_srt_time_format(original_end_seconds)

            # ç”ŸæˆSRTå†…å®¹ï¼Œä½¿ç”¨åŸå§‹è§†é¢‘çš„æ—¶é—´æˆ³
            srt_content = f"1\n{srt_start_time} --> {srt_end_time}\n{transcript_text}\n\n"

            # æ„å»ºSRTæ–‡ä»¶çš„ç›®æ ‡è·¯å¾„
            srt_parent_dir = output_dir / semantic_type
            srt_parent_dir.mkdir(parents=True, exist_ok=True)

            srt_filename = Path(segment_filename).stem + ".srt"
            srt_file_path = srt_parent_dir / srt_filename

            with open(srt_file_path, 'w', encoding='utf-8') as f:
                f.write(srt_content)
            
            srt_files_created_count += 1
            logger.debug(f"å·²ç”ŸæˆSRTæ–‡ä»¶: {srt_file_path} (æ—¶é—´: {srt_start_time} --> {srt_end_time})")

        except Exception as e:
            logger.error(f"ä¸ºç‰‡æ®µ {segment_meta.get('filename', 'æœªçŸ¥')} ç”Ÿæˆ SRT æ—¶å‡ºé”™: {e}", exc_info=True)
            continue
    
    if srt_files_created_count > 0:
        logger.info(f"SRT æ–‡ä»¶ç”Ÿæˆå®Œæˆã€‚å…±åˆ›å»º {srt_files_created_count} ä¸ª SRT æ–‡ä»¶ï¼Œç°åœ¨ä¿æŒåŸå§‹è§†é¢‘æ—¶é—´åç§»ã€‚")
    else:
        logger.info("æ²¡æœ‰æ–°çš„ SRT æ–‡ä»¶è¢«åˆ›å»ºï¼ˆå¯èƒ½å…ƒæ•°æ®ä¸ºç©ºæˆ–å·²å¤„ç†ï¼‰ã€‚")

def update_metadata_with_analysis_results(analyzed_segments, root_dir, logger):
    """
    æ›´æ–°å…ƒæ•°æ®æ–‡ä»¶ï¼Œæ·»åŠ ç‰‡æ®µåˆ†æç»“æœï¼ˆäº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹ï¼‰
    
    Args:
        analyzed_segments (list): åŒ…å«åˆ†æç»“æœçš„ç‰‡æ®µåˆ—è¡¨
        root_dir (str): é¡¹ç›®æ ¹ç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    if not analyzed_segments:
        logger.info("æ²¡æœ‰åˆ†æç»“æœéœ€è¦ä¿å­˜åˆ°å…ƒæ•°æ®ä¸­ã€‚")
        return False
    
    output_root_dir = os.path.join(root_dir, "data", "output")
    metadata_file_path = os.path.join(output_root_dir, "video_segments_metadata.json")
    
    # åŠ è½½ç°æœ‰å…ƒæ•°æ®
    existing_metadata = []
    if os.path.exists(metadata_file_path):
        try:
            with open(metadata_file_path, 'r', encoding='utf-8') as f:
                existing_metadata = json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç°æœ‰å…ƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    if not existing_metadata:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ç°æœ‰å…ƒæ•°æ®ï¼Œæ— æ³•æ›´æ–°åˆ†æç»“æœã€‚")
        return False
    
    # åˆ›å»ºåˆ†æç»“æœçš„æŸ¥æ‰¾å­—å…¸ï¼Œä½¿ç”¨è§†é¢‘ID+å¼€å§‹æ—¶é—´+ç»“æŸæ—¶é—´ä½œä¸ºé”®
    analysis_lookup = {}
    for segment in analyzed_segments:
        video_id = segment.get("video_id", "")
        start_time = segment.get("start_time", 0)
        end_time = segment.get("end_time", 0)
        key = f"{video_id}_{start_time}_{end_time}"
        analysis_lookup[key] = {
            "analyzed_product_type": segment.get("analyzed_product_type", ""),
            "analyzed_selling_points": segment.get("analyzed_selling_points", [])
        }
    
    # æ›´æ–°å…ƒæ•°æ®
    updated_count = 0
    for metadata_item in existing_metadata:
        # ä»æ–‡ä»¶åä¸­å°è¯•æå–è§†é¢‘IDå’Œæ—¶é—´ä¿¡æ¯
        filename = metadata_item.get("filename", "")
        original_video_id = metadata_item.get("original_video_id", "")
        
        # ä»å…ƒæ•°æ®ä¸­è·å–æ¯«ç§’æ—¶é—´å¹¶è½¬æ¢ä¸ºç§’
        start_time_ms = metadata_item.get("start_time_ms")
        end_time_ms = metadata_item.get("end_time_ms")
        
        if start_time_ms is not None and end_time_ms is not None:
            start_time_seconds = start_time_ms / 1000.0
            end_time_seconds = end_time_ms / 1000.0
            
            # æ„å»ºæŸ¥æ‰¾é”®
            lookup_key = f"{original_video_id}_{start_time_seconds}_{end_time_seconds}"
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„åˆ†æç»“æœï¼Œæ›´æ–°å…ƒæ•°æ®
            if lookup_key in analysis_lookup:
                analysis_result = analysis_lookup[lookup_key]
                metadata_item["analyzed_product_type"] = analysis_result["analyzed_product_type"]
                metadata_item["analyzed_selling_points"] = analysis_result["analyzed_selling_points"]
                updated_count += 1
    
    # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
    try:
        with open(metadata_file_path, 'w', encoding='utf-8') as f:
            json.dump(existing_metadata, f, ensure_ascii=False, indent=4)
        logger.info(f"å·²æ›´æ–° {updated_count} ä¸ªç‰‡æ®µçš„åˆ†æç»“æœåˆ°å…ƒæ•°æ®æ–‡ä»¶ã€‚")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®å¤±è´¥: {e}")
        return False

# Example usage (for testing, not to be run directly usually)
if __name__ == '__main__':
    # This is a dummy logger for standalone testing
    import logging
    logging.basicConfig(level=logging.DEBUG)
    test_logger = logging.getLogger(__name__ + "_test")
    
    # Assume project root is two levels up from streamlit_app/modules/data_process
    project_root_for_test = Path(__file__).parent.parent.parent.parent 
    
    # You might need to create a dummy video_segments_metadata.json in 
    # <project_root_for_test>/data/output/ for this test to run.
    # And ensure a config.py exists that get_paths_config can use.
    
    # Dummy metadata for testing _time_str_to_seconds and _seconds_to_srt_time_format
    # print(_time_str_to_seconds("00:01:15.345"))
    # print(_seconds_to_srt_time_format(75.345))
    # print(_time_str_to_seconds("01:15.345"))
    # print(_time_str_to_seconds("15.345"))


    # create_srt_files_for_segments(project_root_for_test, test_logger)
    pass 