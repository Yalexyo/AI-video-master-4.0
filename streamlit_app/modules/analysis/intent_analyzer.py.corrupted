import os
import json
import logging
import time
import traceback
import streamlit as st
from datetime import datetime
from typing import Dict, Any

from streamlit_app.config.config import get_config, TARGET_GROUPS, SELLING_POINTS, PRODUCT_TYPES, BRAND_KEYWORDS, get_semantic_segment_types, get_semantic_type_definitions, DEFAULT_SEMANTIC_SEGMENT_TYPES
from sentence_transformers import SentenceTransformer, util
import torch

# 设置日志
logger = logging.getLogger(__name__)

class SemanticAnalyzer:
    """语义分析器，使用DeepSeek模型进行语义理解"""
    
    def __init__(self, api_key=None, base_url=None, model="deepseek-chat"):
        """初始化语义分析器"""
        # 获取配置信息
        config = get_config()
        self.api_key = api_key or config.get("api_key") or os.environ.get("DEEPSEEK_API_KEY")
        self.base_url = base_url or "https://api.deepseek.com"
        self.model = model
        
        # 检查是否有API密钥
        if not self.api_key:
            logger.warning("缺少API密钥，使用测试模式")
            self.api_key = "sk-test-api-key-for-development-only"
        
        # 母婴奶粉领域的专业术语和同义词映射
        self.domain_terms = {
            "免疫力": ["自御力", "抵抗力", "抵抗能力", "保护力", "自身保护力", "抵御能力"],
            "过敏": ["过敏反应", "过敏现象", "敏感", "食物过敏", "蛋白过敏"],
            "便秘": ["大便干", "排便困难", "排便不畅", "肠胃不适"],
            "腹泻": ["拉肚子", "肚子不舒服", "水便", "消化不良", "消化问题"],
            "配方": ["奶粉配方", "牛奶配方", "营养配方", "特殊配方"],
            "母乳": ["母乳喂养", "纯母乳", "母奶"],
            "奶粉": ["配方奶", "牛奶粉", "婴儿奶粉", "配方奶粉"],
            "混合喂养": ["混喂", "混合喂奶", "母乳+奶粉", "混养"]
        }
        
        logger.info(f"语义分析器初始化完成，使用模型: {model}")
        
        # 导入requests库
        try:
            import requests
            self.requests = requests
        except ImportError:
            logger.error("请安装requests库: pip install requests")
            raise ImportError("缺少必要的requests库，请使用pip install requests安装")
        
        # 检查配置是否禁用了sentence-transformer
        self.similarity_model = None
        use_sentence_transformer = config.get("use_sentence_transformer", False)
        
        if not use_sentence_transformer:
            logger.info("根据配置已禁用SentenceTransformer模型，将使用difflib进行相似度计算")
            return
            
        # 初始化sentence-transformer模型（仅在配置允许的情况下）
        try:
            # 使用一个通用的多语言或中文模型，确保能处理中文
            # 'paraphrase-multilingual-MiniLM-L12-v2' 是一个不错的选择，支持多种语言包括中文
            self.similarity_model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            
            # 设置离线模式，避免下载模型导致的连接问题
            os.environ['HF_DATASETS_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # 尝试加载模型，如果失败则跳过，不阻止程序运行
            try:
                from sentence_transformers import SentenceTransformer
                # 使用本地模型路径
                local_model_path = config.get("sentence_transformer_local_path")
                if local_model_path and os.path.exists(local_model_path):
                    logger.info(f"使用本地模型路径: {local_model_path}")
                    self.similarity_model = SentenceTransformer(self.similarity_model_name, cache_folder=local_model_path, device=device)
                else:
                    logger.warning(f"本地模型路径不存在: {local_model_path}，尝试从默认位置加载")
                    self.similarity_model = SentenceTransformer(self.similarity_model_name, device=device)
                logger.info(f"文本相似度模型 {self.similarity_model_name} 初始化完成，使用设备: {device}")
            except Exception as e:
                logger.error(f"初始化文本相似度模型失败: {e}")
                logger.warning("将使用备用方法进行相似度计算")
                self.similarity_model = None
        except Exception as e:
            logger.error(f"初始化文本相似度模型时发生异常: {e}")
            self.similarity_model = None
    
    def expand_query_with_synonyms(self, query):
        """使用同义词扩展查询内容"""
        expanded_terms = []
        for term, synonyms in self.domain_terms.items():
            if term in query:
                expanded_terms.append(term)
                expanded_terms.extend(synonyms)
            else:
                for synonym in synonyms:
                    if synonym in query:
                        expanded_terms.append(term)
                        expanded_terms.extend(synonyms)
                        break
        
        return list(set(expanded_terms))  # 去重
    
    def analyze_semantic_match(self, transcript, query, context=None):
        """
        分析文本与查询的语义匹配度
        
        Args:
            transcript: 待分析的文本
            query: 用户查询内容
            context: 附加上下文信息 (目标人群、产品类型等)
            
        Returns:
            匹配分数和匹配细节
        """
        if not query:
            return 1.0, {"reason": "无查询内容，默认匹配"}
        
        # 使用同义词扩展查询
        expanded_terms = self.expand_query_with_synonyms(query)
        expanded_query_info = f"扩展关键词: {', '.join(expanded_terms)}" if expanded_terms else ""
        
        system_prompt = """你是一个母婴行业专业视频内容分析专家，特别精通婴幼儿营养、喂养和奶粉产品信息。
你的任务是深入理解视频内容与用户查询之间的语义关联，提供专业的匹配分析。

你需要考虑以下因素:
1. 内容直接相关性 - 视频是否明确讨论了查询主题
2. 语义关联性 - 视频内容是否包含与查询相关的概念、术语或同义表达
3. 解决方案匹配度 - 视频是否提供了与用户查询相关的解决方案或建议
4. 目标人群适配性 - 视频内容是否适合查询所针对的人群
5. 产品适配性 - 视频中讨论的产品是否符合查询需求

你应当理解母婴行业的专业术语和同义词,例如:
- "免疫力"相关: 自御力、抵抗力、保护力等
- "配方"相关: 奶粉配方、营养配方、特殊配方等
- "消化问题"相关: 便秘、腹泻、肠胃不适、排便等

对于品牌术语,要特别注意:
- "启赋水奶": 指启赋品牌的即饮型液态奶
- "启赋蕴淳": 指启赋品牌的特定系列奶粉
- "HMO": 指人乳低聚糖,是一种重要的母乳成分
- "A2蛋白": 指一种特定的牛奶蛋白类型
"""
        
        # 准备上下文信息
        context_str = ""
        if context:
            if context.get("target_audience"):
                context_str += f"目标人群: {context['target_audience']}\n"
            if context.get("product_type") and context.get("product_type") != "-":
                context_str += f"产品类型: {context['product_type']}\n"
            if context.get("selling_points"):
                context_str += f"产品卖点: {', '.join(context['selling_points'])}\n"
        
        user_prompt = f"""
请深入分析以下视频内容转录文本，评估其与用户查询意图的匹配程度:

用户查询: {query}
{expanded_query_info if expanded_query_info else ""}

相关上下文:
{context_str if context_str else "无特定上下文"}

视频转录文本:
{transcript}

请从多个维度评估匹配度，返回一个详细的JSON格式分析结果:
{{
  "match_score": 0.85,  // 总体匹配分数，0-1之间
  "dimension_scores": {{
    "content_relevance": 0.9,  // 内容相关性
    "semantic_coherence": 0.8,  // 语义一致性
    "solution_fit": 0.7,  // 解决方案匹配度
    "audience_fit": 0.9,  // 目标人群匹配度
    "product_fit": 0.85   // 产品匹配度
  }},
  "matching_keywords": ["关键词1", "关键词2"],  // 匹配到的关键词
  "reason": "详细说明匹配原因，包括为什么这段内容与查询相关",
  "recommendation": "对于这段内容是否推荐给用户的专业建议"
}}

只返回JSON格式的分析结果，不包含其他说明。确保分析紧密结合母婴和奶粉领域的专业理解。
"""

        try:
            # 确保API密钥已设置
            if not self.api_key:
                logger.warning("缺少API密钥，使用测试模式")
                self.api_key = "sk-test-api-key-for-development-only"
                
            # 实际调用API
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            # 构建消息
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,  # 低温度使输出更确定性
                "max_tokens": 1500
            }
            
            # 调用API
            response = self.requests.post(
                f"{self.base_url}/v1/chat/completions", 
                headers=headers, 
                json=data,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            # 解析结果
            content = result['choices'][0]['message']['content']
            logger.info("DeepSeek API调用成功")
            
            # 尝试解析JSON响应
            try:
                # 移除可能的Markdown代码块标记
                if "```json" in content and "```" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                result_json = json.loads(content)
                match_score = result_json.get("match_score", 0.0)
                
                # 提取其他有用信息
                dimension_scores = result_json.get("dimension_scores", {})
                matching_keywords = result_json.get("matching_keywords", [])
                reason = result_json.get("reason", "")
                recommendation = result_json.get("recommendation", "")
                
                details = {
                    "dimension_scores": dimension_scores,
                    "matching_keywords": matching_keywords,
                    "reason": reason,
                    "recommendation": recommendation
                }
                
                return match_score, details
                
            except json.JSONDecodeError as e:
                logger.error(f"无法解析模型响应为JSON: {content}")
                logger.error(f"错误信息: {str(e)}")
                return 0.5, {"reason": f"解析错误，使用默认分数。原始响应: {content[:100]}..."}
                
        except Exception as e:
            logger.error(f"调用DeepSeek模型时出错: {str(e)}")
            raise

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        计算两个文本之间的相似度。
        如果已初始化 SentenceTransformer 模型则使用它，否则使用 difflib 进行计算。
        
        Args:
            text1: 第一个文本
            text2: 第二个文本
            
        Returns:
            相似度分数 (0.0-1.0)
        """
        if not text1 or not text2:
            return 0.0
        
        # 如果sentence-transformer模型可用，使用它
        if self.similarity_model:
            try:
                from sentence_transformers import util
                embeddings = self.similarity_model.encode([text1, text2], convert_to_tensor=True)
                cosine_scores = util.pytorch_cos_sim(embeddings[0], embeddings[1])
                similarity = cosine_scores.item()
                logger.debug(f"使用SentenceTransformer计算文本相似度: '{text1[:50]}...' vs '{text2[:50]}...' = {similarity:.4f}")
                return similarity
            except Exception as e:
                logger.error(f"使用sentence-transformer计算文本相似度时出错: {e}")
                logger.info("回退到difflib计算相似度...")
                # 如果出错，回退到基本方法
        else:
            logger.debug("SentenceTransformer模型未初始化，使用difflib计算相似度...")
        
        # 基本相似度计算方法
        try:
            # 导入difflib库用于字符串相似度计算
            import difflib
            # 使用SequenceMatcher计算相似度
            similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
            logger.debug(f"使用difflib计算文本相似度: '{text1[:50]}...' vs '{text2[:50]}...' = {similarity:.4f}")
            return similarity
        except Exception as e:
            logger.error(f"使用difflib计算文本相似度时出错: {e}")
            return 0.0

    def _chat_completion(self, messages, model=None):
        """
        调用DeepSeek API执行聊天请求。

        Args:
            messages: 消息列表，格式为 [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
            model: 要使用的模型，如果未指定则使用初始化时设置的模型

        Returns:
            API响应的JSON对象
        """
        if not self.api_key:
            logger.warning("_chat_completion: 缺少API密钥，使用测试模式")
            self.api_key = "sk-test-api-key-for-development-only"
            
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": model or self.model,
            "messages": messages,
            "temperature": 0.1,  # 低温度使输出更确定性
            "max_tokens": 1500
        }
        
        try:
            response = self.requests.post(
                f"{self.base_url}/v1/chat/completions", 
                headers=headers, 
                json=data,
                timeout=45
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"调用DeepSeek API时出错: {str(e)}")
            # 返回一个模拟的最小响应，以便调用代码可以继续运行
            return {
                "choices": [
                    {
                        "message": {
                            "content": '{"target_audience": [], "product_type": []}'
                        }
                    }
                ]
            }

    def analyze_video_summary(self, full_transcript: str) -> Dict[str, Any]:
        """
        使用DeepSeek模型分析视频完整转录文本，提取目标人群。

        Args:
            full_transcript: 完整的视频转录文本。

        Returns:
            一个包含分析结果的字典，例如:
            {
                "target_audience": ["人群1", "人群2"]
            }
            如果分析失败则返回空字典。
        """
        if not full_transcript:
            logger.warning("完整的转录文本为空，无法进行视频摘要分析。")
            return {}

        import json # 确保导入json
        # 从config中导入TARGET_GROUPS，以便在提示词中使用
        from streamlit_app.config.config import TARGET_GROUPS

        target_groups_json_array_for_prompt = json.dumps(TARGET_GROUPS, ensure_ascii=False)

        # 使用f-string结合多行字符串来构建SYSTEM_PROMPT_TEMPLATE
        # 注意：在f-string中要表示字面量的花括号 { 或 }，需要使用双花括号 {{ 或 }}
        SYSTEM_PROMPT_TEMPLATE = f'''你是一个专业的母婴视频内容分析师，擅长精确识别视频的目标人群。

你的任务是根据视频转录文本，从预定义的目标人群列表中选择**唯一一个最匹配**的分类。

目标人群列表及其详细定义：{target_groups_json_array_for_prompt}

**人群判断指导原则：**

1. **孕期妈妈**：
   - 关键词：怀孕、孕期、待产包、产检、建档、准妈妈、卸货、分娩、生产、产科
   - 场景：讨论孕期营养、待产准备、产前产后护理、新生儿喂养准备
   - 时间节点：怀孕期间、分娩前后、产后初期（0-42天）

2. **二胎妈妈**：
   - 关键词：二胎、老大、老二、两个孩子、大宝、二宝、再次怀孕
   - 场景：比较两胎经验、多孩子养育、二胎备孕/怀孕

3. **混养妈妈**：
   - 关键词：混合喂养、混喂、亲喂、奶粉混合、母乳不足、奶量不够
   - 场景：母乳与奶粉结合喂养、奶水不足补充

4. **新手爸妈**：
   - 关键词：新手、没有经验、第一次、新生儿、不知道怎么、学习、初次
   - 场景：初为父母、缺乏育儿经验、学习喂养知识
   - 包含：新手爸爸、新手妈妈、初次育儿的父母

5. **贵妇妈妈**：
   - 关键词：高端、奢华、精致、品质、贵、高价、进口、顶级
   - 场景：追求高品质产品、注重品牌档次、消费能力强

**优先级判断规则（按重要性排序）：**
1. 如果同时匹配多个人群，按以下优先级选择：
   - "二胎妈妈" > "孕期妈妈" > "混养妈妈" > "贵妇妈妈" > "新手爸妈"
2. 如果提到"刚生完"、"产后"、"新生宝宝"、"出生后"等产后早期关键词，优先考虑"孕期妈妈"
3. 如果明确提到"二胎"、"老大老二"等多孩经验，优先选择"二胎妈妈"
4. 如果明确提到"混合喂养"、"奶水不足"等，优先选择"混养妈妈"
5. 如果强调"高端"、"奢华"等品质追求，优先选择"贵妇妈妈"
6. 其他情况或无明确特征时，选择"新手爸妈"

**重要要求：**
- **必须且只能**选择一个最匹配的人群
- 不允许返回多个人群或空数组
- 必须基于内容特征进行判断，不能随意选择

输出格式：
{{{{
  "target_audience": "从上述列表中选择的唯一一个人群分类"
}}}}
'''

        # 准备用户提示模板
        user_prompt = f"""请仔细分析以下视频转录文本，识别出**唯一一个最匹配**的目标人群：

--- 转录文本开始 ---
{full_transcript}
--- 转录文本结束 ---

分析步骤：
1. 仔细阅读转录文本，识别关键词和场景描述
2. 对照人群判断指导原则，找出最匹配的人群特征
3. 如果匹配多个人群，按照优先级规则选择最重要的一个
4. 从目标人群列表中选择**唯一一个**最合适的分类
5. 确保必须选择一个人群，不能返回空值

重要提醒：
- 只能返回一个目标人群，不能返回多个
- 必须是预定义列表中的人群名称
- 基于内容特征做出最佳判断

请以JSON格式输出分析结果，确保target_audience字段是单个字符串值。
"""

        # 发送分析请求
        try:
            response = self._chat_completion(
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT_TEMPLATE},
                    {"role": "user", "content": user_prompt}
                ],
                model="deepseek-chat"
            )
            
            # 从响应中获取JSON格式结果
            if response and "choices" in response and response["choices"]:
                result_text = response["choices"][0].get("message", {}).get("content", "")
                logger.debug(f"DeepSeek API (analyze_video_summary) 原始响应: {result_text[:500]}...")
                
                # 尝试提取和解析JSON数据
                import re
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # 如果没有Markdown代码块，则尝试直接解析整个文本
                    json_str = result_text
                
                try:
                    result_dict = json.loads(json_str)
                    # 只返回目标人群信息
                    return {
                        "target_audience": result_dict.get("target_audience", "")
                    }
                except json.JSONDecodeError as e:
                    logger.error(f"JSON解析失败: {e}, 原始文本: {json_str[:500]}...")
            
                    # 尝试修复双花括号问题
                    try:
                        # 移除开头和结尾的多余花括号
                        cleaned_json = json_str.strip()
                        if cleaned_json.startswith('{{') and cleaned_json.endswith('}}'):
                            cleaned_json = cleaned_json[1:-1]  # 移除外层花括号
                            logger.info(f"尝试修复双花括号JSON格式: {cleaned_json[:200]}...")
                            result_dict = json.loads(cleaned_json)
                            return {
                                "target_audience": result_dict.get("target_audience", "")
                            }
                    except json.JSONDecodeError as e2:
                        logger.error(f"修复双花括号后仍然JSON解析失败: {e2}")
                    
                    # 如果JSON解析完全失败，尝试正则表达式提取目标人群
                    try:
                        import re
                        # 尝试匹配 "target_audience": ["xxx", "yyy"] 格式
                        pattern = r'"target_audience"\s*:\s*\[(.*?)\]'
                        match = re.search(pattern, json_str, re.DOTALL)
                        if match:
                            audience_str = match.group(1)
                            # 提取引号内的内容
                            audience_pattern = r'"([^"]+)"'
                            audiences = re.findall(audience_pattern, audience_str)
                            logger.info(f"通过正则表达式提取到目标人群: {audiences}")
                            return {
                                "target_audience": audiences[0] if audiences else ""
                            }
                    except Exception as e3:
                        logger.error(f"正则表达式提取目标人群失败: {e3}")
            
            # 如果解析失败，返回空结果
            logger.error(f"无法从DeepSeek响应中提取结构化数据: {response}")
            return {}
            
        except Exception as e:
            logger.error(f"视频内容分析失败: {str(e)}")
            return {}

    def segment_transcript_by_intent(self, srt_file_path: str) -> list:
        """
        使用DeepSeek模型将SRT文件内容按意图划分为语义区块。

        Args:
            srt_file_path: SRT字幕文件的路径。

        Returns:
            一个语义区块列表，每个区块是一个字典，包含:
            'semantic_type' (str): 来自 SEMANTIC_SEGMENT_TYPES 的类型。
            'text' (str): 该语义区块的文本内容 (由SRT行拼接而成)。
            'asr_matched_text' (str): 与 'text' 相同，因为直接来自SRT。
            'start_time' (float): 开始时间 (秒)。
            'end_time' (float): 结束时间 (秒)。
            如果LLM分析失败，则返回空列表。
        """
        if not srt_file_path or not os.path.exists(srt_file_path):
            logger.warning(f"SRT文件路径无效或文件不存在: {srt_file_path}，无法进行语义分段。")
            return []

        # 1. 读取并解析SRT文件
        srt_entries = []
        try:
            with open(srt_file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            srt_blocks_raw = content.split('\n\n')
            entry_id_counter = 1
            for block_raw in srt_blocks_raw:
                lines = block_raw.strip().split('\n')
                if len(lines) >= 2:
                    try:
                        time_line_index = 0
                        if lines[0].isdigit() and len(lines) >=3:
                             time_line_index = 1
                        
                        if '-->' not in lines[time_line_index]:
                            if len(lines) > time_line_index + 1 and '-->' in lines[time_line_index+1]:
                                time_line_index += 1
                            else:
                                logger.warning(f"无法在SRT区块中找到有效的时间行: {block_raw}")
                                continue

                        time_line = lines[time_line_index]
                        text_lines = lines[time_line_index+1:]
                        start_time_str, end_time_str = time_line.split(' --> ')
                        
                        def srt_time_to_seconds(t_str):
                            h, m, s_ms = t_str.split(':')
                            s, ms = s_ms.split(',')
                            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0

                        start_time_sec = srt_time_to_seconds(start_time_str)
                        end_time_sec = srt_time_to_seconds(end_time_str)
                        text_content = " ".join(text_lines).strip()

                        if text_content:
                            srt_entries.append({
                                "id": entry_id_counter,
                                "start_time": start_time_sec,
                                "end_time": end_time_sec,
                                "text": text_content
                            })
                            entry_id_counter += 1
                    except Exception as e:
                        logger.warning(f"解析SRT区块时出错: {str(e)}. 区块内容: {block_raw}")
                        continue
        except Exception as e:
            logger.error(f"读取或解析SRT文件时出错: {str(e)}")
            return []

            if not srt_entries:
            logger.warning(f"SRT文件 {srt_file_path} 中没有有效的字幕条目。")
                return []
            
        logger.info(f"成功解析SRT文件，共 {len(srt_entries)} 个字幕条目。")

        # 2. 构建用于LLM分析的文本
        srt_text_for_llm = ""
        for entry in srt_entries:
            srt_text_for_llm += f"L{entry['id']}: {entry['text']}\n"

        # 3. 获取语义类型定义
        from streamlit_app.config.config import get_semantic_segment_types, get_semantic_type_definitions
        semantic_segment_types = get_semantic_segment_types()
        semantic_definitions = get_semantic_type_definitions()

        type_description_list = []
        for type_name in semantic_segment_types:
            definition = semantic_definitions.get(type_name, {})
            description = definition.get('description', f'{type_name}类型的内容')
            keywords = definition.get('keywords', [])
            examples = definition.get('examples', [])
            
            full_description = f"{type_name}: {description}"
            if keywords:
                full_description += f" 关键词：{', '.join(keywords[:3])}"
            if examples:
                example_text = "; ".join(examples[:3])
                full_description += f" 示例：{example_text}"
            
            type_description_list.append(full_description)

        type_descriptions_formatted_str = "\n".join(type_description_list)

        # 4. 调用DeepSeek API进行语义分段
        try:
            logger.info("准备调用DeepSeek API进行基于SRT的语义分段")
            segments_result = self._call_deepseek_for_srt_segmentation(
                srt_text_for_llm, 
                type_descriptions_formatted_str
            )
            
            if not segments_result or "segments" not in segments_result:
                logger.error("DeepSeek API返回的分段结果无效或不包含segments键。")
                return []
            
            llm_segments_info = segments_result.get("segments", [])
            if not llm_segments_info:
                logger.warning("LLM返回的语义区块列表为空。")
                return []
            
            logger.info(f"成功获取 {len(llm_segments_info)} 个基于SRT的语义区块定义")

            # 5. 处理API返回结果，转换为Segment格式，并进行句子完整性调整
            result_segments = []
            for segment_def in llm_segments_info:
                try:
                    segment_type = segment_def.get("segment_type", "其他")
                    start_line_id = segment_def.get("start_line_id")
                    end_line_id = segment_def.get("end_line_id")

                    if start_line_id is None or end_line_id is None:
                        logger.warning(f"LLM返回的区块定义缺少 start_line_id 或 end_line_id: {segment_def}")
                        continue

                    if segment_type not in semantic_segment_types:
                        logger.warning(f"LLM返回了未知的语义类型 '{segment_type}', 将其归类为 '其他'. 区块: {segment_def}")
                        segment_type = "其他"
                    
                    # 🆕 应用句子完整性调整
                    adjusted_start_line_id, adjusted_end_line_id = self._adjust_segment_boundaries_for_sentence_completeness(
                        start_line_id, end_line_id, srt_entries
                    )
                    
                    relevant_entries = [entry for entry in srt_entries if adjusted_start_line_id <= entry["id"] <= adjusted_end_line_id]
                    if not relevant_entries:
                        logger.warning(f"根据调整后的行号范围 {adjusted_start_line_id}-{adjusted_end_line_id} 未找到匹配的SRT条目. 区块: {segment_def}")
                        continue
                    
                    start_time = min(entry["start_time"] for entry in relevant_entries)
                    end_time = max(entry["end_time"] for entry in relevant_entries)
                    text_content = " ".join(entry["text"] for entry in relevant_entries)
                    
                    # 🆕 记录边界调整信息
                    if adjusted_start_line_id != start_line_id or adjusted_end_line_id != end_line_id:
                        logger.info(f"语义区块 '{segment_type}' 边界已调整: L{start_line_id}-L{end_line_id} → L{adjusted_start_line_id}-L{adjusted_end_line_id} (为保证句子完整性)")
                    
                    segment_result = {
                        "semantic_type": segment_type,
                        "text": text_content,
                        "asr_matched_text": text_content,
                        "start_time": start_time,
                        "end_time": end_time,
                        "time_period": f"{self._format_seconds_to_time(start_time)} - {self._format_seconds_to_time(end_time)}",
                        "srt_line_ids": list(range(adjusted_start_line_id, adjusted_end_line_id + 1)),
                        "original_line_range": f"L{start_line_id}-L{end_line_id}",  # 记录原始范围
                        "adjusted_line_range": f"L{adjusted_start_line_id}-L{adjusted_end_line_id}"  # 记录调整后范围
                    }
                    result_segments.append(segment_result)
                except Exception as e_segment:
                    logger.warning(f"处理LLM返回的单个语义区块时出错: {str(e_segment)}. 区块定义: {segment_def}")
            
            if not result_segments:
                logger.warning("成功调用LLM，但未能从其响应中构建任何有效的语义分段对象。")
                return []

            logger.info(f"成功将LLM分段结果转换为 {len(result_segments)} 个语义分段对象。")
            return result_segments
            
        except Exception as e_api_call:
            logger.error(f"调用DeepSeek API进行语义分段时发生严重错误: {str(e_api_call)}")
            logger.error(f"错误详情: {traceback.format_exc()}")
            return []

    def _adjust_segment_boundaries_for_sentence_completeness(self, start_line_id: int, end_line_id: int, srt_entries: list) -> tuple:
        """
        调整分段边界以确保句子完整性，同时避免跨越产品边界
        
        Args:
            start_line_id: 原始起始行号
            end_line_id: 原始结束行号
            srt_entries: SRT条目列表
            
        Returns:
            tuple: (调整后的起始行号, 调整后的结束行号)
        """
        # 获取相关的SRT条目
        relevant_entries = [entry for entry in srt_entries if start_line_id <= entry["id"] <= end_line_id]
        if not relevant_entries:
            return start_line_id, end_line_id
        
        # 🆕 检查原始片段中的产品类型
        original_text = " ".join(entry["text"] for entry in relevant_entries)
        original_products = self._detect_products_in_text(original_text)
        
        # 检查起始边界的句子完整性
        adjusted_start_line_id = self._adjust_start_boundary_for_sentence_completeness(start_line_id, srt_entries, original_products)
        
        # 检查结束边界的句子完整性
        adjusted_end_line_id = self._adjust_end_boundary_for_sentence_completeness(end_line_id, srt_entries, original_products)
        
        return adjusted_start_line_id, adjusted_end_line_id
    
    def _detect_products_in_text(self, text: str) -> set:
        """
        检测文本中包含的产品类型，区分主要产品和次要提及
        
        Args:
            text: 要检测的文本
            
        Returns:
            set: 检测到的主要产品类型集合
        """
        products = set()
        text_lower = text.lower()
        
        # 🆕 增强的产品关键词映射，按重要性和精确度排序
        product_keywords = {
            "启赋蕴淳": {
                "primary": [
                    "启赋蕴淳", "蕴淳奶粉", "蕴淳的", "蕴淳更适合", "蕴淳采用", 
                    "蕴淳配方", "蕴淳系列", "蕴淳产品", "蕴淳营养", "蕴淳含有",
                    "蕴淳特别", "蕴淳独有", "蕴淳专门", "蕴淳针对"
                ],
                "secondary": ["蕴淳"],
                "context_words": ["配方", "营养", "成分", "奶源", "品质", "适合", "选择"]
            },
            "启赋水奶": {
                "primary": [
                    "启赋水奶", "水奶系列", "水奶是", "水奶的", "水奶更", "水奶可以",
                    "液态奶", "即开即饮", "即饮型", "开盖即饮", "水奶产品", "水奶营养",
                    "水奶方便", "水奶便携", "水奶适合"
                ],
                "secondary": ["水奶"],
                "context_words": ["方便", "携带", "即饮", "液态", "开盖", "便携", "外出"]
            },
            "启赋蓝钻": {
                "primary": [
                    "启赋蓝钻", "蓝钻系列", "蓝钻的", "蓝钻更", "蓝钻采用",
                    "蓝钻配方", "蓝钻产品", "蓝钻营养", "蓝钻品质"
                ],
                "secondary": ["蓝钻"],
                "context_words": ["高端", "品质", "营养", "配方", "成分"]
            }
        }
        
        # 🆕 转换词检测 - 这些词通常表示产品切换
        transition_indicators = [
            "现在我们来看看", "接下来介绍", "另外还有", "除此之外", 
            "我们再来看", "还有一款", "另一个产品", "另一种",
            "相比之下", "而且还有", "同时还有", "此外还有"
        ]
        
        # 首先检查是否有转换词，如果有，更倾向于识别为产品切换
        has_transition = any(indicator in text for indicator in transition_indicators)
        
        # 检查每个产品
        for product, keywords in product_keywords.items():
            product_score = 0
            
            # 检查主要关键词（高权重）
            primary_matches = sum(1 for keyword in keywords["primary"] if keyword in text)
            product_score += primary_matches * 3
            
            # 检查次要关键词（低权重）
            secondary_matches = sum(1 for keyword in keywords["secondary"] if keyword in text)
            product_score += secondary_matches * 1
            
            # 检查上下文词汇（中等权重）
            context_matches = sum(1 for word in keywords["context_words"] if word in text)
            product_score += context_matches * 0.5
            
            # 🆕 如果有转换词，降低识别阈值
            threshold = 2 if has_transition else 3
            
            # 如果分数达到阈值，认为是主要产品
            if product_score >= threshold:
                products.add(product)
                continue
            
            # 🆕 特殊情况：如果有次要关键词且文本较长，进行更详细的分析
            if secondary_matches > 0 and len(text) > 15:
                if self._is_primary_product_discussion(text, product):
                    products.add(product)
        
        # 🆕 记录检测结果用于调试
        if products:
            logger.debug(f"在文本中检测到产品: {products} | 文本: {text[:50]}...")
        
        return products
    
    def _is_primary_product_discussion(self, text: str, product: str) -> bool:
        """
        判断文本是否主要在讨论某个产品
        
        Args:
            text: 文本内容
            product: 产品名称
            
        Returns:
            bool: 是否是主要讨论
        """
        # 如果文本很短（少于15个字符），可能只是顺带提及
        if len(text) < 15:
            return False
        
        # 🆕 增强的产品相关描述性词汇
        product_descriptors = {
            "启赋蕴淳": [
                "配方", "营养", "适合", "选择", "品质", "成分", "奶源", "含有",
                "采用", "特别", "独有", "专门", "针对", "设计", "研发", "科学",
                "天然", "有机", "安全", "健康", "消化", "吸收", "免疫", "发育"
            ],
            "启赋水奶": [
                "方便", "携带", "即饮", "液态", "开盖", "便携", "外出", "随时",
                "即开", "新鲜", "保存", "储存", "温度", "冷藏", "常温", "包装",
                "旅行", "出门", "上班", "工作", "忙碌", "快捷", "简单"
            ],
            "启赋蓝钻": [
                "高端", "品质", "营养", "配方", "成分", "奶源", "进口", "优质",
                "精选", "顶级", "专业", "科学", "先进", "技术", "工艺", "标准"
            ]
        }
        
        descriptors = product_descriptors.get(product, [])
        descriptor_count = sum(1 for desc in descriptors if desc in text)
        
        # 🆕 检查产品动作词汇（表示对产品的具体描述或推荐）
        action_words = [
            "推荐", "建议", "选择", "使用", "喝", "食用", "购买", "尝试",
            "适合", "满足", "解决", "帮助", "提供", "含有", "富含", "添加"
        ]
        action_count = sum(1 for action in action_words if action in text)
        
        # 🆕 检查是否有产品转换的信号词
        transition_words = [
            "现在我们来看看", "接下来介绍", "另外还有", "除此之外", 
            "我们再来看", "还有一款", "另一个产品", "另一种",
            "相比之下", "而且还有", "同时还有", "此外还有"
        ]
        has_transition = any(word in text for word in transition_words)
        
        # 🆕 检查是否有比较性词汇（通常表示在对比不同产品）
        comparison_words = ["相比", "比较", "对比", "而", "但是", "不过", "然而", "区别"]
        has_comparison = any(word in text for word in comparison_words)
        
        # 🆕 综合评分系统
        score = 0
        
        # 描述性词汇得分（每个词0.5分）
        score += descriptor_count * 0.5
        
        # 动作词汇得分（每个词1分）
        score += action_count * 1
        
        # 转换词加分（表示重点介绍）
        if has_transition:
            score += 2
        
        # 比较词汇加分（表示在对比产品）
        if has_comparison:
            score += 1
        
        # 🆕 文本长度加分（较长的文本更可能是主要讨论）
        if len(text) > 30:
            score += 1
        if len(text) > 50:
            score += 1
        
        # 🆕 产品名称出现频率加分
        product_mentions = text.lower().count(product.lower())
        if product_mentions > 1:
            score += product_mentions * 0.5
        
        # 判断阈值：得分>=2.5认为是主要讨论
        is_primary = score >= 2.5
        
        # 🆕 记录判断过程用于调试
        if is_primary:
            logger.debug(f"判断为主要产品讨论 - 产品: {product}, 得分: {score:.1f}, 文本: {text[:30]}...")
        
        return is_primary
    
    def _adjust_start_boundary_for_sentence_completeness(self, start_line_id: int, srt_entries: list, original_products: set = None) -> int:
        """
        调整起始边界以确保不会在句子中间开始，同时避免跨越产品边界
        
        Args:
            start_line_id: 原始起始行号
            srt_entries: SRT条目列表
            original_products: 原始片段中的产品类型
            
        Returns:
            int: 调整后的起始行号
        """
        # 找到对应的SRT条目
        start_entry = None
        for entry in srt_entries:
            if entry["id"] == start_line_id:
                start_entry = entry
                break
        
        if not start_entry:
            return start_line_id
        
        # 检查当前行的文本是否以句子开头的标志开始
        text = start_entry["text"].strip()
        
        # 如果文本以小写字母开始，或者以连接词开始，可能是句子的中间部分
        sentence_continuation_indicators = [
            "而且", "并且", "同时", "另外", "此外", "然而", "但是", "不过", "因此", "所以", 
            "那么", "这样", "这里", "那里", "这个", "那个", "它", "他", "她", "我们", "你们", "他们"
        ]
        
        # 检查是否以连接词开始
        starts_with_continuation = any(text.startswith(indicator) for indicator in sentence_continuation_indicators)
        
        # 检查是否以小写英文字母开始（可能是英文句子的中间）
        starts_with_lowercase = text and text[0].islower() and text[0].isalpha()
        
        if starts_with_continuation or starts_with_lowercase:
            # 🆕 进一步限制搜索范围：最多向前查找2行
            search_limit = min(2, start_line_id - 1)
            
            # 向前查找句子的真正开始，但不超过产品边界
            for i in range(start_line_id - 1, max(1, start_line_id - search_limit - 1), -1):
                prev_entry = None
                for entry in srt_entries:
                    if entry["id"] == i:
                        prev_entry = entry
                        break
                
                if prev_entry:
                    # 🆕 更严格的产品边界检查
                    if original_products:
                        prev_products = self._detect_products_in_text(prev_entry["text"])
                        if prev_products:
                            # 如果前一行有产品且与原始产品不同，停止调整
                            if not prev_products.intersection(original_products):
                                logger.info(f"检测到产品边界，停止向前调整起始边界: {prev_products} vs {original_products}")
                                break
                            # 🆕 即使是相同产品，如果有转换词也要小心
                            transition_words = ["现在我们来看看", "接下来", "另外", "还有"]
                            if any(word in prev_entry["text"] for word in transition_words):
                                logger.info(f"检测到转换词，停止向前调整: {prev_entry['text'][:30]}...")
                                break
                    
                    prev_text = prev_entry["text"].strip()
                    # 检查前一行是否以句号、感叹号、问号结尾
                    if prev_text.endswith(('.', '!', '?', '。', '！', '？')):
                        # 找到了句子的结束，当前行就是新句子的开始
                        break
                    elif i == 1:
                        # 已经到了第一行，从这里开始
                        return 1
                else:
                    break
            
            # 🆕 如果找到了更合适的起始点，但要确保不超过搜索限制
            adjusted_start = max(1, start_line_id - 1)
            if start_line_id - adjusted_start <= search_limit:
                return adjusted_start
        
        return start_line_id
    
    def _adjust_end_boundary_for_sentence_completeness(self, end_line_id: int, srt_entries: list, original_products: set = None) -> int:
        """
        调整结束边界以确保句子完整性，同时避免跨越产品边界
        
        Args:
            end_line_id: 原始结束行号
            srt_entries: SRT条目列表
            original_products: 原始片段中的产品类型
            
        Returns:
            int: 调整后的结束行号
        """
        # 找到对应的SRT条目
        end_entry = None
        for entry in srt_entries:
            if entry["id"] == end_line_id:
                end_entry = entry
                break
        
        if not end_entry:
            return end_line_id
        
        # 检查当前行的文本是否以句子结尾的标志结束
        text = end_entry["text"].strip()
        
        # 如果文本不以句号、感叹号、问号结尾，可能句子还没有完成
        if not text.endswith(('.', '!', '?', '。', '！', '？')):
            # 🆕 进一步限制搜索范围：最多向后查找2行
            max_line_id = max(entry["id"] for entry in srt_entries)
            search_limit = min(2, max_line_id - end_line_id)
            
            # 向后查找句子的真正结束，但不超过产品边界
            for i in range(end_line_id + 1, min(max_line_id + 1, end_line_id + search_limit + 1)):
                next_entry = None
                for entry in srt_entries:
                    if entry["id"] == i:
                        next_entry = entry
                        break
                
                if next_entry:
                    # 🆕 更严格的产品边界检查
                    if original_products:
                        next_products = self._detect_products_in_text(next_entry["text"])
                        if next_products:
                            # 如果后一行有产品且与原始产品不同，停止调整
                            if not next_products.intersection(original_products):
                                logger.info(f"检测到产品边界，停止向后调整结束边界: {next_products} vs {original_products}")
                                break
                            # 🆕 检查是否有转换词，即使是相同产品也要小心
                            transition_words = ["现在我们来看看", "接下来", "另外", "还有"]
                            if any(word in next_entry["text"] for word in transition_words):
                                logger.info(f"检测到转换词，停止向后调整: {next_entry['text'][:30]}...")
                                break
                    
                    next_text = next_entry["text"].strip()
                    # 检查这一行是否以句子结尾标志结束
                    if next_text.endswith(('.', '!', '?', '。', '！', '？')):
                        # 找到了句子的结束，但要确保不超过搜索限制
                        if i - end_line_id <= search_limit:
                            return i
                        else:
                            break
                    elif i == max_line_id:
                        # 已经到了最后一行
                        return max_line_id
                else:
                    break
            
            # 🆕 如果没有找到明确的句子结尾，最多向后延伸1行（进一步减少范围）
            adjusted_end = min(max_line_id, end_line_id + 1)
            if adjusted_end - end_line_id <= search_limit:
                return adjusted_end
        
        return end_line_id

    def _format_seconds_to_time(self, seconds: float) -> str:
        """将秒转换为时间字符串格式 (HH:MM:SS)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    def _call_deepseek_for_srt_segmentation(self, srt_text_for_llm, type_descriptions_formatted_str):
        """
        调用DeepSeek API进行SRT语义分段
        
        Args:
            srt_text_for_llm: 格式化的SRT文本
            type_descriptions_formatted_str: 语义类型描述
            
        Returns:
            包含分段信息的字典
        
        Raises:
            ValueError: 如果API响应无效或JSON解析失败。
            requests.exceptions.RequestException: 如果API请求失败。
        """
        import requests
        import json
        import re
        import traceback
        
        # 🆕 集成用户反馈
        try:
            from streamlit_app.modules.analysis.feedback_manager import get_feedback_manager
            feedback_manager = get_feedback_manager()
        except ImportError:
            feedback_manager = None
            logger.warning("无法导入反馈管理器，将使用基础提示词")
        
        # 构建基础系统提示
        base_system_prompt = (
            "你是一位专业的视频内容结构分析师。你的任务是分析一个以SRT字幕行形式提供的视频转录文本，"
            "并将连续的SRT行组合成符合预定义语义类型的逻辑区块。"
            "每个SRT行都有一个唯一的行号（例如 L1, L2, ...）。你需要确定每个语义区块由哪些SRT行组成。"
            f"\n\n【预定义的语义区块类型及其说明】:\n{type_descriptions_formatted_str}"
            "\n\n【核心分段原则 - 按重要性排序】："
            "\n🔥🔥🔥 1. 产品类型变化强制分段（最高优先级）："
            "\n   当内容从一个产品切换到另一个产品时，必须立即创建新的片段"
            "\n   - 启赋蕴淳 → 启赋水奶：必须分段"
            "\n   - 启赋水奶 → 启赋蕴淳：必须分段"
            "\n   - 启赋蓝钻 → 其他产品：必须分段"
            "\n   - 即使语义类型相同（如都是'产品优势'），也要分成不同片段"
            "\n   - 即使只是简单提及另一个产品，也要考虑分段"
            "\n\n2. 句子完整性：确保每个语义区块都以完整的句子开始和结束"
            "\n3. 语义连贯性：相同语义类型且相同产品的内容归为一个区块"
            "\n4. 自然停顿：优先在自然的语音停顿处分段"
            "\n5. 适度长度：每个区块应该有合理的长度（10-40秒为佳）"
            "\n\n【产品识别关键词 - 精确匹配】："
            "\n- 启赋蕴淳：'启赋蕴淳'、'蕴淳奶粉'、'蕴淳的'、'蕴淳更'、'蕴淳采用'"
            "\n- 启赋水奶：'启赋水奶'、'水奶'、'液态奶'、'即饮'、'即开即饮'"
            "\n- 启赋蓝钻：'启赋蓝钻'、'蓝钻'、'蓝钻系列'"
            "\n\n【产品变化检测策略】："
            "\n1. 逐行扫描，标记每行提到的产品关键词"
            "\n2. 当发现新产品关键词出现时，检查是否为产品切换"
            "\n3. 产品切换的判断标准："
            "\n   - 前面几行主要讨论产品A，当前行开始讨论产品B"
            "\n   - 出现转换词 + 新产品名称"
            "\n   - 产品特性描述发生明显变化"
            "\n\n【转换词识别（产品切换信号）】："
            "\n⚠️ 发现以下转换词时，通常表示产品切换，必须分段："
            "\n   - '现在我们来看看'、'接下来介绍'、'另外还有'、'除此之外'"
            "\n   - '我们再来看'、'还有一款'、'另一个产品'、'另一种'"
            "\n   - '相比之下'、'而'、'但是'、'不过'（当伴随产品名称时）"
            "\n   - '同时'、'此外'、'另外'（当引入新产品时）"
            "\n\n【具体分段示例】："
            "\n❌ 错误示例："
            "\n   L1-L8: '启赋蕴淳的营养配方很好...现在我们来看看启赋水奶的便携性...' → 归为一个'产品优势'片段"
            "\n✅ 正确示例："
            "\n   L1-L4: '启赋蕴淳的营养配方很好...' → '产品优势'片段（启赋蕴淳）"
            "\n   L5-L8: '现在我们来看看启赋水奶的便携性...' → '产品优势'片段（启赋水奶）"
            "\n\n❌ 错误示例："
            "\n   L10-L20: '蕴淳适合...而水奶更方便...' → 归为一个片段"
            "\n✅ 正确示例："
            "\n   L10-L15: '蕴淳适合...' → 片段A（启赋蕴淳）"
            "\n   L16-L20: '而水奶更方便...' → 片段B（启赋水奶）"
            "\n\n【质量检查要点】："
            "\n⚠️ 分段完成后，请自检："
            "\n1. 是否有任何片段同时包含多个不同产品的主要讨论？"
            "\n2. 是否有产品切换点被忽略？"
            "\n3. 是否有转换词后面跟着新产品但没有分段？"
            "\n4. 片段长度是否合理（避免过长或过短）？"
            "\n\n⚠️ 细粒度分段优于粗粒度分段：宁可多分几个片段，也不要将不同产品混在一起"
            "\n\n请严格按照以下JSON格式输出一个列表，确保产品变化时必须分段："
            '\n[\n  {\n    "segment_type": "产品优势",\n    "start_line_id": 1,\n    "end_line_id": 3,\n    "note": "启赋蕴淳产品优势"\n  },\n'
            '  {\n    "segment_type": "产品优势",\n    "start_line_id": 4,\n    "end_line_id": 6,\n    "note": "启赋水奶产品优势"\n  }\n]'
        )
        
        # 🆕 应用用户反馈改进提示词
        if feedback_manager:
            try:
                system_prompt = feedback_manager.apply_feedback_to_prompt(base_system_prompt)
                logger.info("已应用用户反馈改进提示词")
            except Exception as e:
                logger.warning(f"应用用户反馈失败，使用基础提示词: {e}")
                system_prompt = base_system_prompt
        else:
            system_prompt = base_system_prompt

        # 构建用户提示
        user_prompt = (
            "请根据以下逐行标记的SRT转录文本进行语义区块划分。"
            "\n\n🔥🔥🔥【最重要的要求 - 产品变化强制分段】："
            "\n1. 仔细阅读每一行文本，识别产品名称（启赋蕴淳、启赋水奶、启赋蓝钻等）"
            "\n2. 当发现产品名称变化时，必须立即创建新的片段，即使语义类型相同"
            "\n3. 特别注意转换词：'现在我们来看看'、'接下来'、'另外'、'还有'等"
            "\n4. 宁可多分几个片段，也不要将不同产品的内容合并"
            "\n\n【详细分析步骤】："
            "\n第一步：逐行扫描产品标记"
            "\n- 为每一行标记包含的产品关键词"
            "\n- 识别主要讨论的产品（不是简单提及）"
            "\n- 标记产品变化的潜在边界点"
            "\n\n第二步：识别产品变化边界"
            "\n- 寻找从产品A切换到产品B的行"
            "\n- 特别关注转换词后的产品名称"
            "\n- 确认产品特性描述的变化"
            "\n\n第三步：强制分段处理"
            "\n- 在每个产品变化点创建新片段"
            "\n- 确保不同产品不在同一片段中"
            "\n- 检查是否有遗漏的产品边界"
            "\n\n第四步：语义类型分配"
            "\n- 为每个片段分配合适的语义类型"
            "\n- 确保片段内容与语义类型匹配"
            "\n- 优化片段长度和完整性"
            "\n\n【质量检查清单】："
            "\n✅ 每个片段是否只包含一个主要产品的讨论？"
            "\n✅ 是否在所有产品变化点都进行了分段？"
            "\n✅ 是否有转换词+新产品的组合被遗漏？"
            "\n✅ 片段长度是否合理（10-40秒）？"
            "\n✅ 语义类型是否准确匹配片段内容？"
            "\n\n输出的 `start_line_id` 和 `end_line_id` 必须准确对应输入文本中每行前的L{id}标记。"
            f"\n--- 带行号的SRT转录文本开始 ---\n{srt_text_for_llm}"
            "\n--- 带行号的SRT转录文本结束 ---"
            "\n\n请严格按照JSON格式输出，确保产品变化时必须分段。每个片段必须包含note字段说明主要讨论的产品。"
        )
        
        # 调用DeepSeek API
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 4096
            }
            
            logger.debug(f"准备调用DeepSeek API，提示词长度: {len(system_prompt) + len(user_prompt)}")
            
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            response.raise_for_status() # 如果请求失败(状态码 4xx 或 5xx)，将引发 HTTPError
            
            llm_response = response.json()
            
            if not llm_response or "choices" not in llm_response or not llm_response["choices"]:
                logger.error("DeepSeek API响应无效或不包含choices")
                raise ValueError("API响应无效: 响应中缺少 'choices' 字段")
            
            llm_output_text = llm_response['choices'][0]['message']['content']
            preview_length = min(500, len(llm_output_text))
            logger.info(f"DeepSeek API调用成功。原始输出预览: {llm_output_text[:preview_length]}...")

            # 从输出中提取JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', llm_output_text)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_start = llm_output_text.find('[')
                json_end = llm_output_text.rfind(']')
                if json_start != -1 and json_end != -1 and json_start < json_end:
                    json_str = llm_output_text[json_start:json_end+1]
                else:
                    json_str = llm_output_text # 假设整个输出是JSON
                
            logger.debug(f"尝试解析的JSON字符串: {json_str[:500]}...")
            
            segments = json.loads(json_str) # 如果解析失败，会抛出 json.JSONDecodeError
            
            if not isinstance(segments, list) or not segments: # 检查是否为非空列表
                logger.warning("解析后的JSON不是有效的区块列表或区块为空")
                raise ValueError("LLM返回的JSON不是有效的区块列表或区块为空")
                
            logger.info(f"成功解析LLM返回的JSON，获得 {len(segments)} 个基于SRT的语义区块定义。")
            return {"segments": segments}
            
        except requests.exceptions.RequestException as e_req:
            logger.error(f"调用DeepSeek API请求失败: {str(e_req)}")
            logger.error(f"错误详情: {traceback.format_exc()}")
            raise # 重新抛出请求异常
        except json.JSONDecodeError as e_json:
            logger.error(f"解析LLM语义分段结果失败: {e_json}. LLM原始输出 (前200字符): {llm_output_text[:200]}...")
            logger.error(f"错误详情: {traceback.format_exc()}")
            raise ValueError(f"LLM响应JSON解析错误: {e_json}. 原始文本: {llm_output_text[:200]}...")
        except ValueError as e_val:
            logger.error(f"处理API响应时发生值错误: {str(e_val)}")
            logger.error(f"错误详情: {traceback.format_exc()}")
            raise # 重新抛出值错误
        except Exception as e:
            logger.error(f"调用DeepSeek API或处理响应时发生未知错误: {str(e)}")
            logger.error(f"错误详情: {traceback.format_exc()}")
            raise Exception(f"DeepSeek API调用或处理时发生未知错误: {e}") # 抛出通用异常

class IntentAnalyzer:
    """视频意图分析类"""
    
    def __init__(self, segments=None, target_audience=None, product_type=None, selling_points=None):
        """
        初始化意图分析器
        
        Args:
            segments: 视频分段列表
            target_audience: 目标人群
            product_type: 产品类型
            selling_points: 产品卖点列表
        """
        self.segments_data = segments or []
        self.target_audience = target_audience or []
        self.product_type = product_type or []
        self.selling_points = selling_points or []
        self.semantic_analyzer = SemanticAnalyzer()
        
        logger.info(f"初始化意图分析器，目标人群: {target_audience}, 产品类型: {product_type}, 产品卖点: {selling_points}")
    
    def _contains_chinese(self, s: str) -> bool:
        """检查字符串是否包含中文字符"""
        if not s: # 处理空字符串或None的情况
            return False
        for char in s:
            if '\\u4e00' <= char <= '\\u9fff':
                return True
        return False
    
    def _format_time(self, seconds):
        """
        将秒数格式化为时:分:秒格式
        
        Args:
            seconds: 秒数
            
        Returns:
            格式化的时间字符串
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = int(seconds % 60)
        
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def analyze_segments(self, segments_data):
        """
        批量分析音频片段数据，将每个片段归类到特定的语义分类中。

        Args:
            segments_data (list): 包含多个音频片段的列表，每个片段应该有 'text' 字段。

        Returns:
            list: 包含分析结果的列表，每个元素对应一个片段的分类结果。
        """
        if not segments_data:
            logger.warning("没有提供音频片段数据。")
            return []

        from streamlit_app.config.config import get_semantic_type_definitions, DEFAULT_SEMANTIC_SEGMENT_TYPES

        # 动态获取语义类型定义
        semantic_definitions = get_semantic_type_definitions()
        available_types = DEFAULT_SEMANTIC_SEGMENT_TYPES

        logger.info(f"开始分析 {len(segments_data)} 个音频片段，语义类型数量: {len(available_types)}")

        results = []
        for i, segment in enumerate(segments_data):
            segment_text = segment.get('text', '')
            if not segment_text:
                logger.warning(f"片段 {i} 没有文本内容，跳过分析。")
                results.append({
                    'segment_index': i,
                    'text': segment_text,
                    'semantic_type': '其他',
                    'confidence': 0.0,
                    'analysis_result': '文本为空'
                })
                continue

            try:
                # 构建动态的类型描述
                type_descriptions = []
                for type_name in available_types:
                    definition = semantic_definitions.get(type_name, {})
                    description = definition.get('description', f'{type_name}类型的内容')
                    keywords = definition.get('keywords', [])
                    examples = definition.get('examples', [])
                    
                    # 组合描述信息
                    full_description = f"{type_name}: {description}"
                    if keywords:
                        full_description += f" 关键词：{', '.join(keywords[:3])}"  # 只显示前3个关键词
                    if examples:
                        # 🆕 显示多个示例，用分号分隔，最多显示3个
                        example_text = "; ".join(examples[:3])
                        full_description += f" 示例：{example_text}"
                    
                    type_descriptions.append(full_description)

                # 构建系统提示词
                system_prompt = f"""你是一个专业的视频内容分析师，擅长将母婴奶粉营销视频的文本片段归类到合适的语义类型中。

可选的语义类型及其定义：
{chr(10).join([f"{i+1}. {desc}" for i, desc in enumerate(type_descriptions)])}

请根据文本内容，选择最合适的语义类型。如果文本内容不明确或难以归类，请选择"其他"。

返回格式要求：
- semantic_type: 选择的语义类型名称（必须是上述类型之一）
- confidence: 置信度（0.0-1.0之间的浮点数）
- reasoning: 简短的分析理由

请以JSON格式返回结果。"""

                user_prompt = f"请分析以下文本片段的语义类型：\n\n文本内容：{segment_text}"

                # 调用DeepSeek API
                response = self.deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1,
                    max_tokens=500,
                    stream=False
                )

                response_content = response.choices[0].message.content.strip()
                logger.debug(f"片段 {i} DeepSeek API 响应: {response_content}")

                # 解析JSON响应
                try:
                    result_json = json.loads(response_content)
                    semantic_type = result_json.get('semantic_type', '其他')
                    confidence = float(result_json.get('confidence', 0.5))
                    reasoning = result_json.get('reasoning', '自动分析')

                    # 验证语义类型是否在可选范围内
                    if semantic_type not in available_types:
                        logger.warning(f"片段 {i} 返回了无效的语义类型 '{semantic_type}'，设置为'其他'")
                        semantic_type = '其他'
                        confidence = 0.3

                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"片段 {i} JSON解析失败: {e}, 原始响应: {response_content}")
                    # 尝试简单的文本匹配作为后备
                    semantic_type = self._fallback_semantic_classification(segment_text, available_types, semantic_definitions)
                    confidence = 0.4
                    reasoning = "JSON解析失败，使用后备分类"

                results.append({
                    'segment_index': i,
                    'text': segment_text,
                    'semantic_type': semantic_type,
                    'confidence': confidence,
                    'analysis_result': reasoning
                })

                logger.info(f"片段 {i} 分析完成: {semantic_type} (置信度: {confidence:.2f})")

            except Exception as e:
                logger.error(f"分析片段 {i} 时发生错误: {e}")
                results.append({
                    'segment_index': i,
                    'text': segment_text,
                    'semantic_type': '其他',
                    'confidence': 0.0,
                    'analysis_result': f'分析失败: {str(e)}'
                })

        logger.info(f"完成所有 {len(segments_data)} 个片段的语义分析")
        return results

    def _fallback_semantic_classification(self, text: str, available_types: list, semantic_definitions: dict) -> str:
        """
        后备的语义分类方法，基于关键词匹配
        
        Args:
            text: 要分类的文本
            available_types: 可用的语义类型列表
            semantic_definitions: 语义类型定义字典
            
        Returns:
            str: 分类结果
        """
        text_lower = text.lower()
        
        # 按类型检查关键词匹配
        for type_name in available_types:
            if type_name == '其他':
                continue
                
            definition = semantic_definitions.get(type_name, {})
            keywords = definition.get('keywords', [])
            
            # 检查关键词匹配
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    logger.info(f"后备分类：基于关键词 '{keyword}' 将文本分类为 '{type_name}'")
                    return type_name
        
        # 如果没有匹配到任何关键词，返回"其他"
        return '其他'

def main_analysis_pipeline(video_path, target_audience=None, product_type=None, selling_points_config_representation=None, additional_info=None):
    """
    完整的视频分析流水线，将视频分段和意图分析结合起来
    
    Args:
        video_path: 视频文件路径
        target_audience: 目标人群，默认为None
        product_type: 产品类型，默认为None (此参数在此简化流程中不再用于产品类型匹配)
        selling_points_config_representation: 用于缓存控制的产品卖点表示 (例如元组)
        additional_info: 附加的分析信息，默认为None
        
    Returns:
        Tuple: (分析结果列表, 完整的转录数据字典) 或 ([], None) 如果失败
    """
    from streamlit_app.modules.data_process.video_segmenter import segment_video # 保持局部导入
    
    full_transcript_data = None # 初始化
    analysis_results_placeholder = [] # 返回一个空列表作为分析结果的占位符
    try:
        # 1. 视频分段，并获取完整转录数据 (包括SRT文件路径和内容)
        segments, full_transcript_data = segment_video(video_path)
        
        if not segments:
            logger.warning(f"视频分段结果为空或获取转录数据失败: {video_path}")
            # 即使分段为空，也返回转录数据和空分析结果占位符
            return analysis_results_placeholder, full_transcript_data 
        
        # 产品类型识别和目标人群识别将主要在 streamlit_app/app.py 中通过分析SRT文件内容进行。
        # IntentAnalyzer.analyze_segment 中的产品类型和人群匹配逻辑已大部分移除或不再核心。
        # 此处不再调用 analyze_video_segments 进行基于关键词的卖点等分析，
        # 因为其结果（如 matched_selling_points）当前未在UI上直接使用。
        # 如果将来需要这些详细的基于片段的分析，可以重新启用或调整此部分。
        
        # 当前，main_analysis_pipeline 的核心输出是 segments (用于UI显示分段视频)
        # 和 full_transcript_data (用于 app.py 中提取SRT路径/内容进行LLM分析)
        
        # 直接使用 segments 作为一种形式的 "分析结果" 返回给 app.py，
        # app.py 主要消费的是 segments 列表本身，而不是内部更细致的匹配字段。
        # 或者，如果 app.py 仅需 segments 和 full_transcript_data，可以让 analysis_results 为空。
        # 为了保持返回结构一致性，但表明这部分分析被跳过，我们返回原始的segments
        # 或者一个明确的空列表。考虑到 app.py 中对 analysis_results 的迭代，返回 segments 更合适。
        
        # logger.info(f"视频片段意图分析完成，找到 {len(analysis_results)} 个匹配片段 for {video_path}")
        # 返回 segments (语义分段结果) 和 full_transcript_data
        # app.py 将基于 segments 迭代显示，并使用 full_transcript_data 进行LLM分析
        return segments, full_transcript_data
    except Exception as e:
        logger.error(f"视频分析流水线执行失败 ({video_path}): {str(e)}")
        return analysis_results_placeholder, full_transcript_data # 尽量返回转录数据，即使后续分析失败 