import os
import json
import logging
import time
import traceback
import streamlit as st
from datetime import datetime
from typing import Dict, Any

from streamlit_app.config.config import get_config, TARGET_GROUPS, SELLING_POINTS, PRODUCT_TYPES, BRAND_KEYWORDS, get_semantic_segment_types, get_semantic_type_definitions, DEFAULT_SEMANTIC_SEGMENT_TYPES
from sentence_transformers import SentenceTransformer, util
import torch

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class SemanticAnalyzer:
    """è¯­ä¹‰åˆ†æå™¨ï¼Œä½¿ç”¨DeepSeekæ¨¡å‹è¿›è¡Œè¯­ä¹‰ç†è§£"""
    
    def __init__(self, api_key=None, base_url=None, model="deepseek-chat"):
        """åˆå§‹åŒ–è¯­ä¹‰åˆ†æå™¨"""
        # è·å–é…ç½®ä¿¡æ¯
        config = get_config()
        self.api_key = api_key or config.get("api_key") or os.environ.get("DEEPSEEK_API_KEY")
        self.base_url = base_url or "https://api.deepseek.com"
        self.model = model
        
        # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        if not self.api_key:
            logger.warning("ç¼ºå°‘APIå¯†é’¥ï¼Œä½¿ç”¨æµ‹è¯•æ¨¡å¼")
            self.api_key = "sk-test-api-key-for-development-only"
        
        # æ¯å©´å¥¶ç²‰é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å’ŒåŒä¹‰è¯æ˜ å°„
        self.domain_terms = {
            "å…ç–«åŠ›": ["è‡ªå¾¡åŠ›", "æŠµæŠ—åŠ›", "æŠµæŠ—èƒ½åŠ›", "ä¿æŠ¤åŠ›", "è‡ªèº«ä¿æŠ¤åŠ›", "æŠµå¾¡èƒ½åŠ›"],
            "è¿‡æ•": ["è¿‡æ•ååº”", "è¿‡æ•ç°è±¡", "æ•æ„Ÿ", "é£Ÿç‰©è¿‡æ•", "è›‹ç™½è¿‡æ•"],
            "ä¾¿ç§˜": ["å¤§ä¾¿å¹²", "æ’ä¾¿å›°éš¾", "æ’ä¾¿ä¸ç•…", "è‚ èƒƒä¸é€‚"],
            "è…¹æ³»": ["æ‹‰è‚šå­", "è‚šå­ä¸èˆ’æœ", "æ°´ä¾¿", "æ¶ˆåŒ–ä¸è‰¯", "æ¶ˆåŒ–é—®é¢˜"],
            "é…æ–¹": ["å¥¶ç²‰é…æ–¹", "ç‰›å¥¶é…æ–¹", "è¥å…»é…æ–¹", "ç‰¹æ®Šé…æ–¹"],
            "æ¯ä¹³": ["æ¯ä¹³å–‚å…»", "çº¯æ¯ä¹³", "æ¯å¥¶"],
            "å¥¶ç²‰": ["é…æ–¹å¥¶", "ç‰›å¥¶ç²‰", "å©´å„¿å¥¶ç²‰", "é…æ–¹å¥¶ç²‰"],
            "æ··åˆå–‚å…»": ["æ··å–‚", "æ··åˆå–‚å¥¶", "æ¯ä¹³+å¥¶ç²‰", "æ··å…»"]
        }
        
        logger.info(f"è¯­ä¹‰åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model}")
        
        # å¯¼å…¥requestsåº“
        try:
            import requests
            self.requests = requests
        except ImportError:
            logger.error("è¯·å®‰è£…requestsåº“: pip install requests")
            raise ImportError("ç¼ºå°‘å¿…è¦çš„requestsåº“ï¼Œè¯·ä½¿ç”¨pip install requestså®‰è£…")
        
        # æ£€æŸ¥é…ç½®æ˜¯å¦ç¦ç”¨äº†sentence-transformer
        self.similarity_model = None
        use_sentence_transformer = config.get("use_sentence_transformer", False)
        
        if not use_sentence_transformer:
            logger.info("æ ¹æ®é…ç½®å·²ç¦ç”¨SentenceTransformeræ¨¡å‹ï¼Œå°†ä½¿ç”¨difflibè¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—")
            return
            
        # åˆå§‹åŒ–sentence-transformeræ¨¡å‹ï¼ˆä»…åœ¨é…ç½®å…è®¸çš„æƒ…å†µä¸‹ï¼‰
        try:
            # ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„å¤šè¯­è¨€æˆ–ä¸­æ–‡æ¨¡å‹ï¼Œç¡®ä¿èƒ½å¤„ç†ä¸­æ–‡
            # 'paraphrase-multilingual-MiniLM-L12-v2' æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œæ”¯æŒå¤šç§è¯­è¨€åŒ…æ‹¬ä¸­æ–‡
            self.similarity_model_name = 'paraphrase-multilingual-MiniLM-L12-v2'
            
            # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ä¸‹è½½æ¨¡å‹å¯¼è‡´çš„è¿æ¥é—®é¢˜
            os.environ['HF_DATASETS_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡ï¼Œä¸é˜»æ­¢ç¨‹åºè¿è¡Œ
            try:
                from sentence_transformers import SentenceTransformer
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
                local_model_path = config.get("sentence_transformer_local_path")
                if local_model_path and os.path.exists(local_model_path):
                    logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
                    self.similarity_model = SentenceTransformer(self.similarity_model_name, cache_folder=local_model_path, device=device)
                else:
                    logger.warning(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {local_model_path}ï¼Œå°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½")
                    self.similarity_model = SentenceTransformer(self.similarity_model_name, device=device)
                logger.info(f"æ–‡æœ¬ç›¸ä¼¼åº¦æ¨¡å‹ {self.similarity_model_name} åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–æ–‡æœ¬ç›¸ä¼¼åº¦æ¨¡å‹å¤±è´¥: {e}")
                logger.warning("å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—")
                self.similarity_model = None
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æ–‡æœ¬ç›¸ä¼¼åº¦æ¨¡å‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            self.similarity_model = None
    
    def expand_query_with_synonyms(self, query):
        """ä½¿ç”¨åŒä¹‰è¯æ‰©å±•æŸ¥è¯¢å†…å®¹"""
        expanded_terms = []
        for term, synonyms in self.domain_terms.items():
            if term in query:
                expanded_terms.append(term)
                expanded_terms.extend(synonyms)
            else:
                for synonym in synonyms:
                    if synonym in query:
                        expanded_terms.append(term)
                        expanded_terms.extend(synonyms)
                        break
        
        return list(set(expanded_terms))  # å»é‡
    
    def analyze_semantic_match(self, transcript, query, context=None):
        """
        åˆ†ææ–‡æœ¬ä¸æŸ¥è¯¢çš„è¯­ä¹‰åŒ¹é…åº¦
        
        Args:
            transcript: å¾…åˆ†æçš„æ–‡æœ¬
            query: ç”¨æˆ·æŸ¥è¯¢å†…å®¹
            context: é™„åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ (ç›®æ ‡äººç¾¤ã€äº§å“ç±»å‹ç­‰)
            
        Returns:
            åŒ¹é…åˆ†æ•°å’ŒåŒ¹é…ç»†èŠ‚
        """
        if not query:
            return 1.0, {"reason": "æ— æŸ¥è¯¢å†…å®¹ï¼Œé»˜è®¤åŒ¹é…"}
        
        # ä½¿ç”¨åŒä¹‰è¯æ‰©å±•æŸ¥è¯¢
        expanded_terms = self.expand_query_with_synonyms(query)
        expanded_query_info = f"æ‰©å±•å…³é”®è¯: {', '.join(expanded_terms)}" if expanded_terms else ""
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ¯å©´è¡Œä¸šä¸“ä¸šè§†é¢‘å†…å®¹åˆ†æä¸“å®¶ï¼Œç‰¹åˆ«ç²¾é€šå©´å¹¼å„¿è¥å…»ã€å–‚å…»å’Œå¥¶ç²‰äº§å“ä¿¡æ¯ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ·±å…¥ç†è§£è§†é¢‘å†…å®¹ä¸ç”¨æˆ·æŸ¥è¯¢ä¹‹é—´çš„è¯­ä¹‰å…³è”ï¼Œæä¾›ä¸“ä¸šçš„åŒ¹é…åˆ†æã€‚

ä½ éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ :
1. å†…å®¹ç›´æ¥ç›¸å…³æ€§ - è§†é¢‘æ˜¯å¦æ˜ç¡®è®¨è®ºäº†æŸ¥è¯¢ä¸»é¢˜
2. è¯­ä¹‰å…³è”æ€§ - è§†é¢‘å†…å®¹æ˜¯å¦åŒ…å«ä¸æŸ¥è¯¢ç›¸å…³çš„æ¦‚å¿µã€æœ¯è¯­æˆ–åŒä¹‰è¡¨è¾¾
3. è§£å†³æ–¹æ¡ˆåŒ¹é…åº¦ - è§†é¢‘æ˜¯å¦æä¾›äº†ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„è§£å†³æ–¹æ¡ˆæˆ–å»ºè®®
4. ç›®æ ‡äººç¾¤é€‚é…æ€§ - è§†é¢‘å†…å®¹æ˜¯å¦é€‚åˆæŸ¥è¯¢æ‰€é’ˆå¯¹çš„äººç¾¤
5. äº§å“é€‚é…æ€§ - è§†é¢‘ä¸­è®¨è®ºçš„äº§å“æ˜¯å¦ç¬¦åˆæŸ¥è¯¢éœ€æ±‚

ä½ åº”å½“ç†è§£æ¯å©´è¡Œä¸šçš„ä¸“ä¸šæœ¯è¯­å’ŒåŒä¹‰è¯,ä¾‹å¦‚:
- "å…ç–«åŠ›"ç›¸å…³: è‡ªå¾¡åŠ›ã€æŠµæŠ—åŠ›ã€ä¿æŠ¤åŠ›ç­‰
- "é…æ–¹"ç›¸å…³: å¥¶ç²‰é…æ–¹ã€è¥å…»é…æ–¹ã€ç‰¹æ®Šé…æ–¹ç­‰
- "æ¶ˆåŒ–é—®é¢˜"ç›¸å…³: ä¾¿ç§˜ã€è…¹æ³»ã€è‚ èƒƒä¸é€‚ã€æ’ä¾¿ç­‰

å¯¹äºå“ç‰Œæœ¯è¯­,è¦ç‰¹åˆ«æ³¨æ„:
- "å¯èµ‹æ°´å¥¶": æŒ‡å¯èµ‹å“ç‰Œçš„å³é¥®å‹æ¶²æ€å¥¶
- "å¯èµ‹è•´æ·³": æŒ‡å¯èµ‹å“ç‰Œçš„ç‰¹å®šç³»åˆ—å¥¶ç²‰
- "HMO": æŒ‡äººä¹³ä½èšç³–,æ˜¯ä¸€ç§é‡è¦çš„æ¯ä¹³æˆåˆ†
- "A2è›‹ç™½": æŒ‡ä¸€ç§ç‰¹å®šçš„ç‰›å¥¶è›‹ç™½ç±»å‹
"""
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_str = ""
        if context:
            if context.get("target_audience"):
                context_str += f"ç›®æ ‡äººç¾¤: {context['target_audience']}\n"
            if context.get("product_type") and context.get("product_type") != "-":
                context_str += f"äº§å“ç±»å‹: {context['product_type']}\n"
            if context.get("selling_points"):
                context_str += f"äº§å“å–ç‚¹: {', '.join(context['selling_points'])}\n"
        
        user_prompt = f"""
è¯·æ·±å…¥åˆ†æä»¥ä¸‹è§†é¢‘å†…å®¹è½¬å½•æ–‡æœ¬ï¼Œè¯„ä¼°å…¶ä¸ç”¨æˆ·æŸ¥è¯¢æ„å›¾çš„åŒ¹é…ç¨‹åº¦:

ç”¨æˆ·æŸ¥è¯¢: {query}
{expanded_query_info if expanded_query_info else ""}

ç›¸å…³ä¸Šä¸‹æ–‡:
{context_str if context_str else "æ— ç‰¹å®šä¸Šä¸‹æ–‡"}

è§†é¢‘è½¬å½•æ–‡æœ¬:
{transcript}

è¯·ä»å¤šä¸ªç»´åº¦è¯„ä¼°åŒ¹é…åº¦ï¼Œè¿”å›ä¸€ä¸ªè¯¦ç»†çš„JSONæ ¼å¼åˆ†æç»“æœ:
{{
  "match_score": 0.85,  // æ€»ä½“åŒ¹é…åˆ†æ•°ï¼Œ0-1ä¹‹é—´
  "dimension_scores": {{
    "content_relevance": 0.9,  // å†…å®¹ç›¸å…³æ€§
    "semantic_coherence": 0.8,  // è¯­ä¹‰ä¸€è‡´æ€§
    "solution_fit": 0.7,  // è§£å†³æ–¹æ¡ˆåŒ¹é…åº¦
    "audience_fit": 0.9,  // ç›®æ ‡äººç¾¤åŒ¹é…åº¦
    "product_fit": 0.85   // äº§å“åŒ¹é…åº¦
  }},
  "matching_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],  // åŒ¹é…åˆ°çš„å…³é”®è¯
  "reason": "è¯¦ç»†è¯´æ˜åŒ¹é…åŸå› ï¼ŒåŒ…æ‹¬ä¸ºä»€ä¹ˆè¿™æ®µå†…å®¹ä¸æŸ¥è¯¢ç›¸å…³",
  "recommendation": "å¯¹äºè¿™æ®µå†…å®¹æ˜¯å¦æ¨èç»™ç”¨æˆ·çš„ä¸“ä¸šå»ºè®®"
}}

åªè¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼Œä¸åŒ…å«å…¶ä»–è¯´æ˜ã€‚ç¡®ä¿åˆ†æç´§å¯†ç»“åˆæ¯å©´å’Œå¥¶ç²‰é¢†åŸŸçš„ä¸“ä¸šç†è§£ã€‚
"""

        try:
            # ç¡®ä¿APIå¯†é’¥å·²è®¾ç½®
            if not self.api_key:
                logger.warning("ç¼ºå°‘APIå¯†é’¥ï¼Œä½¿ç”¨æµ‹è¯•æ¨¡å¼")
                self.api_key = "sk-test-api-key-for-development-only"
                
            # å®é™…è°ƒç”¨API
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            # æ„å»ºæ¶ˆæ¯
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,  # ä½æ¸©åº¦ä½¿è¾“å‡ºæ›´ç¡®å®šæ€§
                "max_tokens": 1500
            }
            
            # è°ƒç”¨API
            response = self.requests.post(
                f"{self.base_url}/v1/chat/completions", 
                headers=headers, 
                json=data,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            # è§£æç»“æœ
            content = result['choices'][0]['message']['content']
            logger.info("DeepSeek APIè°ƒç”¨æˆåŠŸ")
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # ç§»é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°
                if "```json" in content and "```" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                result_json = json.loads(content)
                match_score = result_json.get("match_score", 0.0)
                
                # æå–å…¶ä»–æœ‰ç”¨ä¿¡æ¯
                dimension_scores = result_json.get("dimension_scores", {})
                matching_keywords = result_json.get("matching_keywords", [])
                reason = result_json.get("reason", "")
                recommendation = result_json.get("recommendation", "")
                
                details = {
                    "dimension_scores": dimension_scores,
                    "matching_keywords": matching_keywords,
                    "reason": reason,
                    "recommendation": recommendation
                }
                
                return match_score, details
                
            except json.JSONDecodeError as e:
                logger.error(f"æ— æ³•è§£ææ¨¡å‹å“åº”ä¸ºJSON: {content}")
                logger.error(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                return 0.5, {"reason": f"è§£æé”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°ã€‚åŸå§‹å“åº”: {content[:100]}..."}
                
        except Exception as e:
            logger.error(f"è°ƒç”¨DeepSeekæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
        å¦‚æœå·²åˆå§‹åŒ– SentenceTransformer æ¨¡å‹åˆ™ä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨ difflib è¿›è¡Œè®¡ç®—ã€‚
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        if not text1 or not text2:
            return 0.0
        
        # å¦‚æœsentence-transformeræ¨¡å‹å¯ç”¨ï¼Œä½¿ç”¨å®ƒ
        if self.similarity_model:
            try:
                from sentence_transformers import util
                embeddings = self.similarity_model.encode([text1, text2], convert_to_tensor=True)
                cosine_scores = util.pytorch_cos_sim(embeddings[0], embeddings[1])
                similarity = cosine_scores.item()
                logger.debug(f"ä½¿ç”¨SentenceTransformerè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦: '{text1[:50]}...' vs '{text2[:50]}...' = {similarity:.4f}")
                return similarity
            except Exception as e:
                logger.error(f"ä½¿ç”¨sentence-transformerè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
                logger.info("å›é€€åˆ°difflibè®¡ç®—ç›¸ä¼¼åº¦...")
                # å¦‚æœå‡ºé”™ï¼Œå›é€€åˆ°åŸºæœ¬æ–¹æ³•
        else:
            logger.debug("SentenceTransformeræ¨¡å‹æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦...")
        
        # åŸºæœ¬ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
        try:
            # å¯¼å…¥difflibåº“ç”¨äºå­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—
            import difflib
            # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
            similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
            logger.debug(f"ä½¿ç”¨difflibè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦: '{text1[:50]}...' vs '{text2[:50]}...' = {similarity:.4f}")
            return similarity
        except Exception as e:
            logger.error(f"ä½¿ç”¨difflibè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 0.0

    def _chat_completion(self, messages, model=None):
        """
        è°ƒç”¨DeepSeek APIæ‰§è¡ŒèŠå¤©è¯·æ±‚ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
            model: è¦ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„æ¨¡å‹

        Returns:
            APIå“åº”çš„JSONå¯¹è±¡
        """
        if not self.api_key:
            logger.warning("_chat_completion: ç¼ºå°‘APIå¯†é’¥ï¼Œä½¿ç”¨æµ‹è¯•æ¨¡å¼")
            self.api_key = "sk-test-api-key-for-development-only"
            
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": model or self.model,
            "messages": messages,
            "temperature": 0.1,  # ä½æ¸©åº¦ä½¿è¾“å‡ºæ›´ç¡®å®šæ€§
            "max_tokens": 1500
        }
        
        try:
            response = self.requests.post(
                f"{self.base_url}/v1/chat/completions", 
                headers=headers, 
                json=data,
                timeout=45
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"è°ƒç”¨DeepSeek APIæ—¶å‡ºé”™: {str(e)}")
            # è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„æœ€å°å“åº”ï¼Œä»¥ä¾¿è°ƒç”¨ä»£ç å¯ä»¥ç»§ç»­è¿è¡Œ
            return {
                "choices": [
                    {
                        "message": {
                            "content": '{"target_audience": [], "product_type": []}'
                        }
                    }
                ]
            }

    def analyze_video_summary(self, full_transcript: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨DeepSeekæ¨¡å‹åˆ†æè§†é¢‘å®Œæ•´è½¬å½•æ–‡æœ¬ï¼Œæå–ç›®æ ‡äººç¾¤ã€‚

        Args:
            full_transcript: å®Œæ•´çš„è§†é¢‘è½¬å½•æ–‡æœ¬ã€‚

        Returns:
            ä¸€ä¸ªåŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œä¾‹å¦‚:
            {
                "target_audience": ["äººç¾¤1", "äººç¾¤2"]
            }
            å¦‚æœåˆ†æå¤±è´¥åˆ™è¿”å›ç©ºå­—å…¸ã€‚
        """
        if not full_transcript:
            logger.warning("å®Œæ•´çš„è½¬å½•æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè§†é¢‘æ‘˜è¦åˆ†æã€‚")
            return {}

        import json # ç¡®ä¿å¯¼å…¥json
        # ä»configä¸­å¯¼å…¥TARGET_GROUPSï¼Œä»¥ä¾¿åœ¨æç¤ºè¯ä¸­ä½¿ç”¨
        from streamlit_app.config.config import TARGET_GROUPS

        target_groups_json_array_for_prompt = json.dumps(TARGET_GROUPS, ensure_ascii=False)

        # ä½¿ç”¨f-stringç»“åˆå¤šè¡Œå­—ç¬¦ä¸²æ¥æ„å»ºSYSTEM_PROMPT_TEMPLATE
        # æ³¨æ„ï¼šåœ¨f-stringä¸­è¦è¡¨ç¤ºå­—é¢é‡çš„èŠ±æ‹¬å· { æˆ– }ï¼Œéœ€è¦ä½¿ç”¨åŒèŠ±æ‹¬å· {{ æˆ– }}
        SYSTEM_PROMPT_TEMPLATE = f'''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¯å©´è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿ç²¾ç¡®è¯†åˆ«è§†é¢‘çš„ç›®æ ‡äººç¾¤ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®è§†é¢‘è½¬å½•æ–‡æœ¬ï¼Œä»é¢„å®šä¹‰çš„ç›®æ ‡äººç¾¤åˆ—è¡¨ä¸­é€‰æ‹©**å”¯ä¸€ä¸€ä¸ªæœ€åŒ¹é…**çš„åˆ†ç±»ã€‚

ç›®æ ‡äººç¾¤åˆ—è¡¨åŠå…¶è¯¦ç»†å®šä¹‰ï¼š{target_groups_json_array_for_prompt}

**äººç¾¤åˆ¤æ–­æŒ‡å¯¼åŸåˆ™ï¼š**

1. **å­•æœŸå¦ˆå¦ˆ**ï¼š
   - å…³é”®è¯ï¼šæ€€å­•ã€å­•æœŸã€å¾…äº§åŒ…ã€äº§æ£€ã€å»ºæ¡£ã€å‡†å¦ˆå¦ˆã€å¸è´§ã€åˆ†å¨©ã€ç”Ÿäº§ã€äº§ç§‘
   - åœºæ™¯ï¼šè®¨è®ºå­•æœŸè¥å…»ã€å¾…äº§å‡†å¤‡ã€äº§å‰äº§åæŠ¤ç†ã€æ–°ç”Ÿå„¿å–‚å…»å‡†å¤‡
   - æ—¶é—´èŠ‚ç‚¹ï¼šæ€€å­•æœŸé—´ã€åˆ†å¨©å‰åã€äº§ååˆæœŸï¼ˆ0-42å¤©ï¼‰

2. **äºŒèƒå¦ˆå¦ˆ**ï¼š
   - å…³é”®è¯ï¼šäºŒèƒã€è€å¤§ã€è€äºŒã€ä¸¤ä¸ªå­©å­ã€å¤§å®ã€äºŒå®ã€å†æ¬¡æ€€å­•
   - åœºæ™¯ï¼šæ¯”è¾ƒä¸¤èƒç»éªŒã€å¤šå­©å­å…»è‚²ã€äºŒèƒå¤‡å­•/æ€€å­•

3. **æ··å…»å¦ˆå¦ˆ**ï¼š
   - å…³é”®è¯ï¼šæ··åˆå–‚å…»ã€æ··å–‚ã€äº²å–‚ã€å¥¶ç²‰æ··åˆã€æ¯ä¹³ä¸è¶³ã€å¥¶é‡ä¸å¤Ÿ
   - åœºæ™¯ï¼šæ¯ä¹³ä¸å¥¶ç²‰ç»“åˆå–‚å…»ã€å¥¶æ°´ä¸è¶³è¡¥å……

4. **æ–°æ‰‹çˆ¸å¦ˆ**ï¼š
   - å…³é”®è¯ï¼šæ–°æ‰‹ã€æ²¡æœ‰ç»éªŒã€ç¬¬ä¸€æ¬¡ã€æ–°ç”Ÿå„¿ã€ä¸çŸ¥é“æ€ä¹ˆã€å­¦ä¹ ã€åˆæ¬¡
   - åœºæ™¯ï¼šåˆä¸ºçˆ¶æ¯ã€ç¼ºä¹è‚²å„¿ç»éªŒã€å­¦ä¹ å–‚å…»çŸ¥è¯†
   - åŒ…å«ï¼šæ–°æ‰‹çˆ¸çˆ¸ã€æ–°æ‰‹å¦ˆå¦ˆã€åˆæ¬¡è‚²å„¿çš„çˆ¶æ¯

5. **è´µå¦‡å¦ˆå¦ˆ**ï¼š
   - å…³é”®è¯ï¼šé«˜ç«¯ã€å¥¢åã€ç²¾è‡´ã€å“è´¨ã€è´µã€é«˜ä»·ã€è¿›å£ã€é¡¶çº§
   - åœºæ™¯ï¼šè¿½æ±‚é«˜å“è´¨äº§å“ã€æ³¨é‡å“ç‰Œæ¡£æ¬¡ã€æ¶ˆè´¹èƒ½åŠ›å¼º

**ä¼˜å…ˆçº§åˆ¤æ–­è§„åˆ™ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š**
1. å¦‚æœåŒæ—¶åŒ¹é…å¤šä¸ªäººç¾¤ï¼ŒæŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§é€‰æ‹©ï¼š
   - "äºŒèƒå¦ˆå¦ˆ" > "å­•æœŸå¦ˆå¦ˆ" > "æ··å…»å¦ˆå¦ˆ" > "è´µå¦‡å¦ˆå¦ˆ" > "æ–°æ‰‹çˆ¸å¦ˆ"
2. å¦‚æœæåˆ°"åˆšç”Ÿå®Œ"ã€"äº§å"ã€"æ–°ç”Ÿå®å®"ã€"å‡ºç”Ÿå"ç­‰äº§åæ—©æœŸå…³é”®è¯ï¼Œä¼˜å…ˆè€ƒè™‘"å­•æœŸå¦ˆå¦ˆ"
3. å¦‚æœæ˜ç¡®æåˆ°"äºŒèƒ"ã€"è€å¤§è€äºŒ"ç­‰å¤šå­©ç»éªŒï¼Œä¼˜å…ˆé€‰æ‹©"äºŒèƒå¦ˆå¦ˆ"
4. å¦‚æœæ˜ç¡®æåˆ°"æ··åˆå–‚å…»"ã€"å¥¶æ°´ä¸è¶³"ç­‰ï¼Œä¼˜å…ˆé€‰æ‹©"æ··å…»å¦ˆå¦ˆ"
5. å¦‚æœå¼ºè°ƒ"é«˜ç«¯"ã€"å¥¢å"ç­‰å“è´¨è¿½æ±‚ï¼Œä¼˜å…ˆé€‰æ‹©"è´µå¦‡å¦ˆå¦ˆ"
6. å…¶ä»–æƒ…å†µæˆ–æ— æ˜ç¡®ç‰¹å¾æ—¶ï¼Œé€‰æ‹©"æ–°æ‰‹çˆ¸å¦ˆ"

**é‡è¦è¦æ±‚ï¼š**
- **å¿…é¡»ä¸”åªèƒ½**é€‰æ‹©ä¸€ä¸ªæœ€åŒ¹é…çš„äººç¾¤
- ä¸å…è®¸è¿”å›å¤šä¸ªäººç¾¤æˆ–ç©ºæ•°ç»„
- å¿…é¡»åŸºäºå†…å®¹ç‰¹å¾è¿›è¡Œåˆ¤æ–­ï¼Œä¸èƒ½éšæ„é€‰æ‹©

è¾“å‡ºæ ¼å¼ï¼š
{{{{
  "target_audience": "ä»ä¸Šè¿°åˆ—è¡¨ä¸­é€‰æ‹©çš„å”¯ä¸€ä¸€ä¸ªäººç¾¤åˆ†ç±»"
}}}}
'''

        # å‡†å¤‡ç”¨æˆ·æç¤ºæ¨¡æ¿
        user_prompt = f"""è¯·ä»”ç»†åˆ†æä»¥ä¸‹è§†é¢‘è½¬å½•æ–‡æœ¬ï¼Œè¯†åˆ«å‡º**å”¯ä¸€ä¸€ä¸ªæœ€åŒ¹é…**çš„ç›®æ ‡äººç¾¤ï¼š

--- è½¬å½•æ–‡æœ¬å¼€å§‹ ---
{full_transcript}
--- è½¬å½•æ–‡æœ¬ç»“æŸ ---

åˆ†ææ­¥éª¤ï¼š
1. ä»”ç»†é˜…è¯»è½¬å½•æ–‡æœ¬ï¼Œè¯†åˆ«å…³é”®è¯å’Œåœºæ™¯æè¿°
2. å¯¹ç…§äººç¾¤åˆ¤æ–­æŒ‡å¯¼åŸåˆ™ï¼Œæ‰¾å‡ºæœ€åŒ¹é…çš„äººç¾¤ç‰¹å¾
3. å¦‚æœåŒ¹é…å¤šä¸ªäººç¾¤ï¼ŒæŒ‰ç…§ä¼˜å…ˆçº§è§„åˆ™é€‰æ‹©æœ€é‡è¦çš„ä¸€ä¸ª
4. ä»ç›®æ ‡äººç¾¤åˆ—è¡¨ä¸­é€‰æ‹©**å”¯ä¸€ä¸€ä¸ª**æœ€åˆé€‚çš„åˆ†ç±»
5. ç¡®ä¿å¿…é¡»é€‰æ‹©ä¸€ä¸ªäººç¾¤ï¼Œä¸èƒ½è¿”å›ç©ºå€¼

é‡è¦æé†’ï¼š
- åªèƒ½è¿”å›ä¸€ä¸ªç›®æ ‡äººç¾¤ï¼Œä¸èƒ½è¿”å›å¤šä¸ª
- å¿…é¡»æ˜¯é¢„å®šä¹‰åˆ—è¡¨ä¸­çš„äººç¾¤åç§°
- åŸºäºå†…å®¹ç‰¹å¾åšå‡ºæœ€ä½³åˆ¤æ–­

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼Œç¡®ä¿target_audienceå­—æ®µæ˜¯å•ä¸ªå­—ç¬¦ä¸²å€¼ã€‚
"""

        # å‘é€åˆ†æè¯·æ±‚
        try:
            response = self._chat_completion(
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT_TEMPLATE},
                    {"role": "user", "content": user_prompt}
                ],
                model="deepseek-chat"
            )
            
            # ä»å“åº”ä¸­è·å–JSONæ ¼å¼ç»“æœ
            if response and "choices" in response and response["choices"]:
                result_text = response["choices"][0].get("message", {}).get("content", "")
                logger.debug(f"DeepSeek API (analyze_video_summary) åŸå§‹å“åº”: {result_text[:500]}...")
                
                # å°è¯•æå–å’Œè§£æJSONæ•°æ®
                import re
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # å¦‚æœæ²¡æœ‰Markdownä»£ç å—ï¼Œåˆ™å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
                    json_str = result_text
                
                try:
                    result_dict = json.loads(json_str)
                    # åªè¿”å›ç›®æ ‡äººç¾¤ä¿¡æ¯
                    return {
                        "target_audience": result_dict.get("target_audience", "")
                    }
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {e}, åŸå§‹æ–‡æœ¬: {json_str[:500]}...")
            
                    # å°è¯•ä¿®å¤åŒèŠ±æ‹¬å·é—®é¢˜
                    try:
                        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„å¤šä½™èŠ±æ‹¬å·
                        cleaned_json = json_str.strip()
                        if cleaned_json.startswith('{{') and cleaned_json.endswith('}}'):
                            cleaned_json = cleaned_json[1:-1]  # ç§»é™¤å¤–å±‚èŠ±æ‹¬å·
                            logger.info(f"å°è¯•ä¿®å¤åŒèŠ±æ‹¬å·JSONæ ¼å¼: {cleaned_json[:200]}...")
                            result_dict = json.loads(cleaned_json)
                            return {
                                "target_audience": result_dict.get("target_audience", "")
                            }
                    except json.JSONDecodeError as e2:
                        logger.error(f"ä¿®å¤åŒèŠ±æ‹¬å·åä»ç„¶JSONè§£æå¤±è´¥: {e2}")
                    
                    # å¦‚æœJSONè§£æå®Œå…¨å¤±è´¥ï¼Œå°è¯•æ­£åˆ™è¡¨è¾¾å¼æå–ç›®æ ‡äººç¾¤
                    try:
                        import re
                        # å°è¯•åŒ¹é… "target_audience": ["xxx", "yyy"] æ ¼å¼
                        pattern = r'"target_audience"\s*:\s*\[(.*?)\]'
                        match = re.search(pattern, json_str, re.DOTALL)
                        if match:
                            audience_str = match.group(1)
                            # æå–å¼•å·å†…çš„å†…å®¹
                            audience_pattern = r'"([^"]+)"'
                            audiences = re.findall(audience_pattern, audience_str)
                            logger.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–åˆ°ç›®æ ‡äººç¾¤: {audiences}")
                            return {
                                "target_audience": audiences[0] if audiences else ""
                            }
                    except Exception as e3:
                        logger.error(f"æ­£åˆ™è¡¨è¾¾å¼æå–ç›®æ ‡äººç¾¤å¤±è´¥: {e3}")
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
            logger.error(f"æ— æ³•ä»DeepSeekå“åº”ä¸­æå–ç»“æ„åŒ–æ•°æ®: {response}")
            return {}
            
        except Exception as e:
            logger.error(f"è§†é¢‘å†…å®¹åˆ†æå¤±è´¥: {str(e)}")
            return {}

    def segment_transcript_by_intent(self, srt_file_path: str) -> list:
        """
        ä½¿ç”¨DeepSeekæ¨¡å‹å°†SRTæ–‡ä»¶å†…å®¹æŒ‰æ„å›¾åˆ’åˆ†ä¸ºè¯­ä¹‰åŒºå—ã€‚

        Args:
            srt_file_path: SRTå­—å¹•æ–‡ä»¶çš„è·¯å¾„ã€‚

        Returns:
            ä¸€ä¸ªè¯­ä¹‰åŒºå—åˆ—è¡¨ï¼Œæ¯ä¸ªåŒºå—æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«:
            'semantic_type' (str): æ¥è‡ª SEMANTIC_SEGMENT_TYPES çš„ç±»å‹ã€‚
            'text' (str): è¯¥è¯­ä¹‰åŒºå—çš„æ–‡æœ¬å†…å®¹ (ç”±SRTè¡Œæ‹¼æ¥è€Œæˆ)ã€‚
            'asr_matched_text' (str): ä¸ 'text' ç›¸åŒï¼Œå› ä¸ºç›´æ¥æ¥è‡ªSRTã€‚
            'start_time' (float): å¼€å§‹æ—¶é—´ (ç§’)ã€‚
            'end_time' (float): ç»“æŸæ—¶é—´ (ç§’)ã€‚
            å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        if not srt_file_path or not os.path.exists(srt_file_path):
            logger.warning(f"SRTæ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨: {srt_file_path}ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰åˆ†æ®µã€‚")
            return []

        # 1. è¯»å–å¹¶è§£æSRTæ–‡ä»¶
        srt_entries = []
        try:
            with open(srt_file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            srt_blocks_raw = content.split('\n\n')
            entry_id_counter = 1
            for block_raw in srt_blocks_raw:
                lines = block_raw.strip().split('\n')
                if len(lines) >= 2:
                    try:
                        time_line_index = 0
                        if lines[0].isdigit() and len(lines) >=3:
                             time_line_index = 1
                        
                        if '-->' not in lines[time_line_index]:
                            if len(lines) > time_line_index + 1 and '-->' in lines[time_line_index+1]:
                                time_line_index += 1
                            else:
                                logger.warning(f"æ— æ³•åœ¨SRTåŒºå—ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´è¡Œ: {block_raw}")
                                continue

                        time_line = lines[time_line_index]
                        text_lines = lines[time_line_index+1:]
                        start_time_str, end_time_str = time_line.split(' --> ')
                        
                        def srt_time_to_seconds(t_str):
                            h, m, s_ms = t_str.split(':')
                            s, ms = s_ms.split(',')
                            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0

                        start_time_sec = srt_time_to_seconds(start_time_str)
                        end_time_sec = srt_time_to_seconds(end_time_str)
                        text_content = " ".join(text_lines).strip()

                        if text_content:
                            srt_entries.append({
                                "id": entry_id_counter,
                                "start_time": start_time_sec,
                                "end_time": end_time_sec,
                                "text": text_content
                            })
                            entry_id_counter += 1
                    except Exception as e:
                        logger.warning(f"è§£æSRTåŒºå—æ—¶å‡ºé”™: {str(e)}. åŒºå—å†…å®¹: {block_raw}")
                        continue
        except Exception as e:
            logger.error(f"è¯»å–æˆ–è§£æSRTæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return []

            if not srt_entries:
            logger.warning(f"SRTæ–‡ä»¶ {srt_file_path} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å­—å¹•æ¡ç›®ã€‚")
                return []
            
        logger.info(f"æˆåŠŸè§£æSRTæ–‡ä»¶ï¼Œå…± {len(srt_entries)} ä¸ªå­—å¹•æ¡ç›®ã€‚")

        # 2. æ„å»ºç”¨äºLLMåˆ†æçš„æ–‡æœ¬
        srt_text_for_llm = ""
        for entry in srt_entries:
            srt_text_for_llm += f"L{entry['id']}: {entry['text']}\n"

        # 3. è·å–è¯­ä¹‰ç±»å‹å®šä¹‰
        from streamlit_app.config.config import get_semantic_segment_types, get_semantic_type_definitions
        semantic_segment_types = get_semantic_segment_types()
        semantic_definitions = get_semantic_type_definitions()

        type_description_list = []
        for type_name in semantic_segment_types:
            definition = semantic_definitions.get(type_name, {})
            description = definition.get('description', f'{type_name}ç±»å‹çš„å†…å®¹')
            keywords = definition.get('keywords', [])
            examples = definition.get('examples', [])
            
            full_description = f"{type_name}: {description}"
            if keywords:
                full_description += f" å…³é”®è¯ï¼š{', '.join(keywords[:3])}"
            if examples:
                example_text = "; ".join(examples[:3])
                full_description += f" ç¤ºä¾‹ï¼š{example_text}"
            
            type_description_list.append(full_description)

        type_descriptions_formatted_str = "\n".join(type_description_list)

        # 4. è°ƒç”¨DeepSeek APIè¿›è¡Œè¯­ä¹‰åˆ†æ®µ
        try:
            logger.info("å‡†å¤‡è°ƒç”¨DeepSeek APIè¿›è¡ŒåŸºäºSRTçš„è¯­ä¹‰åˆ†æ®µ")
            segments_result = self._call_deepseek_for_srt_segmentation(
                srt_text_for_llm, 
                type_descriptions_formatted_str
            )
            
            if not segments_result or "segments" not in segments_result:
                logger.error("DeepSeek APIè¿”å›çš„åˆ†æ®µç»“æœæ— æ•ˆæˆ–ä¸åŒ…å«segmentsé”®ã€‚")
                return []
            
            llm_segments_info = segments_result.get("segments", [])
            if not llm_segments_info:
                logger.warning("LLMè¿”å›çš„è¯­ä¹‰åŒºå—åˆ—è¡¨ä¸ºç©ºã€‚")
                return []
            
            logger.info(f"æˆåŠŸè·å– {len(llm_segments_info)} ä¸ªåŸºäºSRTçš„è¯­ä¹‰åŒºå—å®šä¹‰")

            # 5. å¤„ç†APIè¿”å›ç»“æœï¼Œè½¬æ¢ä¸ºSegmentæ ¼å¼ï¼Œå¹¶è¿›è¡Œå¥å­å®Œæ•´æ€§è°ƒæ•´
            result_segments = []
            for segment_def in llm_segments_info:
                try:
                    segment_type = segment_def.get("segment_type", "å…¶ä»–")
                    start_line_id = segment_def.get("start_line_id")
                    end_line_id = segment_def.get("end_line_id")

                    if start_line_id is None or end_line_id is None:
                        logger.warning(f"LLMè¿”å›çš„åŒºå—å®šä¹‰ç¼ºå°‘ start_line_id æˆ– end_line_id: {segment_def}")
                        continue

                    if segment_type not in semantic_segment_types:
                        logger.warning(f"LLMè¿”å›äº†æœªçŸ¥çš„è¯­ä¹‰ç±»å‹ '{segment_type}', å°†å…¶å½’ç±»ä¸º 'å…¶ä»–'. åŒºå—: {segment_def}")
                        segment_type = "å…¶ä»–"
                    
                    # ğŸ†• åº”ç”¨å¥å­å®Œæ•´æ€§è°ƒæ•´
                    adjusted_start_line_id, adjusted_end_line_id = self._adjust_segment_boundaries_for_sentence_completeness(
                        start_line_id, end_line_id, srt_entries
                    )
                    
                    relevant_entries = [entry for entry in srt_entries if adjusted_start_line_id <= entry["id"] <= adjusted_end_line_id]
                    if not relevant_entries:
                        logger.warning(f"æ ¹æ®è°ƒæ•´åçš„è¡Œå·èŒƒå›´ {adjusted_start_line_id}-{adjusted_end_line_id} æœªæ‰¾åˆ°åŒ¹é…çš„SRTæ¡ç›®. åŒºå—: {segment_def}")
                        continue
                    
                    start_time = min(entry["start_time"] for entry in relevant_entries)
                    end_time = max(entry["end_time"] for entry in relevant_entries)
                    text_content = " ".join(entry["text"] for entry in relevant_entries)
                    
                    # ğŸ†• è®°å½•è¾¹ç•Œè°ƒæ•´ä¿¡æ¯
                    if adjusted_start_line_id != start_line_id or adjusted_end_line_id != end_line_id:
                        logger.info(f"è¯­ä¹‰åŒºå— '{segment_type}' è¾¹ç•Œå·²è°ƒæ•´: L{start_line_id}-L{end_line_id} â†’ L{adjusted_start_line_id}-L{adjusted_end_line_id} (ä¸ºä¿è¯å¥å­å®Œæ•´æ€§)")
                    
                    segment_result = {
                        "semantic_type": segment_type,
                        "text": text_content,
                        "asr_matched_text": text_content,
                        "start_time": start_time,
                        "end_time": end_time,
                        "time_period": f"{self._format_seconds_to_time(start_time)} - {self._format_seconds_to_time(end_time)}",
                        "srt_line_ids": list(range(adjusted_start_line_id, adjusted_end_line_id + 1)),
                        "original_line_range": f"L{start_line_id}-L{end_line_id}",  # è®°å½•åŸå§‹èŒƒå›´
                        "adjusted_line_range": f"L{adjusted_start_line_id}-L{adjusted_end_line_id}"  # è®°å½•è°ƒæ•´åèŒƒå›´
                    }
                    result_segments.append(segment_result)
                except Exception as e_segment:
                    logger.warning(f"å¤„ç†LLMè¿”å›çš„å•ä¸ªè¯­ä¹‰åŒºå—æ—¶å‡ºé”™: {str(e_segment)}. åŒºå—å®šä¹‰: {segment_def}")
            
            if not result_segments:
                logger.warning("æˆåŠŸè°ƒç”¨LLMï¼Œä½†æœªèƒ½ä»å…¶å“åº”ä¸­æ„å»ºä»»ä½•æœ‰æ•ˆçš„è¯­ä¹‰åˆ†æ®µå¯¹è±¡ã€‚")
                return []

            logger.info(f"æˆåŠŸå°†LLMåˆ†æ®µç»“æœè½¬æ¢ä¸º {len(result_segments)} ä¸ªè¯­ä¹‰åˆ†æ®µå¯¹è±¡ã€‚")
            return result_segments
            
        except Exception as e_api_call:
            logger.error(f"è°ƒç”¨DeepSeek APIè¿›è¡Œè¯­ä¹‰åˆ†æ®µæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e_api_call)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return []

    def _adjust_segment_boundaries_for_sentence_completeness(self, start_line_id: int, end_line_id: int, srt_entries: list) -> tuple:
        """
        è°ƒæ•´åˆ†æ®µè¾¹ç•Œä»¥ç¡®ä¿å¥å­å®Œæ•´æ€§ï¼ŒåŒæ—¶é¿å…è·¨è¶Šäº§å“è¾¹ç•Œ
        
        Args:
            start_line_id: åŸå§‹èµ·å§‹è¡Œå·
            end_line_id: åŸå§‹ç»“æŸè¡Œå·
            srt_entries: SRTæ¡ç›®åˆ—è¡¨
            
        Returns:
            tuple: (è°ƒæ•´åçš„èµ·å§‹è¡Œå·, è°ƒæ•´åçš„ç»“æŸè¡Œå·)
        """
        # è·å–ç›¸å…³çš„SRTæ¡ç›®
        relevant_entries = [entry for entry in srt_entries if start_line_id <= entry["id"] <= end_line_id]
        if not relevant_entries:
            return start_line_id, end_line_id
        
        # ğŸ†• æ£€æŸ¥åŸå§‹ç‰‡æ®µä¸­çš„äº§å“ç±»å‹
        original_text = " ".join(entry["text"] for entry in relevant_entries)
        original_products = self._detect_products_in_text(original_text)
        
        # æ£€æŸ¥èµ·å§‹è¾¹ç•Œçš„å¥å­å®Œæ•´æ€§
        adjusted_start_line_id = self._adjust_start_boundary_for_sentence_completeness(start_line_id, srt_entries, original_products)
        
        # æ£€æŸ¥ç»“æŸè¾¹ç•Œçš„å¥å­å®Œæ•´æ€§
        adjusted_end_line_id = self._adjust_end_boundary_for_sentence_completeness(end_line_id, srt_entries, original_products)
        
        return adjusted_start_line_id, adjusted_end_line_id
    
    def _detect_products_in_text(self, text: str) -> set:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­åŒ…å«çš„äº§å“ç±»å‹ï¼ŒåŒºåˆ†ä¸»è¦äº§å“å’Œæ¬¡è¦æåŠ
        
        Args:
            text: è¦æ£€æµ‹çš„æ–‡æœ¬
            
        Returns:
            set: æ£€æµ‹åˆ°çš„ä¸»è¦äº§å“ç±»å‹é›†åˆ
        """
        products = set()
        text_lower = text.lower()
        
        # ğŸ†• å¢å¼ºçš„äº§å“å…³é”®è¯æ˜ å°„ï¼ŒæŒ‰é‡è¦æ€§å’Œç²¾ç¡®åº¦æ’åº
        product_keywords = {
            "å¯èµ‹è•´æ·³": {
                "primary": [
                    "å¯èµ‹è•´æ·³", "è•´æ·³å¥¶ç²‰", "è•´æ·³çš„", "è•´æ·³æ›´é€‚åˆ", "è•´æ·³é‡‡ç”¨", 
                    "è•´æ·³é…æ–¹", "è•´æ·³ç³»åˆ—", "è•´æ·³äº§å“", "è•´æ·³è¥å…»", "è•´æ·³å«æœ‰",
                    "è•´æ·³ç‰¹åˆ«", "è•´æ·³ç‹¬æœ‰", "è•´æ·³ä¸“é—¨", "è•´æ·³é’ˆå¯¹"
                ],
                "secondary": ["è•´æ·³"],
                "context_words": ["é…æ–¹", "è¥å…»", "æˆåˆ†", "å¥¶æº", "å“è´¨", "é€‚åˆ", "é€‰æ‹©"]
            },
            "å¯èµ‹æ°´å¥¶": {
                "primary": [
                    "å¯èµ‹æ°´å¥¶", "æ°´å¥¶ç³»åˆ—", "æ°´å¥¶æ˜¯", "æ°´å¥¶çš„", "æ°´å¥¶æ›´", "æ°´å¥¶å¯ä»¥",
                    "æ¶²æ€å¥¶", "å³å¼€å³é¥®", "å³é¥®å‹", "å¼€ç›–å³é¥®", "æ°´å¥¶äº§å“", "æ°´å¥¶è¥å…»",
                    "æ°´å¥¶æ–¹ä¾¿", "æ°´å¥¶ä¾¿æº", "æ°´å¥¶é€‚åˆ"
                ],
                "secondary": ["æ°´å¥¶"],
                "context_words": ["æ–¹ä¾¿", "æºå¸¦", "å³é¥®", "æ¶²æ€", "å¼€ç›–", "ä¾¿æº", "å¤–å‡º"]
            },
            "å¯èµ‹è“é’»": {
                "primary": [
                    "å¯èµ‹è“é’»", "è“é’»ç³»åˆ—", "è“é’»çš„", "è“é’»æ›´", "è“é’»é‡‡ç”¨",
                    "è“é’»é…æ–¹", "è“é’»äº§å“", "è“é’»è¥å…»", "è“é’»å“è´¨"
                ],
                "secondary": ["è“é’»"],
                "context_words": ["é«˜ç«¯", "å“è´¨", "è¥å…»", "é…æ–¹", "æˆåˆ†"]
            }
        }
        
        # ğŸ†• è½¬æ¢è¯æ£€æµ‹ - è¿™äº›è¯é€šå¸¸è¡¨ç¤ºäº§å“åˆ‡æ¢
        transition_indicators = [
            "ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹", "æ¥ä¸‹æ¥ä»‹ç»", "å¦å¤–è¿˜æœ‰", "é™¤æ­¤ä¹‹å¤–", 
            "æˆ‘ä»¬å†æ¥çœ‹", "è¿˜æœ‰ä¸€æ¬¾", "å¦ä¸€ä¸ªäº§å“", "å¦ä¸€ç§",
            "ç›¸æ¯”ä¹‹ä¸‹", "è€Œä¸”è¿˜æœ‰", "åŒæ—¶è¿˜æœ‰", "æ­¤å¤–è¿˜æœ‰"
        ]
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è½¬æ¢è¯ï¼Œå¦‚æœæœ‰ï¼Œæ›´å€¾å‘äºè¯†åˆ«ä¸ºäº§å“åˆ‡æ¢
        has_transition = any(indicator in text for indicator in transition_indicators)
        
        # æ£€æŸ¥æ¯ä¸ªäº§å“
        for product, keywords in product_keywords.items():
            product_score = 0
            
            # æ£€æŸ¥ä¸»è¦å…³é”®è¯ï¼ˆé«˜æƒé‡ï¼‰
            primary_matches = sum(1 for keyword in keywords["primary"] if keyword in text)
            product_score += primary_matches * 3
            
            # æ£€æŸ¥æ¬¡è¦å…³é”®è¯ï¼ˆä½æƒé‡ï¼‰
            secondary_matches = sum(1 for keyword in keywords["secondary"] if keyword in text)
            product_score += secondary_matches * 1
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡è¯æ±‡ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            context_matches = sum(1 for word in keywords["context_words"] if word in text)
            product_score += context_matches * 0.5
            
            # ğŸ†• å¦‚æœæœ‰è½¬æ¢è¯ï¼Œé™ä½è¯†åˆ«é˜ˆå€¼
            threshold = 2 if has_transition else 3
            
            # å¦‚æœåˆ†æ•°è¾¾åˆ°é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯ä¸»è¦äº§å“
            if product_score >= threshold:
                products.add(product)
                continue
            
            # ğŸ†• ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæœ‰æ¬¡è¦å…³é”®è¯ä¸”æ–‡æœ¬è¾ƒé•¿ï¼Œè¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æ
            if secondary_matches > 0 and len(text) > 15:
                if self._is_primary_product_discussion(text, product):
                    products.add(product)
        
        # ğŸ†• è®°å½•æ£€æµ‹ç»“æœç”¨äºè°ƒè¯•
        if products:
            logger.debug(f"åœ¨æ–‡æœ¬ä¸­æ£€æµ‹åˆ°äº§å“: {products} | æ–‡æœ¬: {text[:50]}...")
        
        return products
    
    def _is_primary_product_discussion(self, text: str, product: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦åœ¨è®¨è®ºæŸä¸ªäº§å“
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            product: äº§å“åç§°
            
        Returns:
            bool: æ˜¯å¦æ˜¯ä¸»è¦è®¨è®º
        """
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆå°‘äº15ä¸ªå­—ç¬¦ï¼‰ï¼Œå¯èƒ½åªæ˜¯é¡ºå¸¦æåŠ
        if len(text) < 15:
            return False
        
        # ğŸ†• å¢å¼ºçš„äº§å“ç›¸å…³æè¿°æ€§è¯æ±‡
        product_descriptors = {
            "å¯èµ‹è•´æ·³": [
                "é…æ–¹", "è¥å…»", "é€‚åˆ", "é€‰æ‹©", "å“è´¨", "æˆåˆ†", "å¥¶æº", "å«æœ‰",
                "é‡‡ç”¨", "ç‰¹åˆ«", "ç‹¬æœ‰", "ä¸“é—¨", "é’ˆå¯¹", "è®¾è®¡", "ç ”å‘", "ç§‘å­¦",
                "å¤©ç„¶", "æœ‰æœº", "å®‰å…¨", "å¥åº·", "æ¶ˆåŒ–", "å¸æ”¶", "å…ç–«", "å‘è‚²"
            ],
            "å¯èµ‹æ°´å¥¶": [
                "æ–¹ä¾¿", "æºå¸¦", "å³é¥®", "æ¶²æ€", "å¼€ç›–", "ä¾¿æº", "å¤–å‡º", "éšæ—¶",
                "å³å¼€", "æ–°é²œ", "ä¿å­˜", "å‚¨å­˜", "æ¸©åº¦", "å†·è—", "å¸¸æ¸©", "åŒ…è£…",
                "æ—…è¡Œ", "å‡ºé—¨", "ä¸Šç­", "å·¥ä½œ", "å¿™ç¢Œ", "å¿«æ·", "ç®€å•"
            ],
            "å¯èµ‹è“é’»": [
                "é«˜ç«¯", "å“è´¨", "è¥å…»", "é…æ–¹", "æˆåˆ†", "å¥¶æº", "è¿›å£", "ä¼˜è´¨",
                "ç²¾é€‰", "é¡¶çº§", "ä¸“ä¸š", "ç§‘å­¦", "å…ˆè¿›", "æŠ€æœ¯", "å·¥è‰º", "æ ‡å‡†"
            ]
        }
        
        descriptors = product_descriptors.get(product, [])
        descriptor_count = sum(1 for desc in descriptors if desc in text)
        
        # ğŸ†• æ£€æŸ¥äº§å“åŠ¨ä½œè¯æ±‡ï¼ˆè¡¨ç¤ºå¯¹äº§å“çš„å…·ä½“æè¿°æˆ–æ¨èï¼‰
        action_words = [
            "æ¨è", "å»ºè®®", "é€‰æ‹©", "ä½¿ç”¨", "å–", "é£Ÿç”¨", "è´­ä¹°", "å°è¯•",
            "é€‚åˆ", "æ»¡è¶³", "è§£å†³", "å¸®åŠ©", "æä¾›", "å«æœ‰", "å¯Œå«", "æ·»åŠ "
        ]
        action_count = sum(1 for action in action_words if action in text)
        
        # ğŸ†• æ£€æŸ¥æ˜¯å¦æœ‰äº§å“è½¬æ¢çš„ä¿¡å·è¯
        transition_words = [
            "ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹", "æ¥ä¸‹æ¥ä»‹ç»", "å¦å¤–è¿˜æœ‰", "é™¤æ­¤ä¹‹å¤–", 
            "æˆ‘ä»¬å†æ¥çœ‹", "è¿˜æœ‰ä¸€æ¬¾", "å¦ä¸€ä¸ªäº§å“", "å¦ä¸€ç§",
            "ç›¸æ¯”ä¹‹ä¸‹", "è€Œä¸”è¿˜æœ‰", "åŒæ—¶è¿˜æœ‰", "æ­¤å¤–è¿˜æœ‰"
        ]
        has_transition = any(word in text for word in transition_words)
        
        # ğŸ†• æ£€æŸ¥æ˜¯å¦æœ‰æ¯”è¾ƒæ€§è¯æ±‡ï¼ˆé€šå¸¸è¡¨ç¤ºåœ¨å¯¹æ¯”ä¸åŒäº§å“ï¼‰
        comparison_words = ["ç›¸æ¯”", "æ¯”è¾ƒ", "å¯¹æ¯”", "è€Œ", "ä½†æ˜¯", "ä¸è¿‡", "ç„¶è€Œ", "åŒºåˆ«"]
        has_comparison = any(word in text for word in comparison_words)
        
        # ğŸ†• ç»¼åˆè¯„åˆ†ç³»ç»Ÿ
        score = 0
        
        # æè¿°æ€§è¯æ±‡å¾—åˆ†ï¼ˆæ¯ä¸ªè¯0.5åˆ†ï¼‰
        score += descriptor_count * 0.5
        
        # åŠ¨ä½œè¯æ±‡å¾—åˆ†ï¼ˆæ¯ä¸ªè¯1åˆ†ï¼‰
        score += action_count * 1
        
        # è½¬æ¢è¯åŠ åˆ†ï¼ˆè¡¨ç¤ºé‡ç‚¹ä»‹ç»ï¼‰
        if has_transition:
            score += 2
        
        # æ¯”è¾ƒè¯æ±‡åŠ åˆ†ï¼ˆè¡¨ç¤ºåœ¨å¯¹æ¯”äº§å“ï¼‰
        if has_comparison:
            score += 1
        
        # ğŸ†• æ–‡æœ¬é•¿åº¦åŠ åˆ†ï¼ˆè¾ƒé•¿çš„æ–‡æœ¬æ›´å¯èƒ½æ˜¯ä¸»è¦è®¨è®ºï¼‰
        if len(text) > 30:
            score += 1
        if len(text) > 50:
            score += 1
        
        # ğŸ†• äº§å“åç§°å‡ºç°é¢‘ç‡åŠ åˆ†
        product_mentions = text.lower().count(product.lower())
        if product_mentions > 1:
            score += product_mentions * 0.5
        
        # åˆ¤æ–­é˜ˆå€¼ï¼šå¾—åˆ†>=2.5è®¤ä¸ºæ˜¯ä¸»è¦è®¨è®º
        is_primary = score >= 2.5
        
        # ğŸ†• è®°å½•åˆ¤æ–­è¿‡ç¨‹ç”¨äºè°ƒè¯•
        if is_primary:
            logger.debug(f"åˆ¤æ–­ä¸ºä¸»è¦äº§å“è®¨è®º - äº§å“: {product}, å¾—åˆ†: {score:.1f}, æ–‡æœ¬: {text[:30]}...")
        
        return is_primary
    
    def _adjust_start_boundary_for_sentence_completeness(self, start_line_id: int, srt_entries: list, original_products: set = None) -> int:
        """
        è°ƒæ•´èµ·å§‹è¾¹ç•Œä»¥ç¡®ä¿ä¸ä¼šåœ¨å¥å­ä¸­é—´å¼€å§‹ï¼ŒåŒæ—¶é¿å…è·¨è¶Šäº§å“è¾¹ç•Œ
        
        Args:
            start_line_id: åŸå§‹èµ·å§‹è¡Œå·
            srt_entries: SRTæ¡ç›®åˆ—è¡¨
            original_products: åŸå§‹ç‰‡æ®µä¸­çš„äº§å“ç±»å‹
            
        Returns:
            int: è°ƒæ•´åçš„èµ·å§‹è¡Œå·
        """
        # æ‰¾åˆ°å¯¹åº”çš„SRTæ¡ç›®
        start_entry = None
        for entry in srt_entries:
            if entry["id"] == start_line_id:
                start_entry = entry
                break
        
        if not start_entry:
            return start_line_id
        
        # æ£€æŸ¥å½“å‰è¡Œçš„æ–‡æœ¬æ˜¯å¦ä»¥å¥å­å¼€å¤´çš„æ ‡å¿—å¼€å§‹
        text = start_entry["text"].strip()
        
        # å¦‚æœæ–‡æœ¬ä»¥å°å†™å­—æ¯å¼€å§‹ï¼Œæˆ–è€…ä»¥è¿æ¥è¯å¼€å§‹ï¼Œå¯èƒ½æ˜¯å¥å­çš„ä¸­é—´éƒ¨åˆ†
        sentence_continuation_indicators = [
            "è€Œä¸”", "å¹¶ä¸”", "åŒæ—¶", "å¦å¤–", "æ­¤å¤–", "ç„¶è€Œ", "ä½†æ˜¯", "ä¸è¿‡", "å› æ­¤", "æ‰€ä»¥", 
            "é‚£ä¹ˆ", "è¿™æ ·", "è¿™é‡Œ", "é‚£é‡Œ", "è¿™ä¸ª", "é‚£ä¸ª", "å®ƒ", "ä»–", "å¥¹", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬"
        ]
        
        # æ£€æŸ¥æ˜¯å¦ä»¥è¿æ¥è¯å¼€å§‹
        starts_with_continuation = any(text.startswith(indicator) for indicator in sentence_continuation_indicators)
        
        # æ£€æŸ¥æ˜¯å¦ä»¥å°å†™è‹±æ–‡å­—æ¯å¼€å§‹ï¼ˆå¯èƒ½æ˜¯è‹±æ–‡å¥å­çš„ä¸­é—´ï¼‰
        starts_with_lowercase = text and text[0].islower() and text[0].isalpha()
        
        if starts_with_continuation or starts_with_lowercase:
            # ğŸ†• è¿›ä¸€æ­¥é™åˆ¶æœç´¢èŒƒå›´ï¼šæœ€å¤šå‘å‰æŸ¥æ‰¾2è¡Œ
            search_limit = min(2, start_line_id - 1)
            
            # å‘å‰æŸ¥æ‰¾å¥å­çš„çœŸæ­£å¼€å§‹ï¼Œä½†ä¸è¶…è¿‡äº§å“è¾¹ç•Œ
            for i in range(start_line_id - 1, max(1, start_line_id - search_limit - 1), -1):
                prev_entry = None
                for entry in srt_entries:
                    if entry["id"] == i:
                        prev_entry = entry
                        break
                
                if prev_entry:
                    # ğŸ†• æ›´ä¸¥æ ¼çš„äº§å“è¾¹ç•Œæ£€æŸ¥
                    if original_products:
                        prev_products = self._detect_products_in_text(prev_entry["text"])
                        if prev_products:
                            # å¦‚æœå‰ä¸€è¡Œæœ‰äº§å“ä¸”ä¸åŸå§‹äº§å“ä¸åŒï¼Œåœæ­¢è°ƒæ•´
                            if not prev_products.intersection(original_products):
                                logger.info(f"æ£€æµ‹åˆ°äº§å“è¾¹ç•Œï¼Œåœæ­¢å‘å‰è°ƒæ•´èµ·å§‹è¾¹ç•Œ: {prev_products} vs {original_products}")
                                break
                            # ğŸ†• å³ä½¿æ˜¯ç›¸åŒäº§å“ï¼Œå¦‚æœæœ‰è½¬æ¢è¯ä¹Ÿè¦å°å¿ƒ
                            transition_words = ["ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹", "æ¥ä¸‹æ¥", "å¦å¤–", "è¿˜æœ‰"]
                            if any(word in prev_entry["text"] for word in transition_words):
                                logger.info(f"æ£€æµ‹åˆ°è½¬æ¢è¯ï¼Œåœæ­¢å‘å‰è°ƒæ•´: {prev_entry['text'][:30]}...")
                                break
                    
                    prev_text = prev_entry["text"].strip()
                    # æ£€æŸ¥å‰ä¸€è¡Œæ˜¯å¦ä»¥å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç»“å°¾
                    if prev_text.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                        # æ‰¾åˆ°äº†å¥å­çš„ç»“æŸï¼Œå½“å‰è¡Œå°±æ˜¯æ–°å¥å­çš„å¼€å§‹
                        break
                    elif i == 1:
                        # å·²ç»åˆ°äº†ç¬¬ä¸€è¡Œï¼Œä»è¿™é‡Œå¼€å§‹
                        return 1
                else:
                    break
            
            # ğŸ†• å¦‚æœæ‰¾åˆ°äº†æ›´åˆé€‚çš„èµ·å§‹ç‚¹ï¼Œä½†è¦ç¡®ä¿ä¸è¶…è¿‡æœç´¢é™åˆ¶
            adjusted_start = max(1, start_line_id - 1)
            if start_line_id - adjusted_start <= search_limit:
                return adjusted_start
        
        return start_line_id
    
    def _adjust_end_boundary_for_sentence_completeness(self, end_line_id: int, srt_entries: list, original_products: set = None) -> int:
        """
        è°ƒæ•´ç»“æŸè¾¹ç•Œä»¥ç¡®ä¿å¥å­å®Œæ•´æ€§ï¼ŒåŒæ—¶é¿å…è·¨è¶Šäº§å“è¾¹ç•Œ
        
        Args:
            end_line_id: åŸå§‹ç»“æŸè¡Œå·
            srt_entries: SRTæ¡ç›®åˆ—è¡¨
            original_products: åŸå§‹ç‰‡æ®µä¸­çš„äº§å“ç±»å‹
            
        Returns:
            int: è°ƒæ•´åçš„ç»“æŸè¡Œå·
        """
        # æ‰¾åˆ°å¯¹åº”çš„SRTæ¡ç›®
        end_entry = None
        for entry in srt_entries:
            if entry["id"] == end_line_id:
                end_entry = entry
                break
        
        if not end_entry:
            return end_line_id
        
        # æ£€æŸ¥å½“å‰è¡Œçš„æ–‡æœ¬æ˜¯å¦ä»¥å¥å­ç»“å°¾çš„æ ‡å¿—ç»“æŸ
        text = end_entry["text"].strip()
        
        # å¦‚æœæ–‡æœ¬ä¸ä»¥å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç»“å°¾ï¼Œå¯èƒ½å¥å­è¿˜æ²¡æœ‰å®Œæˆ
        if not text.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
            # ğŸ†• è¿›ä¸€æ­¥é™åˆ¶æœç´¢èŒƒå›´ï¼šæœ€å¤šå‘åæŸ¥æ‰¾2è¡Œ
            max_line_id = max(entry["id"] for entry in srt_entries)
            search_limit = min(2, max_line_id - end_line_id)
            
            # å‘åæŸ¥æ‰¾å¥å­çš„çœŸæ­£ç»“æŸï¼Œä½†ä¸è¶…è¿‡äº§å“è¾¹ç•Œ
            for i in range(end_line_id + 1, min(max_line_id + 1, end_line_id + search_limit + 1)):
                next_entry = None
                for entry in srt_entries:
                    if entry["id"] == i:
                        next_entry = entry
                        break
                
                if next_entry:
                    # ğŸ†• æ›´ä¸¥æ ¼çš„äº§å“è¾¹ç•Œæ£€æŸ¥
                    if original_products:
                        next_products = self._detect_products_in_text(next_entry["text"])
                        if next_products:
                            # å¦‚æœåä¸€è¡Œæœ‰äº§å“ä¸”ä¸åŸå§‹äº§å“ä¸åŒï¼Œåœæ­¢è°ƒæ•´
                            if not next_products.intersection(original_products):
                                logger.info(f"æ£€æµ‹åˆ°äº§å“è¾¹ç•Œï¼Œåœæ­¢å‘åè°ƒæ•´ç»“æŸè¾¹ç•Œ: {next_products} vs {original_products}")
                                break
                            # ğŸ†• æ£€æŸ¥æ˜¯å¦æœ‰è½¬æ¢è¯ï¼Œå³ä½¿æ˜¯ç›¸åŒäº§å“ä¹Ÿè¦å°å¿ƒ
                            transition_words = ["ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹", "æ¥ä¸‹æ¥", "å¦å¤–", "è¿˜æœ‰"]
                            if any(word in next_entry["text"] for word in transition_words):
                                logger.info(f"æ£€æµ‹åˆ°è½¬æ¢è¯ï¼Œåœæ­¢å‘åè°ƒæ•´: {next_entry['text'][:30]}...")
                                break
                    
                    next_text = next_entry["text"].strip()
                    # æ£€æŸ¥è¿™ä¸€è¡Œæ˜¯å¦ä»¥å¥å­ç»“å°¾æ ‡å¿—ç»“æŸ
                    if next_text.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                        # æ‰¾åˆ°äº†å¥å­çš„ç»“æŸï¼Œä½†è¦ç¡®ä¿ä¸è¶…è¿‡æœç´¢é™åˆ¶
                        if i - end_line_id <= search_limit:
                            return i
                        else:
                            break
                    elif i == max_line_id:
                        # å·²ç»åˆ°äº†æœ€åä¸€è¡Œ
                        return max_line_id
                else:
                    break
            
            # ğŸ†• å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å¥å­ç»“å°¾ï¼Œæœ€å¤šå‘åå»¶ä¼¸1è¡Œï¼ˆè¿›ä¸€æ­¥å‡å°‘èŒƒå›´ï¼‰
            adjusted_end = min(max_line_id, end_line_id + 1)
            if adjusted_end - end_line_id <= search_limit:
                return adjusted_end
        
        return end_line_id

    def _format_seconds_to_time(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºæ—¶é—´å­—ç¬¦ä¸²æ ¼å¼ (HH:MM:SS)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    def _call_deepseek_for_srt_segmentation(self, srt_text_for_llm, type_descriptions_formatted_str):
        """
        è°ƒç”¨DeepSeek APIè¿›è¡ŒSRTè¯­ä¹‰åˆ†æ®µ
        
        Args:
            srt_text_for_llm: æ ¼å¼åŒ–çš„SRTæ–‡æœ¬
            type_descriptions_formatted_str: è¯­ä¹‰ç±»å‹æè¿°
            
        Returns:
            åŒ…å«åˆ†æ®µä¿¡æ¯çš„å­—å…¸
        
        Raises:
            ValueError: å¦‚æœAPIå“åº”æ— æ•ˆæˆ–JSONè§£æå¤±è´¥ã€‚
            requests.exceptions.RequestException: å¦‚æœAPIè¯·æ±‚å¤±è´¥ã€‚
        """
        import requests
        import json
        import re
        import traceback
        
        # ğŸ†• é›†æˆç”¨æˆ·åé¦ˆ
        try:
            from streamlit_app.modules.analysis.feedback_manager import get_feedback_manager
            feedback_manager = get_feedback_manager()
        except ImportError:
            feedback_manager = None
            logger.warning("æ— æ³•å¯¼å…¥åé¦ˆç®¡ç†å™¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æç¤ºè¯")
        
        # æ„å»ºåŸºç¡€ç³»ç»Ÿæç¤º
        base_system_prompt = (
            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†é¢‘å†…å®¹ç»“æ„åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€ä¸ªä»¥SRTå­—å¹•è¡Œå½¢å¼æä¾›çš„è§†é¢‘è½¬å½•æ–‡æœ¬ï¼Œ"
            "å¹¶å°†è¿ç»­çš„SRTè¡Œç»„åˆæˆç¬¦åˆé¢„å®šä¹‰è¯­ä¹‰ç±»å‹çš„é€»è¾‘åŒºå—ã€‚"
            "æ¯ä¸ªSRTè¡Œéƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„è¡Œå·ï¼ˆä¾‹å¦‚ L1, L2, ...ï¼‰ã€‚ä½ éœ€è¦ç¡®å®šæ¯ä¸ªè¯­ä¹‰åŒºå—ç”±å“ªäº›SRTè¡Œç»„æˆã€‚"
            f"\n\nã€é¢„å®šä¹‰çš„è¯­ä¹‰åŒºå—ç±»å‹åŠå…¶è¯´æ˜ã€‘:\n{type_descriptions_formatted_str}"
            "\n\nã€æ ¸å¿ƒåˆ†æ®µåŸåˆ™ - æŒ‰é‡è¦æ€§æ’åºã€‘ï¼š"
            "\nğŸ”¥ğŸ”¥ğŸ”¥ 1. äº§å“ç±»å‹å˜åŒ–å¼ºåˆ¶åˆ†æ®µï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ï¼š"
            "\n   å½“å†…å®¹ä»ä¸€ä¸ªäº§å“åˆ‡æ¢åˆ°å¦ä¸€ä¸ªäº§å“æ—¶ï¼Œå¿…é¡»ç«‹å³åˆ›å»ºæ–°çš„ç‰‡æ®µ"
            "\n   - å¯èµ‹è•´æ·³ â†’ å¯èµ‹æ°´å¥¶ï¼šå¿…é¡»åˆ†æ®µ"
            "\n   - å¯èµ‹æ°´å¥¶ â†’ å¯èµ‹è•´æ·³ï¼šå¿…é¡»åˆ†æ®µ"
            "\n   - å¯èµ‹è“é’» â†’ å…¶ä»–äº§å“ï¼šå¿…é¡»åˆ†æ®µ"
            "\n   - å³ä½¿è¯­ä¹‰ç±»å‹ç›¸åŒï¼ˆå¦‚éƒ½æ˜¯'äº§å“ä¼˜åŠ¿'ï¼‰ï¼Œä¹Ÿè¦åˆ†æˆä¸åŒç‰‡æ®µ"
            "\n   - å³ä½¿åªæ˜¯ç®€å•æåŠå¦ä¸€ä¸ªäº§å“ï¼Œä¹Ÿè¦è€ƒè™‘åˆ†æ®µ"
            "\n\n2. å¥å­å®Œæ•´æ€§ï¼šç¡®ä¿æ¯ä¸ªè¯­ä¹‰åŒºå—éƒ½ä»¥å®Œæ•´çš„å¥å­å¼€å§‹å’Œç»“æŸ"
            "\n3. è¯­ä¹‰è¿è´¯æ€§ï¼šç›¸åŒè¯­ä¹‰ç±»å‹ä¸”ç›¸åŒäº§å“çš„å†…å®¹å½’ä¸ºä¸€ä¸ªåŒºå—"
            "\n4. è‡ªç„¶åœé¡¿ï¼šä¼˜å…ˆåœ¨è‡ªç„¶çš„è¯­éŸ³åœé¡¿å¤„åˆ†æ®µ"
            "\n5. é€‚åº¦é•¿åº¦ï¼šæ¯ä¸ªåŒºå—åº”è¯¥æœ‰åˆç†çš„é•¿åº¦ï¼ˆ10-40ç§’ä¸ºä½³ï¼‰"
            "\n\nã€äº§å“è¯†åˆ«å…³é”®è¯ - ç²¾ç¡®åŒ¹é…ã€‘ï¼š"
            "\n- å¯èµ‹è•´æ·³ï¼š'å¯èµ‹è•´æ·³'ã€'è•´æ·³å¥¶ç²‰'ã€'è•´æ·³çš„'ã€'è•´æ·³æ›´'ã€'è•´æ·³é‡‡ç”¨'"
            "\n- å¯èµ‹æ°´å¥¶ï¼š'å¯èµ‹æ°´å¥¶'ã€'æ°´å¥¶'ã€'æ¶²æ€å¥¶'ã€'å³é¥®'ã€'å³å¼€å³é¥®'"
            "\n- å¯èµ‹è“é’»ï¼š'å¯èµ‹è“é’»'ã€'è“é’»'ã€'è“é’»ç³»åˆ—'"
            "\n\nã€äº§å“å˜åŒ–æ£€æµ‹ç­–ç•¥ã€‘ï¼š"
            "\n1. é€è¡Œæ‰«æï¼Œæ ‡è®°æ¯è¡Œæåˆ°çš„äº§å“å…³é”®è¯"
            "\n2. å½“å‘ç°æ–°äº§å“å…³é”®è¯å‡ºç°æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºäº§å“åˆ‡æ¢"
            "\n3. äº§å“åˆ‡æ¢çš„åˆ¤æ–­æ ‡å‡†ï¼š"
            "\n   - å‰é¢å‡ è¡Œä¸»è¦è®¨è®ºäº§å“Aï¼Œå½“å‰è¡Œå¼€å§‹è®¨è®ºäº§å“B"
            "\n   - å‡ºç°è½¬æ¢è¯ + æ–°äº§å“åç§°"
            "\n   - äº§å“ç‰¹æ€§æè¿°å‘ç”Ÿæ˜æ˜¾å˜åŒ–"
            "\n\nã€è½¬æ¢è¯è¯†åˆ«ï¼ˆäº§å“åˆ‡æ¢ä¿¡å·ï¼‰ã€‘ï¼š"
            "\nâš ï¸ å‘ç°ä»¥ä¸‹è½¬æ¢è¯æ—¶ï¼Œé€šå¸¸è¡¨ç¤ºäº§å“åˆ‡æ¢ï¼Œå¿…é¡»åˆ†æ®µï¼š"
            "\n   - 'ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹'ã€'æ¥ä¸‹æ¥ä»‹ç»'ã€'å¦å¤–è¿˜æœ‰'ã€'é™¤æ­¤ä¹‹å¤–'"
            "\n   - 'æˆ‘ä»¬å†æ¥çœ‹'ã€'è¿˜æœ‰ä¸€æ¬¾'ã€'å¦ä¸€ä¸ªäº§å“'ã€'å¦ä¸€ç§'"
            "\n   - 'ç›¸æ¯”ä¹‹ä¸‹'ã€'è€Œ'ã€'ä½†æ˜¯'ã€'ä¸è¿‡'ï¼ˆå½“ä¼´éšäº§å“åç§°æ—¶ï¼‰"
            "\n   - 'åŒæ—¶'ã€'æ­¤å¤–'ã€'å¦å¤–'ï¼ˆå½“å¼•å…¥æ–°äº§å“æ—¶ï¼‰"
            "\n\nã€å…·ä½“åˆ†æ®µç¤ºä¾‹ã€‘ï¼š"
            "\nâŒ é”™è¯¯ç¤ºä¾‹ï¼š"
            "\n   L1-L8: 'å¯èµ‹è•´æ·³çš„è¥å…»é…æ–¹å¾ˆå¥½...ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å¯èµ‹æ°´å¥¶çš„ä¾¿æºæ€§...' â†’ å½’ä¸ºä¸€ä¸ª'äº§å“ä¼˜åŠ¿'ç‰‡æ®µ"
            "\nâœ… æ­£ç¡®ç¤ºä¾‹ï¼š"
            "\n   L1-L4: 'å¯èµ‹è•´æ·³çš„è¥å…»é…æ–¹å¾ˆå¥½...' â†’ 'äº§å“ä¼˜åŠ¿'ç‰‡æ®µï¼ˆå¯èµ‹è•´æ·³ï¼‰"
            "\n   L5-L8: 'ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å¯èµ‹æ°´å¥¶çš„ä¾¿æºæ€§...' â†’ 'äº§å“ä¼˜åŠ¿'ç‰‡æ®µï¼ˆå¯èµ‹æ°´å¥¶ï¼‰"
            "\n\nâŒ é”™è¯¯ç¤ºä¾‹ï¼š"
            "\n   L10-L20: 'è•´æ·³é€‚åˆ...è€Œæ°´å¥¶æ›´æ–¹ä¾¿...' â†’ å½’ä¸ºä¸€ä¸ªç‰‡æ®µ"
            "\nâœ… æ­£ç¡®ç¤ºä¾‹ï¼š"
            "\n   L10-L15: 'è•´æ·³é€‚åˆ...' â†’ ç‰‡æ®µAï¼ˆå¯èµ‹è•´æ·³ï¼‰"
            "\n   L16-L20: 'è€Œæ°´å¥¶æ›´æ–¹ä¾¿...' â†’ ç‰‡æ®µBï¼ˆå¯èµ‹æ°´å¥¶ï¼‰"
            "\n\nã€è´¨é‡æ£€æŸ¥è¦ç‚¹ã€‘ï¼š"
            "\nâš ï¸ åˆ†æ®µå®Œæˆåï¼Œè¯·è‡ªæ£€ï¼š"
            "\n1. æ˜¯å¦æœ‰ä»»ä½•ç‰‡æ®µåŒæ—¶åŒ…å«å¤šä¸ªä¸åŒäº§å“çš„ä¸»è¦è®¨è®ºï¼Ÿ"
            "\n2. æ˜¯å¦æœ‰äº§å“åˆ‡æ¢ç‚¹è¢«å¿½ç•¥ï¼Ÿ"
            "\n3. æ˜¯å¦æœ‰è½¬æ¢è¯åé¢è·Ÿç€æ–°äº§å“ä½†æ²¡æœ‰åˆ†æ®µï¼Ÿ"
            "\n4. ç‰‡æ®µé•¿åº¦æ˜¯å¦åˆç†ï¼ˆé¿å…è¿‡é•¿æˆ–è¿‡çŸ­ï¼‰ï¼Ÿ"
            "\n\nâš ï¸ ç»†ç²’åº¦åˆ†æ®µä¼˜äºç²—ç²’åº¦åˆ†æ®µï¼šå®å¯å¤šåˆ†å‡ ä¸ªç‰‡æ®µï¼Œä¹Ÿä¸è¦å°†ä¸åŒäº§å“æ··åœ¨ä¸€èµ·"
            "\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä¸€ä¸ªåˆ—è¡¨ï¼Œç¡®ä¿äº§å“å˜åŒ–æ—¶å¿…é¡»åˆ†æ®µï¼š"
            '\n[\n  {\n    "segment_type": "äº§å“ä¼˜åŠ¿",\n    "start_line_id": 1,\n    "end_line_id": 3,\n    "note": "å¯èµ‹è•´æ·³äº§å“ä¼˜åŠ¿"\n  },\n'
            '  {\n    "segment_type": "äº§å“ä¼˜åŠ¿",\n    "start_line_id": 4,\n    "end_line_id": 6,\n    "note": "å¯èµ‹æ°´å¥¶äº§å“ä¼˜åŠ¿"\n  }\n]'
        )
        
        # ğŸ†• åº”ç”¨ç”¨æˆ·åé¦ˆæ”¹è¿›æç¤ºè¯
        if feedback_manager:
            try:
                system_prompt = feedback_manager.apply_feedback_to_prompt(base_system_prompt)
                logger.info("å·²åº”ç”¨ç”¨æˆ·åé¦ˆæ”¹è¿›æç¤ºè¯")
            except Exception as e:
                logger.warning(f"åº”ç”¨ç”¨æˆ·åé¦ˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æç¤ºè¯: {e}")
                system_prompt = base_system_prompt
        else:
            system_prompt = base_system_prompt

        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = (
            "è¯·æ ¹æ®ä»¥ä¸‹é€è¡Œæ ‡è®°çš„SRTè½¬å½•æ–‡æœ¬è¿›è¡Œè¯­ä¹‰åŒºå—åˆ’åˆ†ã€‚"
            "\n\nğŸ”¥ğŸ”¥ğŸ”¥ã€æœ€é‡è¦çš„è¦æ±‚ - äº§å“å˜åŒ–å¼ºåˆ¶åˆ†æ®µã€‘ï¼š"
            "\n1. ä»”ç»†é˜…è¯»æ¯ä¸€è¡Œæ–‡æœ¬ï¼Œè¯†åˆ«äº§å“åç§°ï¼ˆå¯èµ‹è•´æ·³ã€å¯èµ‹æ°´å¥¶ã€å¯èµ‹è“é’»ç­‰ï¼‰"
            "\n2. å½“å‘ç°äº§å“åç§°å˜åŒ–æ—¶ï¼Œå¿…é¡»ç«‹å³åˆ›å»ºæ–°çš„ç‰‡æ®µï¼Œå³ä½¿è¯­ä¹‰ç±»å‹ç›¸åŒ"
            "\n3. ç‰¹åˆ«æ³¨æ„è½¬æ¢è¯ï¼š'ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹'ã€'æ¥ä¸‹æ¥'ã€'å¦å¤–'ã€'è¿˜æœ‰'ç­‰"
            "\n4. å®å¯å¤šåˆ†å‡ ä¸ªç‰‡æ®µï¼Œä¹Ÿä¸è¦å°†ä¸åŒäº§å“çš„å†…å®¹åˆå¹¶"
            "\n\nã€è¯¦ç»†åˆ†ææ­¥éª¤ã€‘ï¼š"
            "\nç¬¬ä¸€æ­¥ï¼šé€è¡Œæ‰«æäº§å“æ ‡è®°"
            "\n- ä¸ºæ¯ä¸€è¡Œæ ‡è®°åŒ…å«çš„äº§å“å…³é”®è¯"
            "\n- è¯†åˆ«ä¸»è¦è®¨è®ºçš„äº§å“ï¼ˆä¸æ˜¯ç®€å•æåŠï¼‰"
            "\n- æ ‡è®°äº§å“å˜åŒ–çš„æ½œåœ¨è¾¹ç•Œç‚¹"
            "\n\nç¬¬äºŒæ­¥ï¼šè¯†åˆ«äº§å“å˜åŒ–è¾¹ç•Œ"
            "\n- å¯»æ‰¾ä»äº§å“Aåˆ‡æ¢åˆ°äº§å“Bçš„è¡Œ"
            "\n- ç‰¹åˆ«å…³æ³¨è½¬æ¢è¯åçš„äº§å“åç§°"
            "\n- ç¡®è®¤äº§å“ç‰¹æ€§æè¿°çš„å˜åŒ–"
            "\n\nç¬¬ä¸‰æ­¥ï¼šå¼ºåˆ¶åˆ†æ®µå¤„ç†"
            "\n- åœ¨æ¯ä¸ªäº§å“å˜åŒ–ç‚¹åˆ›å»ºæ–°ç‰‡æ®µ"
            "\n- ç¡®ä¿ä¸åŒäº§å“ä¸åœ¨åŒä¸€ç‰‡æ®µä¸­"
            "\n- æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„äº§å“è¾¹ç•Œ"
            "\n\nç¬¬å››æ­¥ï¼šè¯­ä¹‰ç±»å‹åˆ†é…"
            "\n- ä¸ºæ¯ä¸ªç‰‡æ®µåˆ†é…åˆé€‚çš„è¯­ä¹‰ç±»å‹"
            "\n- ç¡®ä¿ç‰‡æ®µå†…å®¹ä¸è¯­ä¹‰ç±»å‹åŒ¹é…"
            "\n- ä¼˜åŒ–ç‰‡æ®µé•¿åº¦å’Œå®Œæ•´æ€§"
            "\n\nã€è´¨é‡æ£€æŸ¥æ¸…å•ã€‘ï¼š"
            "\nâœ… æ¯ä¸ªç‰‡æ®µæ˜¯å¦åªåŒ…å«ä¸€ä¸ªä¸»è¦äº§å“çš„è®¨è®ºï¼Ÿ"
            "\nâœ… æ˜¯å¦åœ¨æ‰€æœ‰äº§å“å˜åŒ–ç‚¹éƒ½è¿›è¡Œäº†åˆ†æ®µï¼Ÿ"
            "\nâœ… æ˜¯å¦æœ‰è½¬æ¢è¯+æ–°äº§å“çš„ç»„åˆè¢«é—æ¼ï¼Ÿ"
            "\nâœ… ç‰‡æ®µé•¿åº¦æ˜¯å¦åˆç†ï¼ˆ10-40ç§’ï¼‰ï¼Ÿ"
            "\nâœ… è¯­ä¹‰ç±»å‹æ˜¯å¦å‡†ç¡®åŒ¹é…ç‰‡æ®µå†…å®¹ï¼Ÿ"
            "\n\nè¾“å‡ºçš„ `start_line_id` å’Œ `end_line_id` å¿…é¡»å‡†ç¡®å¯¹åº”è¾“å…¥æ–‡æœ¬ä¸­æ¯è¡Œå‰çš„L{id}æ ‡è®°ã€‚"
            f"\n--- å¸¦è¡Œå·çš„SRTè½¬å½•æ–‡æœ¬å¼€å§‹ ---\n{srt_text_for_llm}"
            "\n--- å¸¦è¡Œå·çš„SRTè½¬å½•æ–‡æœ¬ç»“æŸ ---"
            "\n\nè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿äº§å“å˜åŒ–æ—¶å¿…é¡»åˆ†æ®µã€‚æ¯ä¸ªç‰‡æ®µå¿…é¡»åŒ…å«noteå­—æ®µè¯´æ˜ä¸»è¦è®¨è®ºçš„äº§å“ã€‚"
        )
        
        # è°ƒç”¨DeepSeek API
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 4096
            }
            
            logger.debug(f"å‡†å¤‡è°ƒç”¨DeepSeek APIï¼Œæç¤ºè¯é•¿åº¦: {len(system_prompt) + len(user_prompt)}")
            
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            response.raise_for_status() # å¦‚æœè¯·æ±‚å¤±è´¥(çŠ¶æ€ç  4xx æˆ– 5xx)ï¼Œå°†å¼•å‘ HTTPError
            
            llm_response = response.json()
            
            if not llm_response or "choices" not in llm_response or not llm_response["choices"]:
                logger.error("DeepSeek APIå“åº”æ— æ•ˆæˆ–ä¸åŒ…å«choices")
                raise ValueError("APIå“åº”æ— æ•ˆ: å“åº”ä¸­ç¼ºå°‘ 'choices' å­—æ®µ")
            
            llm_output_text = llm_response['choices'][0]['message']['content']
            preview_length = min(500, len(llm_output_text))
            logger.info(f"DeepSeek APIè°ƒç”¨æˆåŠŸã€‚åŸå§‹è¾“å‡ºé¢„è§ˆ: {llm_output_text[:preview_length]}...")

            # ä»è¾“å‡ºä¸­æå–JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', llm_output_text)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_start = llm_output_text.find('[')
                json_end = llm_output_text.rfind(']')
                if json_start != -1 and json_end != -1 and json_start < json_end:
                    json_str = llm_output_text[json_start:json_end+1]
                else:
                    json_str = llm_output_text # å‡è®¾æ•´ä¸ªè¾“å‡ºæ˜¯JSON
                
            logger.debug(f"å°è¯•è§£æçš„JSONå­—ç¬¦ä¸²: {json_str[:500]}...")
            
            segments = json.loads(json_str) # å¦‚æœè§£æå¤±è´¥ï¼Œä¼šæŠ›å‡º json.JSONDecodeError
            
            if not isinstance(segments, list) or not segments: # æ£€æŸ¥æ˜¯å¦ä¸ºéç©ºåˆ—è¡¨
                logger.warning("è§£æåçš„JSONä¸æ˜¯æœ‰æ•ˆçš„åŒºå—åˆ—è¡¨æˆ–åŒºå—ä¸ºç©º")
                raise ValueError("LLMè¿”å›çš„JSONä¸æ˜¯æœ‰æ•ˆçš„åŒºå—åˆ—è¡¨æˆ–åŒºå—ä¸ºç©º")
                
            logger.info(f"æˆåŠŸè§£æLLMè¿”å›çš„JSONï¼Œè·å¾— {len(segments)} ä¸ªåŸºäºSRTçš„è¯­ä¹‰åŒºå—å®šä¹‰ã€‚")
            return {"segments": segments}
            
        except requests.exceptions.RequestException as e_req:
            logger.error(f"è°ƒç”¨DeepSeek APIè¯·æ±‚å¤±è´¥: {str(e_req)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise # é‡æ–°æŠ›å‡ºè¯·æ±‚å¼‚å¸¸
        except json.JSONDecodeError as e_json:
            logger.error(f"è§£æLLMè¯­ä¹‰åˆ†æ®µç»“æœå¤±è´¥: {e_json}. LLMåŸå§‹è¾“å‡º (å‰200å­—ç¬¦): {llm_output_text[:200]}...")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise ValueError(f"LLMå“åº”JSONè§£æé”™è¯¯: {e_json}. åŸå§‹æ–‡æœ¬: {llm_output_text[:200]}...")
        except ValueError as e_val:
            logger.error(f"å¤„ç†APIå“åº”æ—¶å‘ç”Ÿå€¼é”™è¯¯: {str(e_val)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise # é‡æ–°æŠ›å‡ºå€¼é”™è¯¯
        except Exception as e:
            logger.error(f"è°ƒç”¨DeepSeek APIæˆ–å¤„ç†å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise Exception(f"DeepSeek APIè°ƒç”¨æˆ–å¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}") # æŠ›å‡ºé€šç”¨å¼‚å¸¸

class IntentAnalyzer:
    """è§†é¢‘æ„å›¾åˆ†æç±»"""
    
    def __init__(self, segments=None, target_audience=None, product_type=None, selling_points=None):
        """
        åˆå§‹åŒ–æ„å›¾åˆ†æå™¨
        
        Args:
            segments: è§†é¢‘åˆ†æ®µåˆ—è¡¨
            target_audience: ç›®æ ‡äººç¾¤
            product_type: äº§å“ç±»å‹
            selling_points: äº§å“å–ç‚¹åˆ—è¡¨
        """
        self.segments_data = segments or []
        self.target_audience = target_audience or []
        self.product_type = product_type or []
        self.selling_points = selling_points or []
        self.semantic_analyzer = SemanticAnalyzer()
        
        logger.info(f"åˆå§‹åŒ–æ„å›¾åˆ†æå™¨ï¼Œç›®æ ‡äººç¾¤: {target_audience}, äº§å“ç±»å‹: {product_type}, äº§å“å–ç‚¹: {selling_points}")
    
    def _contains_chinese(self, s: str) -> bool:
        """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        if not s: # å¤„ç†ç©ºå­—ç¬¦ä¸²æˆ–Noneçš„æƒ…å†µ
            return False
        for char in s:
            if '\\u4e00' <= char <= '\\u9fff':
                return True
        return False
    
    def _format_time(self, seconds):
        """
        å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ—¶:åˆ†:ç§’æ ¼å¼
        
        Args:
            seconds: ç§’æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = int(seconds % 60)
        
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def analyze_segments(self, segments_data):
        """
        æ‰¹é‡åˆ†æéŸ³é¢‘ç‰‡æ®µæ•°æ®ï¼Œå°†æ¯ä¸ªç‰‡æ®µå½’ç±»åˆ°ç‰¹å®šçš„è¯­ä¹‰åˆ†ç±»ä¸­ã€‚

        Args:
            segments_data (list): åŒ…å«å¤šä¸ªéŸ³é¢‘ç‰‡æ®µçš„åˆ—è¡¨ï¼Œæ¯ä¸ªç‰‡æ®µåº”è¯¥æœ‰ 'text' å­—æ®µã€‚

        Returns:
            list: åŒ…å«åˆ†æç»“æœçš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªç‰‡æ®µçš„åˆ†ç±»ç»“æœã€‚
        """
        if not segments_data:
            logger.warning("æ²¡æœ‰æä¾›éŸ³é¢‘ç‰‡æ®µæ•°æ®ã€‚")
            return []

        from streamlit_app.config.config import get_semantic_type_definitions, DEFAULT_SEMANTIC_SEGMENT_TYPES

        # åŠ¨æ€è·å–è¯­ä¹‰ç±»å‹å®šä¹‰
        semantic_definitions = get_semantic_type_definitions()
        available_types = DEFAULT_SEMANTIC_SEGMENT_TYPES

        logger.info(f"å¼€å§‹åˆ†æ {len(segments_data)} ä¸ªéŸ³é¢‘ç‰‡æ®µï¼Œè¯­ä¹‰ç±»å‹æ•°é‡: {len(available_types)}")

        results = []
        for i, segment in enumerate(segments_data):
            segment_text = segment.get('text', '')
            if not segment_text:
                logger.warning(f"ç‰‡æ®µ {i} æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡åˆ†æã€‚")
                results.append({
                    'segment_index': i,
                    'text': segment_text,
                    'semantic_type': 'å…¶ä»–',
                    'confidence': 0.0,
                    'analysis_result': 'æ–‡æœ¬ä¸ºç©º'
                })
                continue

            try:
                # æ„å»ºåŠ¨æ€çš„ç±»å‹æè¿°
                type_descriptions = []
                for type_name in available_types:
                    definition = semantic_definitions.get(type_name, {})
                    description = definition.get('description', f'{type_name}ç±»å‹çš„å†…å®¹')
                    keywords = definition.get('keywords', [])
                    examples = definition.get('examples', [])
                    
                    # ç»„åˆæè¿°ä¿¡æ¯
                    full_description = f"{type_name}: {description}"
                    if keywords:
                        full_description += f" å…³é”®è¯ï¼š{', '.join(keywords[:3])}"  # åªæ˜¾ç¤ºå‰3ä¸ªå…³é”®è¯
                    if examples:
                        # ğŸ†• æ˜¾ç¤ºå¤šä¸ªç¤ºä¾‹ï¼Œç”¨åˆ†å·åˆ†éš”ï¼Œæœ€å¤šæ˜¾ç¤º3ä¸ª
                        example_text = "; ".join(examples[:3])
                        full_description += f" ç¤ºä¾‹ï¼š{example_text}"
                    
                    type_descriptions.append(full_description)

                # æ„å»ºç³»ç»Ÿæç¤ºè¯
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿å°†æ¯å©´å¥¶ç²‰è¥é”€è§†é¢‘çš„æ–‡æœ¬ç‰‡æ®µå½’ç±»åˆ°åˆé€‚çš„è¯­ä¹‰ç±»å‹ä¸­ã€‚

å¯é€‰çš„è¯­ä¹‰ç±»å‹åŠå…¶å®šä¹‰ï¼š
{chr(10).join([f"{i+1}. {desc}" for i, desc in enumerate(type_descriptions)])}

è¯·æ ¹æ®æ–‡æœ¬å†…å®¹ï¼Œé€‰æ‹©æœ€åˆé€‚çš„è¯­ä¹‰ç±»å‹ã€‚å¦‚æœæ–‡æœ¬å†…å®¹ä¸æ˜ç¡®æˆ–éš¾ä»¥å½’ç±»ï¼Œè¯·é€‰æ‹©"å…¶ä»–"ã€‚

è¿”å›æ ¼å¼è¦æ±‚ï¼š
- semantic_type: é€‰æ‹©çš„è¯­ä¹‰ç±»å‹åç§°ï¼ˆå¿…é¡»æ˜¯ä¸Šè¿°ç±»å‹ä¹‹ä¸€ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
- reasoning: ç®€çŸ­çš„åˆ†æç†ç”±

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"""

                user_prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µçš„è¯­ä¹‰ç±»å‹ï¼š\n\næ–‡æœ¬å†…å®¹ï¼š{segment_text}"

                # è°ƒç”¨DeepSeek API
                response = self.deepseek_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1,
                    max_tokens=500,
                    stream=False
                )

                response_content = response.choices[0].message.content.strip()
                logger.debug(f"ç‰‡æ®µ {i} DeepSeek API å“åº”: {response_content}")

                # è§£æJSONå“åº”
                try:
                    result_json = json.loads(response_content)
                    semantic_type = result_json.get('semantic_type', 'å…¶ä»–')
                    confidence = float(result_json.get('confidence', 0.5))
                    reasoning = result_json.get('reasoning', 'è‡ªåŠ¨åˆ†æ')

                    # éªŒè¯è¯­ä¹‰ç±»å‹æ˜¯å¦åœ¨å¯é€‰èŒƒå›´å†…
                    if semantic_type not in available_types:
                        logger.warning(f"ç‰‡æ®µ {i} è¿”å›äº†æ— æ•ˆçš„è¯­ä¹‰ç±»å‹ '{semantic_type}'ï¼Œè®¾ç½®ä¸º'å…¶ä»–'")
                        semantic_type = 'å…¶ä»–'
                        confidence = 0.3

                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"ç‰‡æ®µ {i} JSONè§£æå¤±è´¥: {e}, åŸå§‹å“åº”: {response_content}")
                    # å°è¯•ç®€å•çš„æ–‡æœ¬åŒ¹é…ä½œä¸ºåå¤‡
                    semantic_type = self._fallback_semantic_classification(segment_text, available_types, semantic_definitions)
                    confidence = 0.4
                    reasoning = "JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åå¤‡åˆ†ç±»"

                results.append({
                    'segment_index': i,
                    'text': segment_text,
                    'semantic_type': semantic_type,
                    'confidence': confidence,
                    'analysis_result': reasoning
                })

                logger.info(f"ç‰‡æ®µ {i} åˆ†æå®Œæˆ: {semantic_type} (ç½®ä¿¡åº¦: {confidence:.2f})")

            except Exception as e:
                logger.error(f"åˆ†æç‰‡æ®µ {i} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                results.append({
                    'segment_index': i,
                    'text': segment_text,
                    'semantic_type': 'å…¶ä»–',
                    'confidence': 0.0,
                    'analysis_result': f'åˆ†æå¤±è´¥: {str(e)}'
                })

        logger.info(f"å®Œæˆæ‰€æœ‰ {len(segments_data)} ä¸ªç‰‡æ®µçš„è¯­ä¹‰åˆ†æ")
        return results

    def _fallback_semantic_classification(self, text: str, available_types: list, semantic_definitions: dict) -> str:
        """
        åå¤‡çš„è¯­ä¹‰åˆ†ç±»æ–¹æ³•ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…
        
        Args:
            text: è¦åˆ†ç±»çš„æ–‡æœ¬
            available_types: å¯ç”¨çš„è¯­ä¹‰ç±»å‹åˆ—è¡¨
            semantic_definitions: è¯­ä¹‰ç±»å‹å®šä¹‰å­—å…¸
            
        Returns:
            str: åˆ†ç±»ç»“æœ
        """
        text_lower = text.lower()
        
        # æŒ‰ç±»å‹æ£€æŸ¥å…³é”®è¯åŒ¹é…
        for type_name in available_types:
            if type_name == 'å…¶ä»–':
                continue
                
            definition = semantic_definitions.get(type_name, {})
            keywords = definition.get('keywords', [])
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    logger.info(f"åå¤‡åˆ†ç±»ï¼šåŸºäºå…³é”®è¯ '{keyword}' å°†æ–‡æœ¬åˆ†ç±»ä¸º '{type_name}'")
                    return type_name
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œè¿”å›"å…¶ä»–"
        return 'å…¶ä»–'

def main_analysis_pipeline(video_path, target_audience=None, product_type=None, selling_points_config_representation=None, additional_info=None):
    """
    å®Œæ•´çš„è§†é¢‘åˆ†ææµæ°´çº¿ï¼Œå°†è§†é¢‘åˆ†æ®µå’Œæ„å›¾åˆ†æç»“åˆèµ·æ¥
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        target_audience: ç›®æ ‡äººç¾¤ï¼Œé»˜è®¤ä¸ºNone
        product_type: äº§å“ç±»å‹ï¼Œé»˜è®¤ä¸ºNone (æ­¤å‚æ•°åœ¨æ­¤ç®€åŒ–æµç¨‹ä¸­ä¸å†ç”¨äºäº§å“ç±»å‹åŒ¹é…)
        selling_points_config_representation: ç”¨äºç¼“å­˜æ§åˆ¶çš„äº§å“å–ç‚¹è¡¨ç¤º (ä¾‹å¦‚å…ƒç»„)
        additional_info: é™„åŠ çš„åˆ†æä¿¡æ¯ï¼Œé»˜è®¤ä¸ºNone
        
    Returns:
        Tuple: (åˆ†æç»“æœåˆ—è¡¨, å®Œæ•´çš„è½¬å½•æ•°æ®å­—å…¸) æˆ– ([], None) å¦‚æœå¤±è´¥
    """
    from streamlit_app.modules.data_process.video_segmenter import segment_video # ä¿æŒå±€éƒ¨å¯¼å…¥
    
    full_transcript_data = None # åˆå§‹åŒ–
    analysis_results_placeholder = [] # è¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨ä½œä¸ºåˆ†æç»“æœçš„å ä½ç¬¦
    try:
        # 1. è§†é¢‘åˆ†æ®µï¼Œå¹¶è·å–å®Œæ•´è½¬å½•æ•°æ® (åŒ…æ‹¬SRTæ–‡ä»¶è·¯å¾„å’Œå†…å®¹)
        segments, full_transcript_data = segment_video(video_path)
        
        if not segments:
            logger.warning(f"è§†é¢‘åˆ†æ®µç»“æœä¸ºç©ºæˆ–è·å–è½¬å½•æ•°æ®å¤±è´¥: {video_path}")
            # å³ä½¿åˆ†æ®µä¸ºç©ºï¼Œä¹Ÿè¿”å›è½¬å½•æ•°æ®å’Œç©ºåˆ†æç»“æœå ä½ç¬¦
            return analysis_results_placeholder, full_transcript_data 
        
        # äº§å“ç±»å‹è¯†åˆ«å’Œç›®æ ‡äººç¾¤è¯†åˆ«å°†ä¸»è¦åœ¨ streamlit_app/app.py ä¸­é€šè¿‡åˆ†æSRTæ–‡ä»¶å†…å®¹è¿›è¡Œã€‚
        # IntentAnalyzer.analyze_segment ä¸­çš„äº§å“ç±»å‹å’Œäººç¾¤åŒ¹é…é€»è¾‘å·²å¤§éƒ¨åˆ†ç§»é™¤æˆ–ä¸å†æ ¸å¿ƒã€‚
        # æ­¤å¤„ä¸å†è°ƒç”¨ analyze_video_segments è¿›è¡ŒåŸºäºå…³é”®è¯çš„å–ç‚¹ç­‰åˆ†æï¼Œ
        # å› ä¸ºå…¶ç»“æœï¼ˆå¦‚ matched_selling_pointsï¼‰å½“å‰æœªåœ¨UIä¸Šç›´æ¥ä½¿ç”¨ã€‚
        # å¦‚æœå°†æ¥éœ€è¦è¿™äº›è¯¦ç»†çš„åŸºäºç‰‡æ®µçš„åˆ†æï¼Œå¯ä»¥é‡æ–°å¯ç”¨æˆ–è°ƒæ•´æ­¤éƒ¨åˆ†ã€‚
        
        # å½“å‰ï¼Œmain_analysis_pipeline çš„æ ¸å¿ƒè¾“å‡ºæ˜¯ segments (ç”¨äºUIæ˜¾ç¤ºåˆ†æ®µè§†é¢‘)
        # å’Œ full_transcript_data (ç”¨äº app.py ä¸­æå–SRTè·¯å¾„/å†…å®¹è¿›è¡ŒLLMåˆ†æ)
        
        # ç›´æ¥ä½¿ç”¨ segments ä½œä¸ºä¸€ç§å½¢å¼çš„ "åˆ†æç»“æœ" è¿”å›ç»™ app.pyï¼Œ
        # app.py ä¸»è¦æ¶ˆè´¹çš„æ˜¯ segments åˆ—è¡¨æœ¬èº«ï¼Œè€Œä¸æ˜¯å†…éƒ¨æ›´ç»†è‡´çš„åŒ¹é…å­—æ®µã€‚
        # æˆ–è€…ï¼Œå¦‚æœ app.py ä»…éœ€ segments å’Œ full_transcript_dataï¼Œå¯ä»¥è®© analysis_results ä¸ºç©ºã€‚
        # ä¸ºäº†ä¿æŒè¿”å›ç»“æ„ä¸€è‡´æ€§ï¼Œä½†è¡¨æ˜è¿™éƒ¨åˆ†åˆ†æè¢«è·³è¿‡ï¼Œæˆ‘ä»¬è¿”å›åŸå§‹çš„segments
        # æˆ–è€…ä¸€ä¸ªæ˜ç¡®çš„ç©ºåˆ—è¡¨ã€‚è€ƒè™‘åˆ° app.py ä¸­å¯¹ analysis_results çš„è¿­ä»£ï¼Œè¿”å› segments æ›´åˆé€‚ã€‚
        
        # logger.info(f"è§†é¢‘ç‰‡æ®µæ„å›¾åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(analysis_results)} ä¸ªåŒ¹é…ç‰‡æ®µ for {video_path}")
        # è¿”å› segments (è¯­ä¹‰åˆ†æ®µç»“æœ) å’Œ full_transcript_data
        # app.py å°†åŸºäº segments è¿­ä»£æ˜¾ç¤ºï¼Œå¹¶ä½¿ç”¨ full_transcript_data è¿›è¡ŒLLMåˆ†æ
        return segments, full_transcript_data
    except Exception as e:
        logger.error(f"è§†é¢‘åˆ†ææµæ°´çº¿æ‰§è¡Œå¤±è´¥ ({video_path}): {str(e)}")
        return analysis_results_placeholder, full_transcript_data # å°½é‡è¿”å›è½¬å½•æ•°æ®ï¼Œå³ä½¿åç»­åˆ†æå¤±è´¥ 