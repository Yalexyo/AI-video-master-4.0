"""
è§†é¢‘ç‰‡æ®µåˆ†ææ¨¡å—

æ­¤æ¨¡å—ä¸“é—¨ç”¨äºåˆ†æè§†é¢‘ç‰‡æ®µï¼Œæå–äº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹ä¿¡æ¯ã€‚
ä½¿ç”¨DeepSeek APIè¿›è¡Œæ™ºèƒ½åˆ†æï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†ä»¥æé«˜æ•ˆç‡ã€‚
"""

import json
import logging
import asyncio
import concurrent.futures
from typing import List, Dict, Any, Tuple
import streamlit as st
from pathlib import Path
import os
import requests

from streamlit_app.config.config import get_config

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# é»˜è®¤åˆ†ææç¤ºè¯æ¨¡æ¿
DEFAULT_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¯å©´å¥¶ç²‰äº§å“åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æè§†é¢‘ç‰‡æ®µæ–‡æœ¬ï¼Œå‡†ç¡®è¯†åˆ«ä»¥ä¸‹ä¿¡æ¯ï¼š

1. ã€äº§å“ç±»å‹ã€‘: ä»æ–‡æœ¬ä¸­è¯†åˆ«å…·ä½“æåˆ°çš„å¥¶ç²‰äº§å“ç±»å‹ï¼Œå¯é€‰é¡¹ç›®åŒ…æ‹¬ï¼š{product_types}
   - åªæœ‰åœ¨æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°æˆ–æš—ç¤ºç‰¹å®šäº§å“æ—¶æ‰è¾“å‡ºäº§å“ç±»å‹
   - æ¯ä¸ªç‰‡æ®µæœ€å¤šåªèƒ½è¯†åˆ«1ç§äº§å“ç±»å‹
   - å¦‚æœæ²¡æœ‰æ˜ç¡®çš„äº§å“ä¿¡æ¯ï¼Œåˆ™è¾“å‡ºç©ºå­—ç¬¦ä¸²

2. ã€æ ¸å¿ƒå–ç‚¹ã€‘: ä»æ–‡æœ¬ä¸­è¯†åˆ«æåˆ°çš„äº§å“æ ¸å¿ƒå–ç‚¹ï¼Œå¯é€‰é¡¹ç›®åŒ…æ‹¬ï¼š{selling_points}
   - å¯ä»¥è¯†åˆ«å¤šä¸ªå–ç‚¹
   - åªæœ‰åœ¨æ–‡æœ¬ä¸­æ˜ç¡®æåˆ°æˆ–æš—ç¤ºç›¸å…³æ¦‚å¿µæ—¶æ‰è¾“å‡º
   - å¦‚æœæ²¡æœ‰ç›¸å…³å–ç‚¹ä¿¡æ¯ï¼Œåˆ™è¾“å‡ºç©ºæ•°ç»„

æ³¨æ„è¯†åˆ«è¦ç‚¹ï¼š
- "å¯èµ‹æ°´å¥¶"ç›¸å…³è¡¨è¿°: "æ°´å¥¶"ã€"æ¶²æ€å¥¶"ã€"æ‰“å¼€ç›–å­å°±èƒ½å–"ã€"å¼€ç›–å³é¥®"ã€"ä¸ç”¨å†²è°ƒ"
- "å¯èµ‹è•´æ·‡"ç›¸å…³è¡¨è¿°: "è•´æ·‡"ã€"é«˜ç«¯é…æ–¹"ã€"æ——èˆ°äº§å“"
- "å¯èµ‹è“é’»"ç›¸å…³è¡¨è¿°: "è“é’»"ã€"è¶…é«˜ç«¯"ã€"é¡¶çº§é…æ–¹"
- "HMO & æ¯ä¹³ä½èšç³–"ç›¸å…³è¡¨è¿°: "HMO"ã€"æ¯ä¹³ä½èšç³–"ã€"äººä¹³å¯¡ç³–"
- "A2å¥¶æº"ç›¸å…³è¡¨è¿°: "A2è›‹ç™½"ã€"A2å‹è›‹ç™½"ã€"A2å¥¶ç‰›"
- "è‡ªæ„ˆåŠ›"ç›¸å…³è¡¨è¿°: "è‡ªæŠ¤åŠ›"ã€"è‡ªå¾¡åŠ›"ã€"å…ç–«åŠ›"ã€"æŠµæŠ—åŠ›"

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "product_type": "å¯èµ‹æ°´å¥¶", 
  "selling_points": ["HMO & æ¯ä¹³ä½èšç³–", "å¼€ç›–å³é¥®"]
}}"""

class SegmentAnalyzer:
    """ç‰‡æ®µåˆ†æå™¨ï¼Œä¸“é—¨ç”¨äºåˆ†æå•ä¸ªè§†é¢‘ç‰‡æ®µçš„äº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹"""
    
    def __init__(self):
        """åˆå§‹åŒ–SegmentAnalyzer"""
        try:
            self.api_key = os.environ.get("DEEPSEEK_API_KEY")
            if not self.api_key:
                raise ValueError("DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
            
        self.base_url = "https://api.deepseek.com"
        self.model = "deepseek-chat"
            self.requests = requests
            
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„prompté…ç½®
            try:
                from streamlit_app.utils.keyword_config import sync_prompt_templates, get_keyword_config
                config = get_keyword_config()
                
                # ä»é…ç½®æ–‡ä»¶è·å–äº§å“ç±»å‹å’Œå–ç‚¹
                self.product_types = config.get("product_types", [
                    "å¯èµ‹è•´æ·³",
                    "å¯èµ‹æ°´å¥¶", 
                    "å¯èµ‹è“é’»",
                    "å…¶ä»–å¥¶ç²‰äº§å“"
                ])
                
                self.selling_points = config.get("selling_points", [
                    "A2è›‹ç™½",
                    "HMOæ¯ä¹³ä½èšç³–", 
                    "DHA",
                    "è¥å…»é…æ–¹",
                    "ç§‘å­¦å–‚å…»",
                    "å…ç–«åŠ›",
                    "æ¶ˆåŒ–å¸æ”¶",
                    "å¤§è„‘å‘è‚²"
                ])
                
                # ä½¿ç”¨ç»Ÿä¸€çš„åˆ†æprompt
                self.analysis_prompt = config.get("analysis_prompt", self._get_fallback_prompt())
                
            except Exception as e:
                logger.warning(f"æ— æ³•å¯¼å…¥ç»Ÿä¸€é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
                self.product_types = [
                    "å¯èµ‹è•´æ·³",
                    "å¯èµ‹æ°´å¥¶", 
                    "å¯èµ‹è“é’»",
                    "å…¶ä»–å¥¶ç²‰äº§å“"
                ]
                
                self.selling_points = [
                    "A2è›‹ç™½",
                    "HMOæ¯ä¹³ä½èšç³–", 
                    "DHA",
                    "è¥å…»é…æ–¹",
                    "ç§‘å­¦å–‚å…»",
                    "å…ç–«åŠ›",
                    "æ¶ˆåŒ–å¸æ”¶",
                    "å¤§è„‘å‘è‚²"
                ]
                
                self.analysis_prompt = self._get_fallback_prompt()
                
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ–SegmentAnalyzerå¤±è´¥: {e}")
    
    def _get_fallback_prompt(self) -> str:
        """è·å–å…œåº•promptæ¨¡æ¿"""
        return """ä½ æ˜¯ä¸“ä¸šçš„æ¯å©´äº§å“åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«è§†é¢‘å†…å®¹ä¸­çš„äº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹ã€‚

é¢„å®šä¹‰çš„äº§å“ç±»å‹åˆ—è¡¨ï¼š
{product_types}

é¢„å®šä¹‰çš„æ ¸å¿ƒå–ç‚¹åˆ—è¡¨ï¼š
{selling_points}

è¯·åˆ†æè§†é¢‘ç‰‡æ®µæ–‡æœ¬ï¼Œè¯†åˆ«å…¶ä¸­æåˆ°çš„äº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹ã€‚

è¾“å‡ºè¦æ±‚ï¼š
1. åªèƒ½ä»é¢„å®šä¹‰åˆ—è¡¨ä¸­é€‰æ‹©äº§å“ç±»å‹å’Œå–ç‚¹
2. å¦‚æœæœªæ˜ç¡®æåˆ°äº§å“ï¼Œproduct_typeè¿”å›ç©ºå­—ç¬¦ä¸²
3. å–ç‚¹å¯ä»¥é€‰æ‹©å¤šä¸ªï¼Œä½†å¿…é¡»ç¡®å®åœ¨æ–‡æœ¬ä¸­æœ‰ä½“ç°
4. è¿”å›æ ‡å‡†JSONæ ¼å¼

è¾“å‡ºæ ¼å¼ï¼š
{{
  "product_type": "äº§å“ç±»å‹åç§°æˆ–ç©ºå­—ç¬¦ä¸²",
  "selling_points": ["å–ç‚¹1", "å–ç‚¹2"]
}}"""

    def analyze_single_segment(self, segment_text: str, semantic_type: str) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªè§†é¢‘ç‰‡æ®µï¼Œè¯†åˆ«äº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹
        
        Args:
            segment_text: ç‰‡æ®µæ–‡æœ¬å†…å®¹
            semantic_type: ç‰‡æ®µçš„è¯­ä¹‰ç±»å‹
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸: {"product_type": str, "selling_points": List[str]}
        """
        if not segment_text or not segment_text.strip():
            return {"product_type": "", "selling_points": []}
        
        # ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„promptæ¨¡æ¿
        system_prompt = self.analysis_prompt.format(
            product_types=json.dumps(self.product_types, ensure_ascii=False),
            selling_points=json.dumps(self.selling_points, ensure_ascii=False)
        )

        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹è§†é¢‘ç‰‡æ®µæ–‡æœ¬ï¼ˆè¯­ä¹‰ç±»å‹ï¼š{semantic_type}ï¼‰ï¼Œè¯†åˆ«å…¶ä¸­çš„äº§å“ç±»å‹å’Œæ ¸å¿ƒå–ç‚¹ï¼š

æ–‡æœ¬å†…å®¹ï¼š
{segment_text}

è¯·æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚"""

        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                "max_tokens": 500
            }
            
            response = self.requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            content = result['choices'][0]['message']['content']
            
            # è§£æJSONå“åº”
            try:
                # æ¸…ç†Markdownæ ¼å¼
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                analysis_result = json.loads(content)
                
                # éªŒè¯å’Œæ¸…ç†ç»“æœ
                product_type = analysis_result.get("product_type", "")
                selling_points = analysis_result.get("selling_points", [])
                
                # ç¡®ä¿äº§å“ç±»å‹åœ¨å…è®¸åˆ—è¡¨ä¸­
                if product_type and product_type not in self.product_types:
                    logger.warning(f"è¯†åˆ«åˆ°æœªçŸ¥äº§å“ç±»å‹: {product_type}ï¼Œå°†å…¶ç½®ä¸ºç©º")
                    product_type = ""
                
                # ç¡®ä¿å–ç‚¹åœ¨å…è®¸åˆ—è¡¨ä¸­
                valid_selling_points = [sp for sp in selling_points if sp in self.selling_points]
                if len(valid_selling_points) != len(selling_points):
                    logger.warning(f"è¿‡æ»¤äº†ä¸€äº›æ— æ•ˆçš„å–ç‚¹ï¼ŒåŸå§‹: {selling_points}ï¼Œæœ‰æ•ˆ: {valid_selling_points}")
                
                return {
                    "product_type": product_type,
                    "selling_points": valid_selling_points
                }
                
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}ï¼ŒåŸå§‹å†…å®¹: {content[:200]}...")
                return {"product_type": "", "selling_points": []}
                
        except Exception as e:
            logger.error(f"åˆ†æç‰‡æ®µæ—¶å‡ºé”™: {e}")
            return {"product_type": "", "selling_points": []}

def analyze_segments_batch(segments_data: List[Dict[str, Any]], max_workers: int = 5) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡åˆ†æå¤šä¸ªç‰‡æ®µï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†æé«˜æ•ˆç‡
    
    Args:
        segments_data: ç‰‡æ®µæ•°æ®åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
        
    Returns:
        åŒ…å«åˆ†æç»“æœçš„ç‰‡æ®µæ•°æ®åˆ—è¡¨
    """
    if not segments_data:
        return []
    
    analyzer = SegmentAnalyzer()
    
    def analyze_segment_wrapper(segment: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†"""
        try:
            text = segment.get("transcript", "") or segment.get("text", "")
            semantic_type = segment.get("semantic_type", "") or segment.get("type", "")
            
            # æ‰§è¡Œåˆ†æ
            analysis_result = analyzer.analyze_single_segment(text, semantic_type)
            
            # å°†ç»“æœæ·»åŠ åˆ°ç‰‡æ®µæ•°æ®ä¸­
            segment_copy = segment.copy()
            segment_copy.update({
                "analyzed_product_type": analysis_result["product_type"],
                "analyzed_selling_points": analysis_result["selling_points"]
            })
            
            return segment_copy
            
        except Exception as e:
            logger.error(f"åˆ†æç‰‡æ®µæ—¶å‡ºé”™: {e}")
            # è¿”å›åŸå§‹ç‰‡æ®µæ•°æ®ï¼Œæ·»åŠ ç©ºçš„åˆ†æç»“æœ
            segment_copy = segment.copy()
            segment_copy.update({
                "analyzed_product_type": "",
                "analyzed_selling_points": []
            })
            return segment_copy
    
    # ä½¿ç”¨Streamlitçš„è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    analyzed_segments = []
    
    # ä½¿ç”¨å¹¶å‘å¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_segment = {
            executor.submit(analyze_segment_wrapper, segment): i 
            for i, segment in enumerate(segments_data)
        }
        
        # æ”¶é›†ç»“æœ
        for i, future in enumerate(concurrent.futures.as_completed(future_to_segment)):
            try:
                result = future.result()
                analyzed_segments.append(result)
                
                # æ›´æ–°è¿›åº¦
                progress = (i + 1) / len(segments_data)
                progress_bar.progress(progress)
                status_text.text(f"æ­£åœ¨åˆ†æç‰‡æ®µ... ({i + 1}/{len(segments_data)})")
                
            except Exception as e:
                logger.error(f"å¤„ç†ç‰‡æ®µåˆ†æç»“æœæ—¶å‡ºé”™: {e}")
                # æ·»åŠ ä¸€ä¸ªå¸¦é”™è¯¯æ ‡è®°çš„ç»“æœ
                original_segment = segments_data[future_to_segment[future]]
                error_segment = original_segment.copy()
                error_segment.update({
                    "analyzed_product_type": "",
                    "analyzed_selling_points": [],
                    "analysis_error": str(e)
                })
                analyzed_segments.append(error_segment)
    
    # æ¸…ç†è¿›åº¦æ¡
    progress_bar.empty()
    status_text.empty()
    
    # æŒ‰åŸå§‹é¡ºåºæ’åºï¼ˆå› ä¸ºå¹¶å‘å®Œæˆé¡ºåºå¯èƒ½ä¸åŒï¼‰
    analyzed_segments.sort(key=lambda x: segments_data.index(
        next(seg for seg in segments_data 
             if seg.get("filename") == x.get("filename") or 
             seg.get("start_time", 0) == x.get("start_time", 0))
    ))
    
    logger.info(f"å®Œæˆ {len(analyzed_segments)} ä¸ªç‰‡æ®µçš„åˆ†æ")
    return analyzed_segments

@st.cache_data(ttl=3600)  # ç¼“å­˜1å°æ—¶
def cached_analyze_segments(segments_key: str, segments_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å¸¦ç¼“å­˜çš„ç‰‡æ®µåˆ†æå‡½æ•°
    
    Args:
        segments_key: ç”¨äºç¼“å­˜çš„å”¯ä¸€é”®å€¼
        segments_data: ç‰‡æ®µæ•°æ®åˆ—è¡¨
        
    Returns:
        åˆ†æç»“æœåˆ—è¡¨
    """
    return analyze_segments_batch(segments_data) 