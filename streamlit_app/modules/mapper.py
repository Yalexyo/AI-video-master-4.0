"""
è§†é¢‘ç‰‡æ®µæ˜ å°„æ¨¡å—
ç”¨äºå°†video_poolä¸­çš„è§†é¢‘ç‰‡æ®µè‡ªåŠ¨æ˜ å°„åˆ°å››å¤§æ¨¡å—ï¼šç—›ç‚¹ã€è§£å†³æ–¹æ¡ˆå¯¼å…¥ã€å–ç‚¹Â·æˆåˆ†&é…æ–¹ã€ä¿ƒé”€æœºåˆ¶
"""

import logging
import json
import os
import re
import glob
import subprocess
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import threading
import time
import streamlit as st

from modules.selection_logger import get_selection_logger
# from .quality_analyzer import QualityAnalyzer  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œæ–‡ä»¶ä¸å­˜åœ¨
# from .ai_analyzers.analyzer_factory import AnalyzerFactory  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œæ–‡ä»¶ä¸å­˜åœ¨
# from .data_models import Segment, MappedSegment, VideoAnalysisResult, SceneInfo  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œæ–‡ä»¶ä¸å­˜åœ¨
from utils.config_manager import get_config_manager
from utils.path_utils import get_project_root
from modules.ai_analyzers import DeepSeekAnalyzer

logger = logging.getLogger(__name__)

def resolve_video_pool_path(relative_path: str = "data/output/google_video/video_pool") -> str:
    """
    ğŸ”§ è·¨å¹³å°å…¼å®¹çš„video_poolè·¯å¾„è§£æ
    
    è§£å†³streamlit_appå·¥ä½œç›®å½•ä¸‹ç›¸å¯¹è·¯å¾„æ‰¾ä¸åˆ°../dataçš„é—®é¢˜
    
    Args:
        relative_path: ç›¸å¯¹è·¯å¾„ï¼Œé»˜è®¤ä¸ºdata/output/google_video/video_pool
        
    Returns:
        str: è§£æåçš„ç»å¯¹è·¯å¾„
    """
    # æ–¹æ¡ˆ1ï¼šå°è¯•å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ç›¸å¯¹è·¯å¾„
    if os.path.exists(relative_path):
        abs_path = os.path.abspath(relative_path)
        logger.debug(f"âœ… è·¯å¾„è§£ææˆåŠŸ(å½“å‰ç›®å½•): {relative_path} -> {abs_path}")
        return abs_path
    
    # æ–¹æ¡ˆ2ï¼šå°è¯•ä¸Šçº§ç›®å½•ä¸‹çš„ç›¸å¯¹è·¯å¾„ï¼ˆé€‚ç”¨äºstreamlit_appå·¥ä½œç›®å½•ï¼‰
    parent_relative_path = os.path.join("..", relative_path)
    if os.path.exists(parent_relative_path):
        abs_path = os.path.abspath(parent_relative_path)
        logger.debug(f"âœ… è·¯å¾„è§£ææˆåŠŸ(ä¸Šçº§ç›®å½•): {parent_relative_path} -> {abs_path}")
        return abs_path
    
    # æ–¹æ¡ˆ3ï¼šä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•æ‹¼æ¥
    try:
        project_root = get_project_root()
        project_based_path = os.path.join(project_root, relative_path)
        if os.path.exists(project_based_path):
            logger.debug(f"âœ… è·¯å¾„è§£ææˆåŠŸ(é¡¹ç›®æ ¹ç›®å½•): {project_based_path}")
            return project_based_path
    except Exception as e:
        logger.debug(f"é¡¹ç›®æ ¹ç›®å½•æ–¹æ³•å¤±è´¥: {e}")
    
    # æ–¹æ¡ˆ4ï¼šåŸºäºå½“å‰æ–‡ä»¶ä½ç½®æ¨æ–­
    current_file_dir = os.path.dirname(os.path.abspath(__file__))
    # å½“å‰æ–‡ä»¶åœ¨ streamlit_app/modules/mapper.py
    # éœ€è¦è·³åˆ°é¡¹ç›®æ ¹ç›®å½•: ../../data/output/google_video/video_pool
    inferred_path = os.path.join(current_file_dir, "..", "..", relative_path)
    if os.path.exists(inferred_path):
        abs_path = os.path.abspath(inferred_path)
        logger.debug(f"âœ… è·¯å¾„è§£ææˆåŠŸ(æ¨æ–­è·¯å¾„): {inferred_path} -> {abs_path}")
        return abs_path
    
    # æ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹è·¯å¾„å¹¶è®°å½•è­¦å‘Š
    logger.warning(f"âš ï¸ æ— æ³•è§£ævideo_poolè·¯å¾„ï¼Œæ‰€æœ‰å°è¯•çš„è·¯å¾„éƒ½ä¸å­˜åœ¨:")
    logger.warning(f"   1. {os.path.abspath(relative_path)}")
    logger.warning(f"   2. {os.path.abspath(parent_relative_path)}")
    if 'project_based_path' in locals():
        logger.warning(f"   3. {project_based_path}")
    logger.warning(f"   4. {os.path.abspath(inferred_path)}")
    logger.warning(f"   å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    return os.path.abspath(relative_path)  # è¿”å›ç»å¯¹è·¯å¾„ï¼Œå³ä½¿ä¸å­˜åœ¨

class VideoSegmentMapper:
    """
    è§†é¢‘ç‰‡æ®µæ˜ å°„å™¨ï¼Œè´Ÿè´£å°†AIåˆ†æçš„æ ‡ç­¾æ˜ å°„åˆ°ä¸šåŠ¡æ¨¡å—ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ˜ å°„å™¨"""
        self.four_modules = ["ç—›ç‚¹", "è§£å†³æ–¹æ¡ˆå¯¼å…¥", "å–ç‚¹Â·æˆåˆ†&é…æ–¹", "ä¿ƒé”€æœºåˆ¶"]
        
        # ğŸ”§ åˆå§‹åŒ–DeepSeekåˆ†æå™¨
        try:
            self.deepseek_analyzer = DeepSeekAnalyzer()
            logger.info("DeepSeekåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        except ImportError as e:
            logger.warning(f"æ— æ³•å¯¼å…¥DeepSeekåˆ†æå™¨: {e}")
            self.deepseek_analyzer = None
        except Exception as e:
            logger.error(f"DeepSeekåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.deepseek_analyzer = None
        
        # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®ä¸­å¿ƒåŠ è½½è§„åˆ™
        try:
            config_manager = get_config_manager()
            self.keyword_rules = config_manager.get_matching_rules()
            
            # ä»åŸå§‹é…ç½®ä¸­æå–ç—›ç‚¹è§„åˆ™å’Œå“ç‰Œå…³é”®è¯ç”¨äºç‰¹æ®Šå¤„ç†ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
            # æ³¨æ„ï¼šå¤§éƒ¨åˆ†é€»è¾‘åº”ç›´æ¥ä½¿ç”¨ get_matching_rules() çš„ç»“æœ
            raw_config = config_manager.get_raw_config()
            self.pain_point_rules = raw_config.get("pain_points", {}) # ç¤ºä¾‹ï¼Œå¯èƒ½ä¸å†éœ€è¦
            self.brand_priority_keywords = raw_config.get("features_formula", {}).get("brands", [])

            logger.info("ğŸ¯ æ˜ å°„å™¨é…ç½®åŠ è½½æˆåŠŸï¼Œä½¿ç”¨ç»Ÿä¸€é…ç½®ä¸­å¿ƒ")
            logger.info(f"   æ¨¡å—æ•°é‡: {len(self.keyword_rules)}")
            
        except Exception as e:
            logger.error(f"æ— æ³•ä»é…ç½®ä¸­å¿ƒåŠ è½½è§„åˆ™ï¼Œå°†ä½¿ç”¨é»˜è®¤å…œåº•é…ç½®: {e}")
            # å…œåº•é…ç½®
            self.keyword_rules = {
                "ç—›ç‚¹": {"core_identity": ["åŒ»é™¢", "å“­é—¹", "å‘çƒ§"]},
                "è§£å†³æ–¹æ¡ˆå¯¼å…¥": {"core_identity": ["å†²å¥¶", "å¥¶ç²‰ç½", "å¥¶ç“¶"]},
                "å–ç‚¹Â·æˆåˆ†&é…æ–¹": {"core_identity": ["A2", "HMO", "DHA", "å¯èµ‹"]},
                "ä¿ƒé”€æœºåˆ¶": {"core_identity": ["ä¼˜æƒ ", "é™æ—¶", "ä¿ƒé”€"]}
            }
            self.pain_point_rules = {}
            self.brand_priority_keywords = ["å¯èµ‹", "illuma", "Wyeth", "A2", "ATWO", "HMO", "DHA"]
        
        # ğŸ”§ Intentionally disable embedding models
        logger.info("EMBEDDING MODELS ARE DISABLED. Classification will rely on keywords and DeepSeek API.")
        self.embedding_model = None
        self.embedding_util = None
    
        # ğŸ”§ ã€å·²å½»åº•ç§»é™¤ã€‘ä¸å†éœ€è¦æ‰‹åŠ¨åŠ è½½matching_rules.jsonï¼Œå®Œå…¨ä¾èµ–ConfigManager
        # ç¡®ä¿ruleså±æ€§å­˜åœ¨ï¼Œç”¨äºå‘åå…¼å®¹æ€§
        if not hasattr(self, 'keyword_rules') or not self.keyword_rules:
            logger.error("ConfigManageræœªèƒ½æä¾›ä»»ä½•è§„åˆ™ï¼Œå°†ä½¿ç”¨ä¸€ä¸ªç©ºçš„é»˜è®¤é…ç½®ã€‚")
            self.keyword_rules = self._create_default_rules()
        
        # ä¸ºå‘åå…¼å®¹ï¼ŒåŒæ—¶è®¾ç½®ruleså±æ€§
        self.rules = self.keyword_rules
                
    def _load_matching_rules(self):
        """
        ã€å·²åºŸå¼ƒã€‘æ­¤æ–¹æ³•ä¸å†ä½¿ç”¨ã€‚æ‰€æœ‰è§„åˆ™å‡ç”±ConfigManagerç»Ÿä¸€æä¾›ã€‚
        """
        logger.warning("è°ƒç”¨äº†å·²åºŸå¼ƒçš„_load_matching_rulesæ–¹æ³•ï¼Œæ­¤æ–¹æ³•ä¸åº”å†è¢«ä½¿ç”¨ã€‚")
        self.rules = self._create_default_rules()
    
    def _create_default_rules(self) -> dict:
        """
        åˆ›å»ºé»˜è®¤çš„è§„åˆ™é…ç½®ï¼ˆä½œä¸ºfallbackï¼‰
        
        Returns:
            é»˜è®¤é…ç½®å­—å…¸
        """
        return {
            "ç—›ç‚¹": {
                "object_keywords": ["å®å®", "å©´å„¿", "æ–°ç”Ÿå„¿"],
                "scene_keywords": ["å“­é—¹", "ä¸å®‰", "éš¾å—"],
                "emotion_keywords": ["ç„¦è™‘", "æ‹…å¿ƒ", "å›°æ‰°"],
                "negative_keywords": ["å¼€å¿ƒ", "å¿«ä¹", "æ»¡æ„"],
                "required_keywords": [],
                "min_score_threshold": 0.3
            },
            "å–ç‚¹Â·æˆåˆ†&é…æ–¹": {
                "object_keywords": ["å¥¶ç²‰", "é…æ–¹", "è¥å…»"],
                "scene_keywords": ["äº§å“", "å±•ç¤º", "ä»‹ç»"],
                "emotion_keywords": ["ä¸“ä¸š", "ç§‘å­¦", "å®‰å…¨"],
                "negative_keywords": ["å“­é—¹", "é—®é¢˜", "éš¾å—"],
                "required_keywords": [],
                "min_score_threshold": 0.3
            },
            "è§£å†³æ–¹æ¡ˆå¯¼å…¥": {
                "object_keywords": ["å»ºè®®", "æ–¹æ³•", "è§£å†³"],
                "scene_keywords": ["æŒ‡å¯¼", "æ•™å­¦", "æ¼”ç¤º"],
                "emotion_keywords": ["ä¸“ä¸š", "ä¿¡ä»»", "å®‰å¿ƒ"],
                "negative_keywords": ["äº§å“", "æ¨é”€", "å¹¿å‘Š"],
                "required_keywords": [],
                "min_score_threshold": 0.3
            },
            "ä¿ƒé”€æœºåˆ¶": {
                "object_keywords": ["å®å®", "å©´å„¿", "å­©å­"],
                "scene_keywords": ["å¼€å¿ƒ", "æ´»åŠ›", "å¥åº·"],
                "emotion_keywords": ["å¿«ä¹", "æ»¡æ„", "æˆé•¿"],
                "negative_keywords": ["å“­é—¹", "é—®é¢˜", "æ‹…å¿ƒ"],
                "required_keywords": [],
                "min_score_threshold": 0.3
            },
            "GLOBAL_SETTINGS": {
                "global_exclusion_keywords": ["ç–‘ä¼¼", "æ¨¡ç³Š", "ä¸æ¸…æ¥š"],
                "irrelevant_scene_categories": {
                    "æ— å…³åœºæ™¯": ["å¹¿å‘Š", "logo", "æ–‡å­—"]
                }
            }
        }
    
    def _init_embedding_model_offline(self):
        """ç¦»çº¿æ¨¡å¼ä¸‹åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            import os
            import torch
            from pathlib import Path
            
            # æ£€æŸ¥ç¦»çº¿é…ç½®æ–‡ä»¶
            offline_config_path = Path("config/offline_config.py")
            if not offline_config_path.exists():
                logger.debug("ç¦»çº¿é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç¦»çº¿æ¨¡å¼")
                return False
            
            # æ£€æŸ¥ä¸»æ¨¡å‹è·¯å¾„
            primary_model_path = Path("models/sentence_transformers/all-MiniLM-L6-v2")
            fallback_model_path = Path("models/sentence_transformers/paraphrase-multilingual-MiniLM-L12-v2")
            
            if not primary_model_path.exists():
                logger.debug(f"ä¸»æ¨¡å‹ä¸å­˜åœ¨: {primary_model_path}")
                return False
            
            # ğŸ”§ å¢å¼ºï¼šå¤šå±‚æ¬¡æ¨¡å‹åŠ è½½ç­–ç•¥
            from sentence_transformers import SentenceTransformer, util
            
            # ç­–ç•¥1ï¼šå°è¯•æ— è®¾å¤‡æŒ‡å®šåŠ è½½
            try:
                logger.debug("å°è¯•ç­–ç•¥1ï¼šæ— è®¾å¤‡æŒ‡å®šåŠ è½½")
                self.embedding_model = SentenceTransformer(str(primary_model_path))
                self.embedding_util = util
                logger.info(f"âœ… ç­–ç•¥1æˆåŠŸï¼šæ— è®¾å¤‡æŒ‡å®šåŠ è½½: {primary_model_path}")
                return True
                
            except Exception as e1:
                logger.debug(f"ç­–ç•¥1å¤±è´¥: {e1}")
                
                # ç­–ç•¥2ï¼šæ˜¾å¼æŒ‡å®šCPUè®¾å¤‡
                try:
                    logger.debug("å°è¯•ç­–ç•¥2ï¼šæ˜¾å¼CPUè®¾å¤‡åŠ è½½")
                    self.embedding_model = SentenceTransformer(str(primary_model_path), device='cpu')
                    self.embedding_util = util
                    logger.info(f"âœ… ç­–ç•¥2æˆåŠŸï¼šCPUè®¾å¤‡åŠ è½½: {primary_model_path}")
                    return True
                    
                except Exception as e2:
                    logger.debug(f"ç­–ç•¥2å¤±è´¥: {e2}")
                    
                    # ç­–ç•¥3ï¼šå°è¯•æ‰‹åŠ¨é…ç½®torch
                    try:
                        logger.debug("å°è¯•ç­–ç•¥3ï¼šæ‰‹åŠ¨é…ç½®torchè®¾å¤‡")
                        
                        # ä¸´æ—¶ç¦ç”¨MPS
                        original_mps_available = None
                        if hasattr(torch.backends, 'mps'):
                            original_mps_available = torch.backends.mps.is_available
                            torch.backends.mps.is_available = lambda: False
                        
                        self.embedding_model = SentenceTransformer(str(primary_model_path))
                        self.embedding_util = util
                        
                        # æ¢å¤MPSè®¾ç½®
                        if original_mps_available is not None:
                            torch.backends.mps.is_available = original_mps_available
                        
                        logger.info(f"âœ… ç­–ç•¥3æˆåŠŸï¼šæ‰‹åŠ¨é…ç½®torch: {primary_model_path}")
                        return True
                        
                    except Exception as e3:
                        logger.debug(f"ç­–ç•¥3å¤±è´¥: {e3}")
                        
                        # ç­–ç•¥4ï¼šå°è¯•ä»ä¸åŒè·¯å¾„åŠ è½½
                        try:
                            logger.debug("å°è¯•ç­–ç•¥4ï¼šç»å¯¹è·¯å¾„åŠ è½½")
                            abs_path = primary_model_path.resolve()
                            self.embedding_model = SentenceTransformer(str(abs_path), device='cpu')
                            self.embedding_util = util
                            logger.info(f"âœ… ç­–ç•¥4æˆåŠŸï¼šç»å¯¹è·¯å¾„åŠ è½½: {abs_path}")
                            return True
                            
                        except Exception as e4:
                            logger.debug(f"ç­–ç•¥4å¤±è´¥: {e4}")
                            e5 = None  # Initialize e5 here
                            
                            # ç­–ç•¥5ï¼šå°è¯•fallbackæ¨¡å‹
                            if fallback_model_path.exists():
                                try:
                                    logger.debug("å°è¯•ç­–ç•¥5ï¼šfallbackæ¨¡å‹")
                                    self.embedding_model = SentenceTransformer(str(fallback_model_path), device='cpu')
                                    self.embedding_util = util
                                    logger.info(f"âœ… ç­–ç•¥5æˆåŠŸï¼šfallbackæ¨¡å‹: {fallback_model_path}")
                                    return True
                                    
                                except Exception as err_fallback: # Changed variable name for clarity in assignment
                                    logger.debug(f"ç­–ç•¥5å¤±è´¥: {err_fallback}")
                                    e5 = err_fallback # Assign the actual error to e5
                                    
                            # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
                            logger.error(f"âŒ æ‰€æœ‰ç¦»çº¿åŠ è½½ç­–ç•¥éƒ½å¤±è´¥:")
                            logger.error(f"  ç­–ç•¥1 (æ— è®¾å¤‡): {e1}")
                            logger.error(f"  ç­–ç•¥2 (CPU): {e2}")
                            logger.error(f"  ç­–ç•¥3 (ç¦ç”¨MPS): {e3}")
                            logger.error(f"  ç­–ç•¥4 (ç»å¯¹è·¯å¾„): {e4}")
                            if fallback_model_path.exists():
                                logger.error(f"  ç­–ç•¥5 (fallback): {e5}") # Now e5 will be defined (or None)
                            
                            return False
            
        except ImportError as e:
            logger.warning(f"ç¦»çº¿æ¨¡å¼ï¼šsentence_transformersæœªå®‰è£…: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ ç¦»çº¿æ¨¡å¼åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def get_video_duration_ffprobe(self, file_path: str) -> float:
        """
        ä½¿ç”¨ffprobeè·å–è§†é¢‘æ—¶é•¿
        
        Args:
            file_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            float: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥è¿”å›0
        """
        try:
            cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', file_path
            ]
            # ğŸ”§ å‡å°‘è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                duration = float(data['format']['duration'])
                logger.debug(f"æˆåŠŸè·å–è§†é¢‘æ—¶é•¿: {file_path} -> {duration}ç§’")
                return duration
            else:
                logger.warning(f"ffprobeå‘½ä»¤æ‰§è¡Œå¤±è´¥: {file_path}, é”™è¯¯: {result.stderr}")
                return 0
                
        except subprocess.TimeoutExpired:
            logger.error(f"ffprobeè¶…æ—¶(10ç§’): {file_path}")
            return 0
        except json.JSONDecodeError as e:
            logger.error(f"ffprobeè¾“å‡ºJSONè§£æå¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return 0
        except Exception as e:
            logger.error(f"ffprobeè¯»å–å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return 0
    
    def classify_segment_by_tags(self, all_tags: List[str]) -> str:
        """
        ä½¿ç”¨æƒé‡ç´¯åŠ æœºåˆ¶ï¼Œæ ¹æ®ai_batché…ç½®å¯¹ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼Œå½’å±å¾—åˆ†æœ€é«˜çš„æ¨¡å—ï¼ˆä¸¥æ ¼æ’é™¤è´Ÿå‘å…³é”®è¯ï¼‰ã€‚
        å¢åŠ å…¨å±€æ’é™¤é€»è¾‘ã€‚
        """
        # ğŸ”§ è¿‡æ»¤Noneå€¼ï¼Œç¡®ä¿ç±»å‹å®‰å…¨
        all_tags = [tag for tag in all_tags if tag is not None and isinstance(tag, str)]
        
        # 0. å…¨å±€æ’é™¤æ£€æŸ¥ (æœ€é«˜ä¼˜å…ˆçº§)
        config_manager = get_config_manager()
        global_exclusion_keywords = config_manager.get_global_exclusion_keywords()
        if global_exclusion_keywords:
            # å°†æ‰€æœ‰æ ‡ç­¾å’Œå…³é”®è¯è½¬ä¸ºå°å†™ä»¥è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æ¯”è¾ƒ
            lower_all_tags = {tag.lower() for tag in all_tags if tag}  # é¢å¤–è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            lower_global_exclusion_keywords = {kw.lower() for kw in global_exclusion_keywords if kw}
            
            # æŸ¥æ‰¾äº¤é›†
            intersection = lower_all_tags.intersection(lower_global_exclusion_keywords)
            if intersection:
                logger.info(f"ğŸš« ç‰‡æ®µå› åŒ…å«å…¨å±€æ’é™¤å…³é”®è¯è¢«è¿‡æ»¤: {intersection}")
                return None # ç›´æ¥è¿”å›Noneï¼Œä¸è¿›è¡Œä»»ä½•åˆ†ç±»

        # 1. åŠ è½½åŸå§‹é…ç½®ï¼Œè·å–å„æ¨¡å—ai_batchæƒé‡è¯è¡¨
        try:
            raw_config = config_manager.get_raw_config()
        except Exception as e:
            logger.error(f"æ— æ³•åŠ è½½é…ç½®ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘: {e}")
            return None
            
        modules = [
            ("pain_points", "ç—›ç‚¹"),
            ("solutions", "è§£å†³æ–¹æ¡ˆå¯¼å…¥"),
            ("features_formula", "å–ç‚¹Â·æˆåˆ†&é…æ–¹"),
            ("promotions", "ä¿ƒé”€æœºåˆ¶")
        ]
        tag_text = " ".join(all_tags).lower()
        module_scores = {}
        excluded_modules = set()

        # 2. å…ˆæ‰§è¡Œå…¨å±€æ’é™¤ï¼ˆè´Ÿå‘å…³é”®è¯ï¼‰- æ”¯æŒæ¨¡å—çº§å’Œå…¨å±€çº§æ’é™¤
        global_overrides = raw_config.get("global_settings", {}).get("overrides", {})
        for key, module_name in modules:
            # æ£€æŸ¥å…¨å±€overridesæ’é™¤
            negatives_key = f"{key}_negatives"
            negatives = global_overrides.get(negatives_key, [])
            for neg in negatives:
                if neg and isinstance(neg, str) and neg.lower() in tag_text:
                    excluded_modules.add(module_name)
                    logger.debug(f"æ¨¡å— {module_name} è¢«å…¨å±€æ’é™¤è¯ '{neg}' æ’é™¤")
                    break
            
            # ğŸ†• æ£€æŸ¥æ¨¡å—çº§ negative_keywords
            if module_name not in excluded_modules:
                module_data = raw_config.get(key, {})
                module_negatives = module_data.get("negative_keywords", [])
                for neg in module_negatives:
                    if isinstance(neg, str) and neg.lower() in tag_text:
                        excluded_modules.add(module_name)
                        logger.debug(f"æ¨¡å— {module_name} è¢«æ¨¡å—çº§æ’é™¤è¯ '{neg}' æ’é™¤")
                        break

        # 3. éå†æ¯ä¸ªæ¨¡å—ï¼Œç´¯åŠ æƒé‡åˆ†
        for key, module_name in modules:
            if module_name in excluded_modules:
                continue
            module_data = raw_config.get(key, {})
            ai_batch = module_data.get("ai_batch", {})
            score = 0
            for cat in ["object", "scene", "emotion", "brand"]:
                words = ai_batch.get(cat, [])
                for item in words:
                    if isinstance(item, dict):
                        word = item.get("word", "")
                        weight = item.get("weight", 1)
                    else:
                        word = str(item)
                        weight = 1
                    if word and word.lower() in tag_text:
                        score += weight
            if score > 0:
                module_scores[module_name] = score

        if not module_scores:
            return None
        # 4. å¾—åˆ†æœ€é«˜çš„æ¨¡å—å½’å±
        best_module = max(module_scores, key=module_scores.get)
        logger.info(f"æƒé‡æ‰“åˆ†å½’å±: {' '.join(all_tags)} -> {best_module} (åˆ†æ•°: {module_scores[best_module]})")
        
        # ğŸ†• è¾“å‡ºæ’é™¤æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
        if excluded_modules:
            logger.debug(f"è¢«æ’é™¤çš„æ¨¡å—: {list(excluded_modules)}")
        
        return best_module
    
    def _is_pain_point_by_combination(self, all_tags: List[str]) -> bool:
        """
        é€šè¿‡ç»„åˆåˆ¤æ–­æ˜¯å¦ä¸ºç—›ç‚¹åœºæ™¯
        å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šå®å®/å©´å„¿åœ¨åœº + è´Ÿé¢æƒ…ç»ª/å“­é—¹
        
        Args:
            all_tags: ç‰‡æ®µçš„æ‰€æœ‰æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦ä¸ºç—›ç‚¹åœºæ™¯
        """
        if not all_tags:
            return False
        
        # ğŸ”§ è¿‡æ»¤Noneå€¼ï¼Œç¡®ä¿ç±»å‹å®‰å…¨
        all_tags = [tag for tag in all_tags if tag is not None and isinstance(tag, str)]
        
        tags_text = " ".join(all_tags).lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®å®åœ¨åœº
        has_baby = False
        for baby_keyword in self.pain_point_rules.get("baby_prescene", []):
            if baby_keyword and isinstance(baby_keyword, str) and baby_keyword.lower() in tags_text:
                has_baby = True
                logger.debug(f"ç—›ç‚¹æ£€æµ‹: å‘ç°å®å®å…³é”®è¯ '{baby_keyword.lower()}'")
                break
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è´Ÿé¢æƒ…ç»ª
        has_negative_emotion = False
        matched_emotion = None
        for emotion_keyword in self.pain_point_rules.get("negative_emotions", []):
            if emotion_keyword and isinstance(emotion_keyword, str) and emotion_keyword.lower() in tags_text:
                has_negative_emotion = True
                matched_emotion = emotion_keyword.lower()
                logger.debug(f"ç—›ç‚¹æ£€æµ‹: å‘ç°è´Ÿé¢æƒ…ç»ªå…³é”®è¯ '{emotion_keyword.lower()}'")
                break
        
        # åªæœ‰åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶æ‰åˆ¤å®šä¸ºç—›ç‚¹
        is_pain_point = has_baby and has_negative_emotion
        
        if is_pain_point:
            logger.info(f"ç—›ç‚¹æ£€æµ‹: âœ… åŒæ—¶æ»¡è¶³å®å®åœ¨åœºå’Œè´Ÿé¢æƒ…ç»ªï¼Œåˆ¤å®šä¸ºç—›ç‚¹åœºæ™¯")
            logger.info(f"    å®å®åœ¨åœº: {has_baby}")
            logger.info(f"    è´Ÿé¢æƒ…ç»ª: {has_negative_emotion} (åŒ¹é…è¯: {matched_emotion})")
        else:
            logger.debug(f"ç—›ç‚¹æ£€æµ‹: âŒ ä¸æ»¡è¶³ç—›ç‚¹æ¡ä»¶")
            logger.debug(f"    å®å®åœ¨åœº: {has_baby}")
            logger.debug(f"    è´Ÿé¢æƒ…ç»ª: {has_negative_emotion}")
        
        return is_pain_point
    
    def _check_duration_limit(self, segment_info: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        æ£€æŸ¥ç‰‡æ®µæ—¶é•¿æ˜¯å¦è¶…è¿‡é™åˆ¶
        
        Args:
            segment_info: ç‰‡æ®µä¿¡æ¯å­—å…¸
            
        Returns:
            Dict: å¦‚æœè¶…è¿‡é™åˆ¶åˆ™è¿”å›æ’é™¤ä¿¡æ¯ï¼Œå¦åˆ™è¿”å›None
        """
        try:
            if not segment_info or not isinstance(segment_info, dict):
                return None
            
            # è·å–ç‰‡æ®µæ—¶é•¿
            duration = segment_info.get('duration', 0)
            if not duration:
                return None
            
            # è·å–é…ç½®ä¸­çš„æœ€å¤§æ—¶é•¿é™åˆ¶
            max_duration = 10  # é»˜è®¤10ç§’
            
            if hasattr(self, 'rules') and self.rules:
                global_settings = self.rules.get("GLOBAL_SETTINGS", {})
                if isinstance(global_settings, dict):
                    max_duration = global_settings.get("max_duration_seconds", 10)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if duration > max_duration:
                segment_name = segment_info.get("file_name", "unknown")
                reason = f"æ—¶é•¿{duration:.1f}sè¶…è¿‡é™åˆ¶{max_duration}s"
                
                logger.info(f"ğŸ•’ æ—¶é•¿è¿‡æ»¤: {segment_name} ({reason})")
                
                return {
                    "is_excluded": True,
                    "reason": reason,
                    "duration": duration,
                    "max_duration": max_duration
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"æ—¶é•¿æ£€æŸ¥å¤±è´¥: {e}")
            return None
    
    def _detailed_exclusion_check(self, tags_text: str) -> Dict[str, Any]:
        """
        è¯¦ç»†çš„æ’é™¤å…³é”®è¯æ£€æŸ¥ï¼Œè¿”å›å®Œæ•´çš„æ£€æŸ¥ç»“æœ
        
        Args:
            tags_text: æ ‡ç­¾æ–‡æœ¬
            
        Returns:
            Dict: åŒ…å«æ˜¯å¦æ’é™¤å’Œå…·ä½“åŸå› çš„è¯¦ç»†ç»“æœ
        """
        result = {
            "is_excluded": False,
            "exclusion_reasons": [],
            "matched_keywords": {}
        }
        
        try:
            # ğŸ”§ ä½¿ç”¨ConfigManageræ›¿ä»£ç›´æ¥è¯»å–matching_rules.json
            from utils.config_manager import get_config_manager
            config_manager = get_config_manager()
            matching_rules = config_manager.get_matching_rules()
            
            if not isinstance(matching_rules, dict):
                logger.warning(f"é…ç½®è§„åˆ™ç±»å‹é”™è¯¯: {type(matching_rules).__name__}")
                return result
            
            # æ£€æŸ¥æ¨¡å—çº§åˆ«çš„æ’é™¤å…³é”®è¯
            for module, rules in matching_rules.items():
                if not isinstance(rules, dict):
                    logger.debug(f"è·³è¿‡æ¨¡å— {module}ï¼Œæ•°æ®ç±»å‹é”™è¯¯: {type(rules).__name__}")
                    continue
                
                negative_keywords = rules.get("negative_keywords", [])
                
                if not isinstance(negative_keywords, list):
                    logger.warning(f"æ¨¡å— {module} negative_keywordsç±»å‹é”™è¯¯: {type(negative_keywords).__name__}")
                    continue
                    
                module_matches = []
                
                for neg_kw in negative_keywords:
                    if isinstance(neg_kw, str) and neg_kw.lower() in tags_text.lower():
                        module_matches.append(neg_kw)
                
                if module_matches:
                    result["is_excluded"] = True
                    result["exclusion_reasons"].append(f"{module}æ¨¡å—æ’é™¤: {module_matches}")
                    result["matched_keywords"][module] = module_matches
        
        except Exception as e:
            logger.error(f"æ’é™¤å…³é”®è¯æ£€æŸ¥å¤±è´¥: {e}")
        
        return result
    
    def _get_keyword_matches(self, all_tags: List[str]) -> Dict[str, List[str]]:
        """
        è·å–æ‰€æœ‰æ¨¡å—çš„å…³é”®è¯åŒ¹é…æƒ…å†µ
        
        Args:
            all_tags: ç‰‡æ®µæ ‡ç­¾
            
        Returns:
            Dict: æ¯ä¸ªæ¨¡å—åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
        """
        matches = {}
        
        # ğŸ”§ è¿‡æ»¤Noneå€¼ï¼Œç¡®ä¿ç±»å‹å®‰å…¨
        all_tags = [tag for tag in all_tags if tag is not None and isinstance(tag, str)]
        
        tags_text = " ".join(all_tags).lower()
        
        for module in self.four_modules:
            module_matches = []
            
            # æ£€æŸ¥æ¯ç§ç±»å‹çš„å…³é”®è¯
            if module in self.keyword_rules:
                keywords = self.keyword_rules[module]
                for kw in keywords:
                    if kw and isinstance(kw, str) and kw.lower() in tags_text:
                        module_matches.append(kw)
            
            matches[module] = module_matches
        
        return matches
    
    def _is_excluded_by_negative_keywords(self, tags_text: str) -> bool:
        """
        ğŸ”§ ç®€åŒ–ç‰ˆï¼šæ£€æŸ¥ç‰‡æ®µæ˜¯å¦è¢«å…¨å±€æ’é™¤å…³é”®è¯æ’é™¤
        
        Args:
            tags_text: æ ‡ç­¾æ–‡æœ¬ï¼ˆå·²è½¬æ¢ä¸ºå°å†™ï¼‰
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥è¢«æ’é™¤
        """
        try:
            # ğŸ”§ ç®€åŒ–ï¼šä½¿ç”¨å·²åŠ è½½çš„é…ç½®
            if not hasattr(self, 'rules') or not self.rules:
                # å¦‚æœæ²¡æœ‰åŠ è½½é…ç½®ï¼Œå°è¯•åŠ è½½
                self._load_matching_rules()
                
            # ğŸ”§ NEW: å¼ºåŒ–ç±»å‹æ£€æŸ¥
            if not self.rules or not isinstance(self.rules, dict):
                logger.warning(f"é…ç½®è§„åˆ™ç±»å‹é”™è¯¯æˆ–ä¸ºç©º: {type(self.rules).__name__ if self.rules else 'None'}")
                return False
            
            # ğŸš« åªæ£€æŸ¥å…¨å±€æ’é™¤å…³é”®è¯ï¼ˆç®€åŒ–é€»è¾‘ï¼‰
            if "GLOBAL_SETTINGS" in self.rules:
                global_settings = self.rules["GLOBAL_SETTINGS"]
                
                # ğŸ”§ NEW: ç¡®ä¿GLOBAL_SETTINGSä¹Ÿæ˜¯å­—å…¸
                if not isinstance(global_settings, dict):
                    logger.warning(f"GLOBAL_SETTINGSç±»å‹é”™è¯¯: {type(global_settings).__name__}")
                    return False
                
                # æ£€æŸ¥å…¨å±€æ’é™¤å…³é”®è¯
                global_exclusion = global_settings.get("global_exclusion_keywords", [])
                if isinstance(global_exclusion, list):
                    for global_kw in global_exclusion:
                        if isinstance(global_kw, str) and global_kw.lower() in tags_text:
                            logger.info(f"ğŸš« å…¨å±€æ’é™¤å…³é”®è¯è¿‡æ»¤: '{global_kw}' åœ¨ '{tags_text}' ä¸­")
                            return True
            
                # æ£€æŸ¥æ— å…³åœºæ™¯ç±»åˆ«
                irrelevant_categories = global_settings.get("irrelevant_scene_categories", {})
                if isinstance(irrelevant_categories, dict):
                    for category, keywords in irrelevant_categories.items():
                        if isinstance(keywords, list):
                            for keyword in keywords:
                                if isinstance(keyword, str) and keyword.lower() in tags_text:
                                    logger.info(f"ğŸš« æ— å…³åœºæ™¯è¿‡æ»¤ - {category}: '{keyword}' åœ¨ '{tags_text}' ä¸­")
                                    return True
            
            return False
            
        except Exception as e:
            logger.warning(f"æ’é™¤å…³é”®è¯æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _classify_by_embedding_similarity(self, all_tags: List[str]) -> str:
        """
        ä½¿ç”¨embeddingç›¸ä¼¼åº¦è¿›è¡Œåˆ†ç±»ï¼ˆDeepSeekè¶…æ—¶æ—¶çš„fallbackï¼‰
        å¦‚æœembeddingæ¨¡å‹ä¸å¯ç”¨ï¼Œåˆ™ä½¿ç”¨å…³é”®è¯åˆ†ç±»
        
        Args:
            all_tags: æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            åˆ†ç±»ç»“æœ
        """
        # ğŸ”§ Embedding models are disabled. This method now directly falls back to keyword classification.
        logger.info("Embedding models are disabled. _classify_by_embedding_similarity falling back to keywords.")
        
        # Original logic when embedding_model is None:
        # if not self.embedding_model or not self.embedding_util:
        #     logger.info("Embeddingæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯åˆ†ç±»ä½œä¸ºfallback")
        #     keyword_result = self.classify_segment_by_tags(all_tags)
        #     return keyword_result if keyword_result else "å…¶ä»–"
        
        # Simplified fallback:
        keyword_result = self.classify_segment_by_tags(all_tags)
        return keyword_result if keyword_result else "å…¶ä»–"
            
        # The rest of the original method (embedding logic) is now unreachable 
        # because self.embedding_model will always be None.
        # Consider removing the unreachable code below in a future cleanup if this change is permanent.
        
        # if not all_tags:
        #     return "å…¶ä»–"
            
        # tags_text = " ".join(all_tags)
        # logger.info(f"ä½¿ç”¨embeddingç›¸ä¼¼åº¦åˆ†ç±»: {tags_text}")
        
        # # ğŸ”§ å°è¯•ä¸»æ¨¡å‹åˆ†ç±»
        # result = self._classify_with_model(all_tags, self.embedding_model, "ä¸»æ¨¡å‹")
        # if result != "æ¨¡å‹å¤±è´¥":
        #     return result
        
        # # ğŸ”§ ä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•fallbackæ¨¡å‹
        # if hasattr(self, 'fallback_model') and self.fallback_model:
        #     logger.warning("ä¸»æ¨¡å‹åˆ†ç±»å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨fallbackæ¨¡å‹")
        #     result = self._classify_with_model(all_tags, self.fallback_model, "fallbackæ¨¡å‹")
        #     if result != "æ¨¡å‹å¤±è´¥":
        #         return result
        
        # # ğŸ”§ æ‰€æœ‰embeddingæ¨¡å‹éƒ½å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»
        # logger.error("æ‰€æœ‰embeddingæ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»")
        # keyword_result = self.classify_segment_by_tags(all_tags)
        # return keyword_result if keyword_result else "å…¶ä»–"
    
    def _classify_with_model(self, all_tags: List[str], model, model_name: str) -> str:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œåˆ†ç±»
        
        Args:
            all_tags: æ ‡ç­¾åˆ—è¡¨
            model: SentenceTransformeræ¨¡å‹å®ä¾‹
            model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            åˆ†ç±»ç»“æœï¼Œå¤±è´¥æ—¶è¿”å›"æ¨¡å‹å¤±è´¥"
        """
        if not model or not self.embedding_util:
            logger.debug(f"{model_name}ä¸å¯ç”¨")
            return "æ¨¡å‹å¤±è´¥"
            
        if not all_tags:
            return "å…¶ä»–"
            
        tags_text = " ".join(all_tags)
        
        try:
            # å®šä¹‰å„ç±»åˆ«çš„ä»£è¡¨æ€§æ–‡æœ¬
            category_texts = {
                "ç—›ç‚¹æœºåˆ¶": "å®å®å“­é—¹ å©´å„¿ä¸é€‚ å–‚å…»å›°éš¾ ç¡çœ é—®é¢˜ å¥åº·æ‹…å¿§",
                "ä¿ƒé”€æœºåˆ¶": "å®å®å–å¥¶ç²‰å¼€å¿ƒ å®å®ç©è€å¼€å¿ƒ å®å®å¥”è·‘ æˆ·å¤–ç©è€ æ¸¸ä¹åœº å¤§ç¬‘ æ¬¢ä¹ æ´»åŠ›",
                "ç§‘æ™®æœºåˆ¶": "è¥å…»çŸ¥è¯† è‚²å„¿æ–¹æ³• å¥åº·æŒ‡å¯¼ ä¸“ä¸šå»ºè®® ç§‘å­¦å–‚å…»",
                "æƒ…æ„Ÿæœºåˆ¶": "æ¯çˆ± äº²æƒ… æ¸©é¦¨ é™ªä¼´ æˆé•¿ å…³çˆ±"
            }
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            embeddings = model.encode([tags_text] + list(category_texts.values()))
            similarities = self.embedding_util.pytorch_cos_sim(embeddings[0], embeddings[1:])
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç±»åˆ«
            max_similarity = float(similarities.max())
            max_index = int(similarities.argmax())
            
            categories = list(category_texts.keys())
            best_category = categories[max_index]
            
            # è®¾ç½®é˜ˆå€¼ï¼Œç›¸ä¼¼åº¦å¤ªä½åˆ™è¿”å›"å…¶ä»–"
            threshold = 0.4
            if max_similarity < threshold:
                logger.info(f"{model_name} ç›¸ä¼¼åº¦ {max_similarity:.3f} ä½äºé˜ˆå€¼ {threshold}ï¼Œè¿”å›å…¶ä»–")
                return "å…¶ä»–"
            
            logger.info(f"{model_name} åˆ†ç±»æˆåŠŸ: {tags_text} -> {best_category} (ç›¸ä¼¼åº¦: {max_similarity:.3f})")
            return best_category
            
        except Exception as e:
            logger.error(f"{model_name} åˆ†ç±»å¤±è´¥: {e}")
            return "æ¨¡å‹å¤±è´¥"
    
    def classify_segment_by_deepseek(self, all_tags: List[str]) -> str:
        """
        ğŸ”§ ä½¿ç”¨DeepSeekæ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼ˆç»Ÿä¸€prompté…ç½®ï¼‰
        """
        if not self.deepseek_analyzer or not self.deepseek_analyzer.is_available():
            logger.warning("DeepSeekåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨embedding fallback")
            return self._classify_by_embedding_similarity(all_tags)
        
        if not all_tags:
            logger.info("æ ‡ç­¾ä¸ºç©ºï¼ŒDeepSeekè·³è¿‡åˆ†ç±»ï¼Œè¿”å›å…¶ä»–")
            return "å…¶ä»–"
        
        tags_text = " ".join(all_tags)
        logger.info(f"ä½¿ç”¨DeepSeekåˆ†ç±»: {tags_text}")
        
        try:
            analyzer = self.deepseek_analyzer
            
            # ğŸ”§ ä½¿ç”¨ç®€åŒ–çš„åˆ†ç±»promptï¼Œé¿å…å†—ä½™
            try:
                # åŠ¨æ€æ„å»ºåˆ†ç±»æŒ‡ä»¤
                config_manager = get_config_manager()
                rules = config_manager.get_matching_rules()
                
                system_prompt_parts = ["ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¯å©´è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·æ ¹æ®è§†é¢‘æ ‡ç­¾ï¼Œå°†å†…å®¹åˆ†ç±»ä¸ºä»¥ä¸‹å››ä¸ªæ¨¡å—ä¹‹ä¸€ï¼š"]
                for module, config in rules.items():
                    # æ‹¼æ¥æ ¸å¿ƒè¯ä½œä¸ºæ¨¡å—æè¿°
                    core_terms = ", ".join(config.get('core_identity', []))
                    system_prompt_parts.append(f"- **{module}**: æ ¸å¿ƒä¿¡å·åŒ…æ‹¬ {core_terms}")
                
                system_prompt_parts.append("\nè¯·åªå›ç­”æ¨¡å—åç§°ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œå›ç­”\"å…¶ä»–\"ã€‚")
                system_content = "\n".join(system_prompt_parts)
                
            except Exception as e:
                logger.warning(f"æ— æ³•ä»é…ç½®ä¸­å¿ƒæ„å»ºDeepSeek prompt: {e}ï¼Œä½¿ç”¨å…œåº•prompt")
                system_content = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¯å©´è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·æ ¹æ®è§†é¢‘æ ‡ç­¾ï¼Œå°†å†…å®¹åˆ†ç±»ä¸ºä»¥ä¸‹å››ä¸ªæ¨¡å—ä¹‹ä¸€ï¼š

1. ç—›ç‚¹ï¼šå®å®å“­é—¹ã€ä¸é€‚ã€å–‚å…»å›°éš¾ã€ç”Ÿç—…ã€ç„¦è™‘ç­‰è´Ÿé¢æƒ…å†µ
2. è§£å†³æ–¹æ¡ˆå¯¼å…¥ï¼šå†²å¥¶ç²‰ã€ä½¿ç”¨å¥¶ç“¶ã€å–‚å…»è¿‡ç¨‹ã€äº§å“ä½¿ç”¨ç­‰è¡ŒåŠ¨åœºæ™¯
3. å–ç‚¹Â·æˆåˆ†&é…æ–¹ï¼šA2è›‹ç™½ã€HMOã€DHAã€è¥å…»æˆåˆ†ã€äº§å“ç‰¹è‰²ç­‰ä¸“ä¸šå†…å®¹
4. ä¿ƒé”€æœºåˆ¶ï¼šå®å®å¼€å¿ƒã€æ´»åŠ›æ»¡æ»¡ã€å¥åº·æˆé•¿ã€ä¼˜æƒ æ´»åŠ¨ç­‰æ­£é¢æ¨å¹¿

è¯·åªå›ç­”æ¨¡å—åç§°ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œå›ç­”"å…¶ä»–"ã€‚"""

            user_content = f"è§†é¢‘æ ‡ç­¾: {', '.join(all_tags)}ã€‚è¯·åˆ†ç±»ã€‚"
            
            response = analyzer.analyze_text(system_content, user_content)
            
            if response and "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0]["message"]["content"].strip()
                if content in self.four_modules:
                    logger.info(f"DeepSeekåˆ†ç±»æˆåŠŸ: {tags_text} -> {content}")
                    return content
                else:
                    logger.warning(f"DeepSeekè¿”å›æ— æ•ˆåˆ†ç±»: {content}, å›é€€åˆ°å…³é”®è¯åˆ†ç±»")
                    keyword_result = self.classify_segment_by_tags(all_tags)
                    return keyword_result if keyword_result else "å…¶ä»–"
            else:
                logger.warning("DeepSeek APIå“åº”æ— æ•ˆï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»")
                keyword_result = self.classify_segment_by_tags(all_tags)
                return keyword_result if keyword_result else "å…¶ä»–"
                
        except Exception as e:
            logger.error(f"DeepSeekåˆ†ç±»å¤±è´¥: {e}")
            logger.info("å›é€€åˆ°å…³é”®è¯åˆ†ç±»")
            keyword_result = self.classify_segment_by_tags(all_tags)
            return keyword_result if keyword_result else "å…¶ä»–"
    
    def classify_segment(self, all_tags: List[str], segment_info: Optional[Dict[str, Any]] = None) -> str:
        """
        å¯¹ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼ˆå…³é”®è¯ä¼˜å…ˆï¼ŒDeepSeekå…œåº•ï¼‰ï¼Œå¹¶è®°å½•è¯¦ç»†å†³ç­–è¿‡ç¨‹
        """
        selection_logger = get_selection_logger()
        logger.debug(f"å¼€å§‹åˆ†ç±»ç‰‡æ®µï¼Œæ ‡ç­¾: {all_tags}")
        
        # ç¬¬ä¸€æ­¥ï¼šå…³é”®è¯è§„åˆ™åˆ†ç±»
        category = self.classify_segment_by_tags(all_tags)
        
        if category:
            log_reason = "å…³é”®è¯åˆ†ç±»æˆåŠŸ"
            selection_logger.log_step(
                step_type="keyword_classification",
                input_tags=all_tags,
                result=category
            )
        
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœå…³é”®è¯åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨DeepSeekæ¨¡å‹
        else:
            logger.info(f"å…³é”®è¯è§„åˆ™æ— æ³•åˆ†ç±»æ ‡ç­¾ {all_tags}ï¼Œä½¿ç”¨DeepSeekæ¨¡å‹")
            category = self.classify_segment_by_deepseek(all_tags)
            log_reason = "AIåˆ†ç±»æˆåŠŸï¼Œå…³é”®è¯åˆ†ç±»å¤±è´¥" if category != "å…¶ä»–" else "å…³é”®è¯å’ŒAIåˆ†ç±»éƒ½æ— æ³•ç¡®å®šç±»åˆ«"

        # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨è´Ÿé¢å…³é”®è¯è¿‡æ»¤
        if category and category != "å…¶ä»–":
            filtered_category, filter_reason = self._apply_module_negative_filter(category, all_tags)
            if filtered_category != category:
                logger.info(f"ğŸš« åˆ†ç±»å·²ç”±è´Ÿé¢å…³é”®è¯è¿‡æ»¤: {category} -> {filtered_category} ({filter_reason})")
                category = filtered_category
                log_reason = f"è´Ÿé¢å…³é”®è¯è¿‡æ»¤: {filter_reason}"

        # è®°å½•æœ€ç»ˆç»“æœ
        selection_logger.log_final_result(
            final_category=category,
            reason=log_reason,
            segment_info=segment_info
        )
        
        return category
    
    def scan_video_pool(self, video_pool_path: str = "data/output/google_video/video_pool") -> List[Dict[str, Any]]:
        """
        æ‰«ævideo_poolç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
        
        Args:
            video_pool_path: video_poolç›®å½•è·¯å¾„
            
        Returns:
            List[Dict]: æ˜ å°„åçš„ç‰‡æ®µåˆ—è¡¨
        """
        mapped_segments = []
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šæ·»åŠ æ˜ å°„é˜¶æ®µå»é‡æœºåˆ¶
        seen_segment_ids = set()  # ç”¨äºè·Ÿè¸ªå·²ç»æ˜ å°„çš„ç‰‡æ®µID
        
        # ğŸ”§ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„è·¯å¾„è§£æ
        resolved_path = resolve_video_pool_path(video_pool_path)
        logger.info(f"ğŸ” è§£ævideo_poolè·¯å¾„: {video_pool_path} -> {resolved_path}")
        
        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        if not os.path.exists(resolved_path):
            logger.error(f"video_poolç›®å½•ä¸å­˜åœ¨: {resolved_path}")
            logger.error(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            return mapped_segments
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(resolved_path, "*.json"))
        logger.info(f"åœ¨ {resolved_path} ä¸­æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        if not json_files:
            logger.warning(f"åœ¨ {resolved_path} ä¸­æœªæ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶")
            return mapped_segments
        
        # å¤„ç†æ¯ä¸ªJSONæ–‡ä»¶ï¼Œæ·»åŠ è¿›åº¦è·Ÿè¸ª
        for file_idx, json_file in enumerate(json_files):
            try:
                logger.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {file_idx + 1}/{len(json_files)}: {os.path.basename(json_file)}")
                
                # ğŸ”§ æ·»åŠ æ–‡ä»¶å¤§å°æ£€æŸ¥ï¼Œé¿å…å¤„ç†è¿‡å¤§çš„æ–‡ä»¶
                file_size = os.path.getsize(json_file)
                if file_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
                    logger.warning(f"è·³è¿‡è¿‡å¤§çš„æ–‡ä»¶: {json_file} ({file_size/1024/1024:.1f}MB)")
                    continue
                
                with open(json_file, 'r', encoding='utf-8') as f:
                    video_data = json.load(f)
                
                video_id = video_data.get('video_id', 'unknown')
                segments = video_data.get('segments', [])
                
                logger.info(f"å¤„ç†è§†é¢‘ {video_id}ï¼ŒåŒ…å« {len(segments)} ä¸ªç‰‡æ®µ")
                
                # ğŸ”§ é™åˆ¶æ¯ä¸ªæ–‡ä»¶å¤„ç†çš„ç‰‡æ®µæ•°é‡ï¼Œé¿å…è¿‡åº¦å¤„ç†
                max_segments_per_file = 100
                if len(segments) > max_segments_per_file:
                    logger.warning(f"æ–‡ä»¶ {json_file} åŒ…å« {len(segments)} ä¸ªç‰‡æ®µï¼Œåªå¤„ç†å‰ {max_segments_per_file} ä¸ª")
                    segments = segments[:max_segments_per_file]
                
                # å¤„ç†æ¯ä¸ªç‰‡æ®µ
                for seg_idx, segment in enumerate(segments):
                    try:
                        # ğŸ”§ æ·»åŠ ç‰‡æ®µå¤„ç†è¿›åº¦æ—¥å¿—
                        if seg_idx % 10 == 0:  # æ¯10ä¸ªç‰‡æ®µè®°å½•ä¸€æ¬¡
                            logger.info(f"ğŸ”„ å¤„ç†ç‰‡æ®µè¿›åº¦: {seg_idx + 1}/{len(segments)}")
                        
                        # åŸºæœ¬ä¿¡æ¯æå–
                        file_path = segment.get('file_path', '')
                        all_tags = segment.get('all_tags', [])
                        quality_score = segment.get('quality_score', 0.9)
                        confidence = segment.get('confidence', 0.8)
                        file_name = segment.get('file_name', '')
                        
                        # ğŸ”§ æ·»åŠ analysis_methodå­—æ®µå¤„ç†ï¼Œä¸ºæ—§JSONæ–‡ä»¶æä¾›é»˜è®¤å€¼
                        analysis_method = segment.get('analysis_method', 'visual')  # é»˜è®¤ä¸ºvisual
                        transcription = segment.get('transcription', None)  # è¯­éŸ³è½¬å½•å†…å®¹
                        
                        # ğŸ”§ NEW: å…¼å®¹æ—§æ ¼å¼JSONæ–‡ä»¶ï¼Œä»å…¶ä»–å­—æ®µæ„å»ºall_tags
                        if not all_tags:
                            # ä»æ—§æ ¼å¼å­—æ®µæ„å»ºæ ‡ç­¾ - æ”¯æŒå­—ç¬¦ä¸²å’Œé€—å·åˆ†éš”æ ¼å¼
                            raw_fields = [
                                segment.get('object', ''),
                                segment.get('scene', ''),
                                segment.get('emotion', ''),
                                segment.get('brand_elements', '')
                            ]
                            
                            all_tags = []
                            for raw_field in raw_fields:
                                if not raw_field:
                                    continue
                                    
                                # å¤„ç†é€—å·åˆ†éš”çš„æƒ…å†µ
                                if ',' in raw_field:
                                    tags = raw_field.split(',')
                                else:
                                    tags = [raw_field]
                                
                                # æ¸…ç†å’Œæ·»åŠ æ ‡ç­¾
                                for tag in tags:
                                    clean_tag = tag.strip()
                                    if clean_tag and clean_tag not in all_tags:
                                        all_tags.append(clean_tag)
                            
                            logger.debug(f"ä»æ—§æ ¼å¼æ„å»ºæ ‡ç­¾: {file_name} -> {all_tags}")
                        
                        # è·³è¿‡ç©ºæ ‡ç­¾ç‰‡æ®µ
                        if not all_tags:
                            logger.debug(f"è·³è¿‡ç©ºæ ‡ç­¾ç‰‡æ®µ: {file_name}")
                            continue
                        
                        # ğŸ¯ NEW: è·³è¿‡äººè„¸ç‰¹å†™ç‰‡æ®µ
                        is_face_close_up = segment.get('is_face_close_up', False)
                        is_unusable = segment.get('unusable', False)
                        unusable_reason = segment.get('unusable_reason', '')
                        
                        if is_face_close_up or is_unusable:
                            logger.info(f"ğŸš« è·³è¿‡äººè„¸ç‰¹å†™/ä¸å¯ç”¨ç‰‡æ®µ: {file_name} (åŸå› : {unusable_reason})")
                            continue
                        
                        # ğŸ”§ æ·»åŠ è¶…æ—¶æ§åˆ¶çš„è§†é¢‘æ—¶é•¿æå–
                        duration = 0
                        if file_path and os.path.exists(file_path):
                            try:
                                duration = self.get_video_duration_ffprobe(file_path)
                            except Exception as e:
                                logger.warning(f"è·å–è§†é¢‘æ—¶é•¿å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                                duration = 0
                        else:
                            logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                        
                        # ğŸ•’ NEW: æ£€æŸ¥æ—¶é•¿é™åˆ¶ï¼Œç›´æ¥åœ¨æ‰«æé˜¶æ®µè¿‡æ»¤é•¿è§†é¢‘
                        max_duration = 10  # é»˜è®¤10ç§’
                        if hasattr(self, 'rules') and self.rules:
                            global_settings = self.rules.get("GLOBAL_SETTINGS", {})
                            if isinstance(global_settings, dict):
                                max_duration = global_settings.get("max_duration_seconds", 10)
                        
                        if duration > max_duration:
                            logger.info(f"ğŸ•’ æ—¶é•¿è¿‡æ»¤: {file_name} (æ—¶é•¿{duration:.1f}s > é™åˆ¶{max_duration}s)")
                            continue
                        
                        # ğŸ”§ æ„å»ºç‰‡æ®µä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•
                        segment_info_for_logging = {
                            "file_name": file_name,
                            "duration": duration,
                            "all_tags": all_tags,
                            "combined_quality": quality_score * confidence,
                            "is_face_close_up": is_face_close_up
                        }
                        
                        # ğŸ”§ æ·»åŠ è¶…æ—¶æ§åˆ¶çš„åˆ†ç±»å¤„ç†
                        try:
                            logger.debug(f"ğŸ” å¼€å§‹åˆ†ç±»ç‰‡æ®µ: {file_name} æ ‡ç­¾: {all_tags}")
                            category = self.classify_segment(all_tags, segment_info_for_logging)
                            logger.info(f"ğŸ¯ åˆ†ç±»å®Œæˆ: {file_name} -> {category}")
                        except Exception as e:
                            logger.error(f"âŒ ç‰‡æ®µåˆ†ç±»å¤±è´¥: {file_name} {all_tags}, é”™è¯¯: {e}")
                            category = "å…¶ä»–"
                        
                        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†
                        combined_quality = quality_score * confidence
                        
                        # ğŸ”§ ç”Ÿæˆç‰‡æ®µçš„å”¯ä¸€æ ‡è¯†ç¬¦è¿›è¡Œå»é‡æ£€æŸ¥ï¼ˆå¢å¼ºç‰ˆï¼‰
                        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„+æ–‡ä»¶å+æ ‡ç­¾çš„ç»„åˆæ¥ç¡®ä¿å”¯ä¸€æ€§
                        tags_signature = "_".join(sorted(all_tags[:5]))  # å‰5ä¸ªæ ‡ç­¾æ’åºç»„åˆ
                        unique_id = f"{video_id}::{file_name}::{file_path}::{tags_signature}"
                        
                        # ğŸ”§ æ ¸å¿ƒå»é‡æ£€æŸ¥ï¼šé¿å…é‡å¤æ˜ å°„
                        if unique_id in seen_segment_ids:
                            logger.info(f"ğŸš« è·³è¿‡é‡å¤ç‰‡æ®µ: {file_name} (ID: {unique_id[:100]}...)")
                            continue
                        
                        # æ„å»ºæ˜ å°„ç»“æœ
                        mapped_segment = {
                            "segment_id": f"{video_id}_{file_name}",
                            "file_path": file_path,
                            "file_name": file_name,  # ğŸ”§ ç¡®ä¿ä½¿ç”¨file_nameå­—æ®µå
                            "filename": file_name,   # ğŸ”§ æ·»åŠ filenameå­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
                            "all_tags": all_tags,
                            "category": category,    # ğŸ”§ ç¡®ä¿ä½¿ç”¨categoryå­—æ®µå
                            "classification": category,  # ğŸ”§ æ·»åŠ classificationå­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
                            "quality_score": quality_score,
                            "confidence": confidence,
                            "combined_quality": combined_quality,
                            "duration": duration,
                            "video_id": video_id,
                            "analysis_method": analysis_method,  # ğŸ”§ æ·»åŠ analysis_methodå­—æ®µ
                            "transcription": transcription,      # ğŸ”§ æ·»åŠ transcriptionå­—æ®µ
                            "success": True
                        }
                        
                        # ğŸ”§ æ·»åŠ åˆ°å·²è§é›†åˆå’Œç»“æœåˆ—è¡¨
                        seen_segment_ids.add(unique_id)
                        mapped_segments.append(mapped_segment)
                        logger.info(f"âœ… æ˜ å°„ç‰‡æ®µ: {file_name} -> {category} (æ—¶é•¿: {duration:.2f}s, æ ‡ç­¾: {all_tags[:3]})")
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†ç‰‡æ®µå¤±è´¥: {segment.get('file_name', segment.get('filename', 'unknown'))}, é”™è¯¯: {e}")
                        continue
                
                logger.info(f"æ–‡ä»¶ {os.path.basename(json_file)} å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len([s for s in mapped_segments if s['video_id'] == video_id])} ä¸ªç‰‡æ®µ")
                        
            except Exception as e:
                logger.error(f"å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {json_file}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"ğŸ¯ æ˜ å°„å®Œæˆç»Ÿè®¡:")
        logger.info(f"   - æœ€ç»ˆæœ‰æ•ˆç‰‡æ®µ: {len(mapped_segments)} ä¸ª")
        logger.info(f"   - å·²å¤„ç†å”¯ä¸€ID: {len(seen_segment_ids)} ä¸ª")
        logger.info(f"   - å»é‡æ•ˆæœ: {len(seen_segment_ids) - len(mapped_segments)} ä¸ªé‡å¤è¢«è¿‡æ»¤")
        
        return mapped_segments
    
    def get_mapping_statistics(self, mapped_segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            mapped_segments: æ˜ å°„åçš„ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "total_segments": len(mapped_segments),
            "by_category": {},
            "by_video": {},
            "quality_stats": {
                "avg_quality": 0,
                "avg_confidence": 0,
                "avg_duration": 0,
                "total_duration": 0
            }
        }
        
        if not mapped_segments:
            return stats
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for module in self.four_modules + ["å…¶ä»–"]:
            module_segments = [s for s in mapped_segments if s["category"] == module]
            stats["by_category"][module] = {
                "count": len(module_segments),
                "total_duration": sum(s["duration"] for s in module_segments),
                "avg_quality": sum(s["combined_quality"] for s in module_segments) / len(module_segments) if module_segments else 0
            }
        
        # æŒ‰è§†é¢‘ç»Ÿè®¡
        video_counts = {}
        for segment in mapped_segments:
            video_id = segment["video_id"]
            if video_id not in video_counts:
                video_counts[video_id] = 0
            video_counts[video_id] += 1
        stats["by_video"] = video_counts
        
        # è´¨é‡ç»Ÿè®¡
        if mapped_segments:
            stats["quality_stats"]["avg_quality"] = sum(s["quality_score"] for s in mapped_segments) / len(mapped_segments)
            stats["quality_stats"]["avg_confidence"] = sum(s["confidence"] for s in mapped_segments) / len(mapped_segments)
            stats["quality_stats"]["avg_duration"] = sum(s["duration"] for s in mapped_segments) / len(mapped_segments)
            stats["quality_stats"]["total_duration"] = sum(s["duration"] for s in mapped_segments)
        
        return stats

    def _init_embedding_model_online(self):
        """åœ¨çº¿æ¨¡å¼ä¸‹åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        import time
        
        # åœ¨çº¿æ¨¡å¼çš„å¤šä¸ªfallbacké€‰é¡¹
        online_models = [
            'all-MiniLM-L6-v2',          # ä¸»é€‰æ‹©
            'all-mpnet-base-v2',         # å¤‡é€‰1
            'paraphrase-MiniLM-L6-v2'    # å¤‡é€‰2
        ]
        
        for attempt, model_name in enumerate(online_models, 1):
            logger.info(f"å°è¯•åœ¨çº¿æ¨¡å‹ {attempt}/{len(online_models)}: {model_name}")
            
            # æ¯ä¸ªæ¨¡å‹å°è¯•3æ¬¡
            for retry in range(3):
                try:
                    from sentence_transformers import SentenceTransformer, util
                    
                    if retry > 0:
                        wait_time = retry * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                        logger.info(f"ç¬¬{retry + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…{wait_time}ç§’...")
                        time.sleep(wait_time)
                    
                    # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                    self.embedding_model = SentenceTransformer(model_name, device='cpu')
                    self.embedding_util = util
                    logger.info(f"âœ… åœ¨çº¿æ¨¡å¼æˆåŠŸ: {model_name} (é‡è¯•{retry}æ¬¡)")
                    return True
                    
                except ImportError as e:
                    logger.error(f"âŒ æ— æ³•å¯¼å…¥sentence_transformers: {e}")
                    return False
                    
                except Exception as e:
                    logger.warning(f"åœ¨çº¿æ¨¡å‹ {model_name} ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                    if "Connection" in str(e) or "timeout" in str(e).lower():
                        logger.debug("æ£€æµ‹åˆ°ç½‘ç»œé—®é¢˜ï¼Œç»§ç»­é‡è¯•...")
                        continue
                    else:
                        logger.debug("éç½‘ç»œé—®é¢˜ï¼Œè·³è¿‡é‡è¯•")
                        break
        
        # æ‰€æœ‰åœ¨çº¿é€‰é¡¹éƒ½å¤±è´¥ï¼Œä½¿ç”¨çº¯å…³é”®è¯æ¨¡å¼
        logger.error("âŒ æ‰€æœ‰åœ¨çº¿æ¨¡å‹éƒ½æ— æ³•åŠ è½½")
        logger.info("ğŸ”„ åˆ‡æ¢åˆ°çº¯å…³é”®è¯åˆ†ç±»æ¨¡å¼ï¼ˆæ— éœ€embeddingï¼‰")
        self.embedding_model = None
        self.embedding_util = None
        return False

    def _apply_module_negative_filter(self, predicted_module: str, all_tags: List[str]) -> Tuple[str, str]:
        """
        åº”ç”¨æ¨¡å—ç‰¹å®šçš„è´Ÿé¢å…³é”®è¯è¿‡æ»¤
        
        Args:
            predicted_module: é¢„æµ‹çš„æ¨¡å—å
            all_tags: ç‰‡æ®µæ ‡ç­¾åˆ—è¡¨
            
        Returns:
            Tuple[str, str]: (æœ€ç»ˆæ¨¡å—, è¿‡æ»¤åŸå› )
        """
        # ğŸ”§ è¿‡æ»¤Noneå€¼ï¼Œç¡®ä¿ç±»å‹å®‰å…¨
        all_tags = [tag for tag in all_tags if tag is not None and isinstance(tag, str)]
        
        tags_text = ' '.join(all_tags).lower()
        
        # è·å–å…³é”®è¯é…ç½®
        try:
            from utils.config_manager import get_config_manager
            config_manager = get_config_manager()
            raw_config = config_manager.get_raw_config()
            keywords_config = raw_config
        except Exception as e:
            logger.error(f"æ— æ³•åŠ è½½å…³é”®è¯é…ç½®: {e}")
            return predicted_module, ""
        
        # ç‰¹æ®Šå¤„ç†ï¼šå–ç‚¹Â·æˆåˆ†&é…æ–¹æ¨¡å—çš„è´Ÿé¢è¿‡æ»¤
        if predicted_module == "å–ç‚¹Â·æˆåˆ†&é…æ–¹":
            try:
                # æ£€æŸ¥features_formulaçš„è´Ÿé¢å…³é”®è¯
                negatives = keywords_config.get('features_formula', {}).get('negative_keywords', [])
                detected_negatives = [neg for neg in negatives if neg and isinstance(neg, str) and neg in tags_text]
                
                if detected_negatives:
                    # ç‰¹åˆ«æ£€æŸ¥ç©è€ã€å•†åœºç­‰åœºæ™¯
                    play_scene_negatives = ['ç©å…·', 'æ»‘æ¢¯', 'å•†åœº', 'æ¸¸ä¹åœº', 'æˆ·å¤–', 'å…¬å›­', 'ç©è€', 'æ¸¸æˆ']
                    if any(neg in detected_negatives for neg in play_scene_negatives):
                        logger.info(f"ğŸš« ç§»é™¤å–ç‚¹Â·æˆåˆ†&é…æ–¹åˆ†ç±»: æ£€æµ‹åˆ°ç©è€/å•†åœºåœºæ™¯ {detected_negatives}")
                        return "å…¶ä»–", f"æ£€æµ‹åˆ°éå¥¶ç²‰ç›¸å…³åœºæ™¯: {detected_negatives}"
                    
                    # æ£€æŸ¥åŒ»ç–—åœºæ™¯
                    medical_negatives = ['åŒ»é™¢', 'è¯Šæ‰€', 'åŒ»ç”Ÿ', 'æ€¥è¯Š', 'å„¿ç§‘', 'æ²»ç–—']
                    if any(neg in detected_negatives for neg in medical_negatives):
                        logger.info(f"ğŸš« ç§»é™¤å–ç‚¹Â·æˆåˆ†&é…æ–¹åˆ†ç±»: æ£€æµ‹åˆ°åŒ»ç–—åœºæ™¯ {detected_negatives}")
                        return "ç—›ç‚¹", f"é‡æ–°åˆ†ç±»ä¸ºç—›ç‚¹: {detected_negatives}"
                    
                    # å…¶ä»–è´Ÿé¢å…³é”®è¯
                    logger.info(f"ğŸš« ç§»é™¤å–ç‚¹Â·æˆåˆ†&é…æ–¹åˆ†ç±»: æ£€æµ‹åˆ°è´Ÿé¢å…³é”®è¯ {detected_negatives}")
                    return "å…¶ä»–", f"æ£€æµ‹åˆ°è´Ÿé¢å…³é”®è¯: {detected_negatives}"
                    
            except Exception as e:
                logger.error(f"å–ç‚¹æ¨¡å—è´Ÿé¢è¿‡æ»¤å¤±è´¥: {e}")
        
        return predicted_module, ""

# ç¼“å­˜çš„æ˜ å°„å‡½æ•°
@st.cache_data(ttl=3600, show_spinner=False)
def get_cached_mapping_results(video_pool_path: str) -> tuple:
    """
    ç¼“å­˜çš„æ˜ å°„ç»“æœè·å–å‡½æ•°
    
    Args:
        video_pool_path: video_poolç›®å½•è·¯å¾„
        
    Returns:
        tuple: (mapped_segments, statistics)
    """
    mapper = VideoSegmentMapper()
    mapped_segments = mapper.scan_video_pool(video_pool_path)
    statistics = mapper.get_mapping_statistics(mapped_segments)
    return mapped_segments, statistics 