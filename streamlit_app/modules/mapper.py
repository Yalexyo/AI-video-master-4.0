"""
è§†é¢‘ç‰‡æ®µæ˜ å°„æ¨¡å—
ç”¨äºå°†video_poolä¸­çš„è§†é¢‘ç‰‡æ®µè‡ªåŠ¨æ˜ å°„åˆ°å››å¤§æ¨¡å—ï¼šç—›ç‚¹ã€è§£å†³æ–¹æ¡ˆå¯¼å…¥ã€å–ç‚¹Â·æˆåˆ†&é…æ–¹ã€ä¿ƒé”€æœºåˆ¶
"""

import os
import json
import glob
import subprocess
import logging
from typing import List, Dict, Any, Optional
import streamlit as st
from pathlib import Path
import threading
import time

logger = logging.getLogger(__name__)

class VideoSegmentMapper:
    """è§†é¢‘ç‰‡æ®µæ˜ å°„å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ˜ å°„å™¨"""
        self.four_modules = ["ç—›ç‚¹", "è§£å†³æ–¹æ¡ˆå¯¼å…¥", "å–ç‚¹Â·æˆåˆ†&é…æ–¹", "ä¿ƒé”€æœºåˆ¶"]
        
        # ğŸ”§ åˆå§‹åŒ–DeepSeekåˆ†æå™¨
        try:
            from streamlit_app.modules.ai_analyzers import DeepSeekAnalyzer
            self.deepseek_analyzer = DeepSeekAnalyzer()
            logger.info("DeepSeekåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        except ImportError as e:
            logger.warning(f"æ— æ³•å¯¼å…¥DeepSeekåˆ†æå™¨: {e}")
            self.deepseek_analyzer = None
        except Exception as e:
            logger.error(f"DeepSeekåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.deepseek_analyzer = None
        
        # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€é…ç½®æ–‡ä»¶åŠ è½½å…³é”®è¯è§„åˆ™
        try:
            from streamlit_app.utils.keyword_config import get_mapper_keywords, get_pain_point_rules
            
            # ä»é…ç½®æ–‡ä»¶åŠ è½½å…³é”®è¯æ˜ å°„
            self.keyword_rules = get_mapper_keywords()
            
            # ä»é…ç½®æ–‡ä»¶åŠ è½½ç—›ç‚¹ä¸“ç”¨è§„åˆ™
            self.pain_point_rules = get_pain_point_rules()
            
            logger.info("ğŸ¯ æ˜ å°„å™¨é…ç½®åŠ è½½æˆåŠŸï¼Œä½¿ç”¨ç»Ÿä¸€å…³é”®è¯é…ç½®æ–‡ä»¶")
            logger.info(f"   æ¨¡å—æ•°é‡: {len(self.keyword_rules)}")
            logger.info(f"   ç—›ç‚¹è§„åˆ™: {len(self.pain_point_rules)}")
            
        except ImportError:
            logger.warning("æ— æ³•å¯¼å…¥å…³é”®è¯é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„è§„åˆ™")
            # å…œåº•é…ç½®
            self.keyword_rules = {
                "ç—›ç‚¹": ["åŒ»é™¢", "å“­é—¹", "å‘çƒ§"],
                "è§£å†³æ–¹æ¡ˆå¯¼å…¥": ["å†²å¥¶", "å¥¶ç²‰ç½", "å¥¶ç“¶"],
                "å–ç‚¹Â·æˆåˆ†&é…æ–¹": ["A2", "HMO", "DHA", "å¯èµ‹"],
                "ä¿ƒé”€æœºåˆ¶": ["ä¼˜æƒ ", "é™æ—¶", "ä¿ƒé”€"]
            }
            self.pain_point_rules = {
                "baby_presence": ["å®å®", "å©´å„¿"],
                "negative_emotions": ["å“­", "ç—›è‹¦", "ç„¦è™‘"],
                "visual_signals": ["å®å®å“­", "åŒ»é™¢"]
            }
        
        # ğŸ”§ ç‰¹æ®Šå¤„ç†ï¼šå“ç‰Œä¼˜å…ˆçº§å…³é”®è¯ï¼ˆä»é…ç½®æ–‡ä»¶è·å–ï¼‰
        try:
            from streamlit_app.utils.keyword_config import get_brands
            self.brand_priority_keywords = get_brands()
        except ImportError:
            self.brand_priority_keywords = ["å¯èµ‹", "illuma", "Wyeth", "A2", "ATWO", "HMO", "DHA"]
        
        # ğŸ”§ Intentionally disable embedding models
        logger.info("EMBEDDING MODELS ARE DISABLED. Classification will rely on keywords and DeepSeek API.")
        self.embedding_model = None
        self.fallback_model = None # Though fallback_model was part of embedding, set to None for clarity
        self.embedding_util = None
    
    def _init_embedding_model_offline(self):
        """ç¦»çº¿æ¨¡å¼ä¸‹åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            import os
            import torch
            from pathlib import Path
            
            # æ£€æŸ¥ç¦»çº¿é…ç½®æ–‡ä»¶
            offline_config_path = Path("config/offline_config.py")
            if not offline_config_path.exists():
                logger.debug("ç¦»çº¿é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ç¦»çº¿æ¨¡å¼")
                return False
            
            # æ£€æŸ¥ä¸»æ¨¡å‹è·¯å¾„
            primary_model_path = Path("models/sentence_transformers/all-MiniLM-L6-v2")
            fallback_model_path = Path("models/sentence_transformers/paraphrase-multilingual-MiniLM-L12-v2")
            
            if not primary_model_path.exists():
                logger.debug(f"ä¸»æ¨¡å‹ä¸å­˜åœ¨: {primary_model_path}")
                return False
            
            # ğŸ”§ å¢å¼ºï¼šå¤šå±‚æ¬¡æ¨¡å‹åŠ è½½ç­–ç•¥
            from sentence_transformers import SentenceTransformer, util
            
            # ç­–ç•¥1ï¼šå°è¯•æ— è®¾å¤‡æŒ‡å®šåŠ è½½
            try:
                logger.debug("å°è¯•ç­–ç•¥1ï¼šæ— è®¾å¤‡æŒ‡å®šåŠ è½½")
                self.embedding_model = SentenceTransformer(str(primary_model_path))
                self.embedding_util = util
                logger.info(f"âœ… ç­–ç•¥1æˆåŠŸï¼šæ— è®¾å¤‡æŒ‡å®šåŠ è½½: {primary_model_path}")
                return True
                
            except Exception as e1:
                logger.debug(f"ç­–ç•¥1å¤±è´¥: {e1}")
                
                # ç­–ç•¥2ï¼šæ˜¾å¼æŒ‡å®šCPUè®¾å¤‡
                try:
                    logger.debug("å°è¯•ç­–ç•¥2ï¼šæ˜¾å¼CPUè®¾å¤‡åŠ è½½")
                    self.embedding_model = SentenceTransformer(str(primary_model_path), device='cpu')
                    self.embedding_util = util
                    logger.info(f"âœ… ç­–ç•¥2æˆåŠŸï¼šCPUè®¾å¤‡åŠ è½½: {primary_model_path}")
                    return True
                    
                except Exception as e2:
                    logger.debug(f"ç­–ç•¥2å¤±è´¥: {e2}")
                    
                    # ç­–ç•¥3ï¼šå°è¯•æ‰‹åŠ¨é…ç½®torch
                    try:
                        logger.debug("å°è¯•ç­–ç•¥3ï¼šæ‰‹åŠ¨é…ç½®torchè®¾å¤‡")
                        
                        # ä¸´æ—¶ç¦ç”¨MPS
                        original_mps_available = None
                        if hasattr(torch.backends, 'mps'):
                            original_mps_available = torch.backends.mps.is_available
                            torch.backends.mps.is_available = lambda: False
                        
                        self.embedding_model = SentenceTransformer(str(primary_model_path))
                        self.embedding_util = util
                        
                        # æ¢å¤MPSè®¾ç½®
                        if original_mps_available is not None:
                            torch.backends.mps.is_available = original_mps_available
                        
                        logger.info(f"âœ… ç­–ç•¥3æˆåŠŸï¼šæ‰‹åŠ¨é…ç½®torch: {primary_model_path}")
                        return True
                        
                    except Exception as e3:
                        logger.debug(f"ç­–ç•¥3å¤±è´¥: {e3}")
                        
                        # ç­–ç•¥4ï¼šå°è¯•ä»ä¸åŒè·¯å¾„åŠ è½½
                        try:
                            logger.debug("å°è¯•ç­–ç•¥4ï¼šç»å¯¹è·¯å¾„åŠ è½½")
                            abs_path = primary_model_path.resolve()
                            self.embedding_model = SentenceTransformer(str(abs_path), device='cpu')
                            self.embedding_util = util
                            logger.info(f"âœ… ç­–ç•¥4æˆåŠŸï¼šç»å¯¹è·¯å¾„åŠ è½½: {abs_path}")
                            return True
                            
                        except Exception as e4:
                            logger.debug(f"ç­–ç•¥4å¤±è´¥: {e4}")
                            e5 = None  # Initialize e5 here
                            
                            # ç­–ç•¥5ï¼šå°è¯•fallbackæ¨¡å‹
                            if fallback_model_path.exists():
                                try:
                                    logger.debug("å°è¯•ç­–ç•¥5ï¼šfallbackæ¨¡å‹")
                                    self.embedding_model = SentenceTransformer(str(fallback_model_path), device='cpu')
                                    self.embedding_util = util
                                    logger.info(f"âœ… ç­–ç•¥5æˆåŠŸï¼šfallbackæ¨¡å‹: {fallback_model_path}")
                                    return True
                                    
                                except Exception as err_fallback: # Changed variable name for clarity in assignment
                                    logger.debug(f"ç­–ç•¥5å¤±è´¥: {err_fallback}")
                                    e5 = err_fallback # Assign the actual error to e5
                                    
                            # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
                            logger.error(f"âŒ æ‰€æœ‰ç¦»çº¿åŠ è½½ç­–ç•¥éƒ½å¤±è´¥:")
                            logger.error(f"  ç­–ç•¥1 (æ— è®¾å¤‡): {e1}")
                            logger.error(f"  ç­–ç•¥2 (CPU): {e2}")
                            logger.error(f"  ç­–ç•¥3 (ç¦ç”¨MPS): {e3}")
                            logger.error(f"  ç­–ç•¥4 (ç»å¯¹è·¯å¾„): {e4}")
                            if fallback_model_path.exists():
                                logger.error(f"  ç­–ç•¥5 (fallback): {e5}") # Now e5 will be defined (or None)
                            
                            return False
            
        except ImportError as e:
            logger.warning(f"ç¦»çº¿æ¨¡å¼ï¼šsentence_transformersæœªå®‰è£…: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ ç¦»çº¿æ¨¡å¼åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def get_video_duration_ffprobe(self, file_path: str) -> float:
        """
        ä½¿ç”¨ffprobeè·å–è§†é¢‘æ—¶é•¿
        
        Args:
            file_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            float: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥è¿”å›0
        """
        try:
            cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', file_path
            ]
            # ğŸ”§ å‡å°‘è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                duration = float(data['format']['duration'])
                logger.debug(f"æˆåŠŸè·å–è§†é¢‘æ—¶é•¿: {file_path} -> {duration}ç§’")
                return duration
            else:
                logger.warning(f"ffprobeå‘½ä»¤æ‰§è¡Œå¤±è´¥: {file_path}, é”™è¯¯: {result.stderr}")
                return 0
                
        except subprocess.TimeoutExpired:
            logger.error(f"ffprobeè¶…æ—¶(10ç§’): {file_path}")
            return 0
        except json.JSONDecodeError as e:
            logger.error(f"ffprobeè¾“å‡ºJSONè§£æå¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return 0
        except Exception as e:
            logger.error(f"ffprobeè¯»å–å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return 0
    
    def classify_segment_by_tags(self, all_tags: List[str]) -> str:
        """
        åŸºäºç®€åŒ–5æƒ…ç»ª+å…³é”®è¯çš„é«˜ç²¾åº¦åˆ†ç±»
        
        Args:
            all_tags: ç‰‡æ®µçš„æ‰€æœ‰æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            str: åˆ†ç±»ç»“æœï¼Œå¦‚æœæ— æ³•åˆ†ç±»è¿”å›None
        """
        if not all_tags:
            logger.debug("classify_segment_by_tags: æ ‡ç­¾åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›None")
            return None
            
        tags_text = " ".join(all_tags).lower()
        logger.debug(f"classify_segment_by_tags: å¾…åˆ†ç±»æ ‡ç­¾æ–‡æœ¬: '{tags_text}'")
        
        # ğŸš« ç¬¬é›¶ä¼˜å…ˆçº§ï¼šæ£€æŸ¥å…¨å±€æ’é™¤å…³é”®è¯å’Œè´Ÿé¢å…³é”®è¯è¿‡æ»¤
        if self._is_excluded_by_negative_keywords(tags_text):
            logger.warning(f"ğŸš« ç‰‡æ®µè¢«æ’é™¤å…³é”®è¯è¿‡æ»¤: '{tags_text}'")
            return None
        
        # ğŸ¯ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šåŸºäº5ä¸ªå›ºå®šæƒ…ç»ªè¿›è¡Œåˆ†ç±»
        if "ç—›è‹¦" in tags_text or "ç„¦è™‘" in tags_text:
            logger.info(f"ğŸ¯ ç—›è‹¦/ç„¦è™‘æƒ…ç»ªåŒ¹é… -> ç—›ç‚¹")
            return "ç—›ç‚¹"
        
        if "å¿«ä¹" in tags_text or "å…´å¥‹" in tags_text:
            logger.info(f"ğŸ¯ å¿«ä¹/å…´å¥‹æƒ…ç»ªåŒ¹é… -> ä¿ƒé”€æœºåˆ¶")
            return "ä¿ƒé”€æœºåˆ¶"
        
        # ğŸ¯ ç¬¬äºŒä¼˜å…ˆçº§ï¼šç—›ç‚¹åœºæ™¯ç›´æ¥è¯†åˆ«
        try:
            from streamlit_app.utils.keyword_config import get_pain_point_rules
            pain_rules = get_pain_point_rules()
            pain_signals = pain_rules.get("visual_signals", [])
        except ImportError:
            pain_signals = [
                "å®å®å“­", "è¾“æ¶²ç®¡", "åŒ»é™¢", "ç—…åºŠ", "å‘çƒ§", "å¤œé†’", "çˆ¶æ¯ç„¦è™‘",
                "å“­é—¹", "æ‹‰è‚šå­", "ç”Ÿç—…", "åŒ»é™¢åœºæ™¯"
            ]
        
        for signal in pain_signals:
            if signal in tags_text:
                logger.info(f"ğŸ¯ ç—›ç‚¹åœºæ™¯ä¿¡å·åŒ¹é…: '{signal}' -> ç—›ç‚¹")
                return "ç—›ç‚¹"
        
        # ğŸ¯ ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šæ´»åŠ›ä¿ƒé”€åœºæ™¯è¯†åˆ«ï¼ˆğŸ”§ æ–°ç­–ç•¥ï¼šåªè®¤æ¬¢ä¹æ´»åŠ›é•œå¤´ï¼‰
        try:
            from streamlit_app.utils.keyword_config import get_promotion_vitality_keywords
            vitality_keywords = get_promotion_vitality_keywords()  # ğŸ”§ ä½¿ç”¨æ–°çš„æ´»åŠ›å…³é”®è¯
        except ImportError:
            vitality_keywords = [
                "å®å®å–å¥¶ç²‰å¼€å¿ƒ", "å–å¥¶ç²‰å¼€å¿ƒ", "å®å®ç©è€å¼€å¿ƒ", "å®å®å¥”è·‘", "å®å®è·³è·ƒ",
                "æˆ·å¤–ç©è€", "å…¬å›­", "æ¸¸ä¹åœº", "æ»‘æ¢¯", "è¹¦åºŠ", "å¤§ç¬‘", "æ¬¢ä¹", "æ´»åŠ›"
            ]
        
        # ğŸ¯ æ–°çº²é¢†ï¼šåªè®¤æ¬¢ä¹æ´»åŠ›é•œå¤´ï¼Œä¸éœ€æ–‡å­—CTA
        for keyword in vitality_keywords:
            if keyword in tags_text:
                logger.info(f"ğŸ¯ æ´»åŠ›æ¬¢ä¹ä¿¡å·åŒ¹é…: '{keyword}' -> ä¿ƒé”€æœºåˆ¶")
                return "ä¿ƒé”€æœºåˆ¶"
        
        # ğŸ¯ ç¬¬å››ä¼˜å…ˆçº§ï¼šå“ç‰Œå–ç‚¹è¯†åˆ«
        try:
            from streamlit_app.utils.keyword_config import get_brands
            brands = get_brands()
            brand_signals = [brand.lower() for brand in brands] + [
                "è¥å…»è¡¨", "è¥å…»æˆåˆ†", "åˆ†å­ç»“æ„", "å“ç‰Œlogo"
            ]
        except ImportError:
            brand_signals = [
                "å¯èµ‹", "wyeth", "illuma", "a2", "atwo", "hmo", "dha",
                "è¥å…»è¡¨", "è¥å…»æˆåˆ†", "åˆ†å­ç»“æ„", "å“ç‰Œlogo"
            ]
        
        for signal in brand_signals:
            if signal in tags_text:
                logger.info(f"ğŸ¯ å“ç‰Œå–ç‚¹ä¿¡å·åŒ¹é…: '{signal}' -> å–ç‚¹Â·æˆåˆ†&é…æ–¹")
                return "å–ç‚¹Â·æˆåˆ†&é…æ–¹"
        
        # ğŸ¯ ç¬¬äº”ä¼˜å…ˆçº§ï¼šè§£å†³æ–¹æ¡ˆåœºæ™¯è¯†åˆ«ï¼ˆæ‰©å±•å…³é”®è¯ï¼‰
        solution_signals = [
            # ğŸ”§ æ ¸å¿ƒç‰¹å¾ï¼šå¦ˆå¦ˆè¯´æ•™åœºæ™¯
            "å¦ˆå¦ˆ", "æ¯äº²", "é•¿è¾ˆ", "å¥¶å¥¶", "å©†å©†", "ä¸“å®¶", "åŒ»ç”Ÿ",
            "è¯´è¯", "è®²è§£", "æŒ‡å¯¼", "æ•™å¯¼", "ä¼ æˆ", "åˆ†äº«", "å‘Šè¯‰",
            "ç»éªŒ", "å»ºè®®", "æé†’", "å®å˜±", "å…³æ€€", "å‘µæŠ¤",
            
            # ğŸ”§ æ•™å­¦åœºæ™¯ç‰¹å¾
            "æ•™å­¦", "æ•™ç¨‹", "æ¼”ç¤º", "ç¤ºèŒƒ", "æŒ‡å¯¼è§†é¢‘", "ç»éªŒåˆ†äº«",
            "çŸ¥è¯†", "æ–¹æ³•", "æŠ€å·§", "çªé—¨", "æ³¨æ„äº‹é¡¹", "å°è´´å£«",
            "æ­£ç¡®æ–¹æ³•", "ä½¿ç”¨æ–¹æ³•", "å¦‚ä½•", "æ€ä¹ˆ", "æ­¥éª¤",
            
            # ğŸ”§ äº§å“ä½¿ç”¨åœºæ™¯ï¼ˆä¿ç•™é‡è¦çš„ï¼‰
            "å†²å¥¶", "å†²è°ƒ", "è°ƒé…", "é…åˆ¶", "å¥¶ç²‰ç½", "å¥¶ç“¶", "å‹ºå­",
            "å‡†å¤‡å¥¶ç²‰", "æ“ä½œæ¼”ç¤º", "äº§å“æ¼”ç¤º", "å°é¢æ“ä½œ",
            
            # ğŸ”§ å…³çˆ±äº’åŠ¨åœºæ™¯
            "è€å¿ƒ", "ç»†å¿ƒ", "æ¸©æŸ”", "å…³çˆ±", "æ¯çˆ±", "äº²æƒ…",
            "å¯¹è¯", "äº¤æµ", "æ²Ÿé€š", "è§£ç­”", "å›åº”",
            
            # ğŸ”§ åœºæ™¯ç¯å¢ƒï¼ˆå¦ˆå¦ˆè¯´æ•™å¸¸è§åœºæ™¯ï¼‰
            "å®¢å…", "æ²™å‘", "é¤æ¡Œ", "å¨æˆ¿", "å®¶åº­", "å±…å®¶",
            "é¢å¯¹é¢", "åç€", "èŠå¤©", "è°ˆè¯"
        ]
        
        for signal in solution_signals:
            if signal in tags_text:
                logger.info(f"ğŸ¯ è§£å†³æ–¹æ¡ˆä¿¡å·åŒ¹é…: '{signal}' -> è§£å†³æ–¹æ¡ˆå¯¼å…¥")
                return "è§£å†³æ–¹æ¡ˆå¯¼å…¥"
        
        # ğŸ”§ å¢å¼ºå…œåº•è§„åˆ™ï¼šæ¸©é¦¨ + äº§å“ç›¸å…³ = è§£å†³æ–¹æ¡ˆ
        if "æ¸©é¦¨" in tags_text:
            # æ£€æŸ¥æ˜¯å¦æœ‰äº§å“ç›¸å…³å…ƒç´ 
            product_related = ["å¥¶ç²‰", "å¥¶ç“¶", "å–‚å…»", "å†²è°ƒ", "äº§å“", "åŒ…è£…"]
            has_product = any(prod in tags_text for prod in product_related)
            if has_product:
                logger.info("ğŸ¯ æ¸©é¦¨+äº§å“åœºæ™¯åŒ¹é… -> è§£å†³æ–¹æ¡ˆå¯¼å…¥")
                return "è§£å†³æ–¹æ¡ˆå¯¼å…¥"
            else:
                logger.info("ğŸ¯ çº¯æ¸©é¦¨æƒ…ç»ªï¼Œæ— äº§å“å…ƒç´  -> è·³è¿‡")
        
        # ğŸ”§ ä¿ç•™åŸæœ‰çš„ç—›ç‚¹ç»„åˆåˆ¤æ–­ä½œä¸ºå…œåº•
        if self._is_pain_point_by_combination(all_tags):
            logger.info(f"ğŸ¯ ç—›ç‚¹ç»„åˆè§„åˆ™åŒ¹é… -> ç—›ç‚¹")
            return "ç—›ç‚¹"
        
        # ğŸ”§ ä¿ç•™åŸæœ‰çš„å…³é”®è¯è§„åˆ™ä½œä¸ºæœ€åå…œåº•
        for module, keywords in self.keyword_rules.items():
            if module == "ç—›ç‚¹":  # ç—›ç‚¹å·²ç»é€šè¿‡ä¸Šé¢çš„é€»è¾‘å¤„ç†
                continue
                
            for keyword in keywords:
                if keyword.lower() in tags_text:
                    logger.info(f"ğŸ¯ ä¼ ç»Ÿå…³é”®è¯åŒ¹é…: '{keyword}' -> {module}")
                    return module
        
        logger.info(f"ğŸ¯ æ‰€æœ‰è§„åˆ™å‡æœªåŒ¹é…: '{tags_text}'")
        return None
    
    def _is_pain_point_by_combination(self, all_tags: List[str]) -> bool:
        """
        é€šè¿‡ç»„åˆåˆ¤æ–­æ˜¯å¦ä¸ºç—›ç‚¹åœºæ™¯
        å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šå®å®/å©´å„¿åœ¨åœº + è´Ÿé¢æƒ…ç»ª/å“­é—¹
        
        Args:
            all_tags: ç‰‡æ®µçš„æ‰€æœ‰æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦ä¸ºç—›ç‚¹åœºæ™¯
        """
        if not all_tags:
            return False
            
        tags_text = " ".join(all_tags).lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®å®åœ¨åœº
        has_baby = False
        for baby_keyword in self.pain_point_rules["baby_presence"]:
            if baby_keyword.lower() in tags_text:
                has_baby = True
                logger.debug(f"ç—›ç‚¹æ£€æµ‹: å‘ç°å®å®å…³é”®è¯ '{baby_keyword.lower()}'")
                break
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è´Ÿé¢æƒ…ç»ª
        has_negative_emotion = False
        matched_emotion = None
        for emotion_keyword in self.pain_point_rules["negative_emotions"]:
            if emotion_keyword.lower() in tags_text:
                has_negative_emotion = True
                matched_emotion = emotion_keyword.lower()
                logger.debug(f"ç—›ç‚¹æ£€æµ‹: å‘ç°è´Ÿé¢æƒ…ç»ªå…³é”®è¯ '{emotion_keyword.lower()}'")
                break
        
        # åªæœ‰åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶æ‰åˆ¤å®šä¸ºç—›ç‚¹
        is_pain_point = has_baby and has_negative_emotion
        
        if is_pain_point:
            logger.info(f"ç—›ç‚¹æ£€æµ‹: âœ… åŒæ—¶æ»¡è¶³å®å®åœ¨åœºå’Œè´Ÿé¢æƒ…ç»ªï¼Œåˆ¤å®šä¸ºç—›ç‚¹åœºæ™¯")
            logger.info(f"    å®å®åœ¨åœº: {has_baby}")
            logger.info(f"    è´Ÿé¢æƒ…ç»ª: {has_negative_emotion} (åŒ¹é…è¯: {matched_emotion})")
        else:
            logger.debug(f"ç—›ç‚¹æ£€æµ‹: âŒ ä¸æ»¡è¶³ç—›ç‚¹æ¡ä»¶")
            logger.debug(f"    å®å®åœ¨åœº: {has_baby}")
            logger.debug(f"    è´Ÿé¢æƒ…ç»ª: {has_negative_emotion}")
        
        return is_pain_point
    
    def _detailed_exclusion_check(self, tags_text: str) -> Dict[str, Any]:
        """
        è¯¦ç»†çš„æ’é™¤å…³é”®è¯æ£€æŸ¥ï¼Œè¿”å›å®Œæ•´çš„æ£€æŸ¥ç»“æœ
        
        Args:
            tags_text: æ ‡ç­¾æ–‡æœ¬
            
        Returns:
            Dict: åŒ…å«æ˜¯å¦æ’é™¤å’Œå…·ä½“åŸå› çš„è¯¦ç»†ç»“æœ
        """
        result = {
            "is_excluded": False,
            "exclusion_reasons": [],
            "matched_keywords": {}
        }
        
        try:
            # æ£€æŸ¥æ˜ å°„è§„åˆ™é…ç½®æ–‡ä»¶
            config_file = "../config/matching_rules.json"
            if not os.path.exists(config_file):
                config_file = "config/matching_rules.json"
            
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    matching_rules = json.load(f)
                
                # æ£€æŸ¥æ¨¡å—çº§åˆ«çš„æ’é™¤å…³é”®è¯
                for module, rules in matching_rules.items():
                    if module == "GLOBAL_SETTINGS":
                        continue
                    
                    negative_keywords = rules.get("negative_keywords", [])
                    module_matches = []
                    
                    for neg_kw in negative_keywords:
                        if neg_kw.lower() in tags_text.lower():
                            module_matches.append(neg_kw)
                    
                    if module_matches:
                        result["is_excluded"] = True
                        result["exclusion_reasons"].append(f"{module}æ¨¡å—æ’é™¤: {module_matches}")
                        result["matched_keywords"][module] = module_matches
                
                # æ£€æŸ¥å…¨å±€æ’é™¤è®¾ç½®
                if "GLOBAL_SETTINGS" in matching_rules:
                    global_settings = matching_rules["GLOBAL_SETTINGS"]
                    irrelevant_categories = global_settings.get("irrelevant_scene_categories", {})
                    
                    for category, keywords in irrelevant_categories.items():
                        global_matches = []
                        for kw in keywords:
                            if kw.lower() in tags_text.lower():
                                global_matches.append(kw)
                        
                        if global_matches:
                            result["is_excluded"] = True
                            result["exclusion_reasons"].append(f"å…¨å±€æ’é™¤-{category}: {global_matches}")
                            result["matched_keywords"][f"global_{category}"] = global_matches
            
        except Exception as e:
            logger.error(f"æ’é™¤å…³é”®è¯æ£€æŸ¥å¤±è´¥: {e}")
        
        return result
    
    def _get_keyword_matches(self, all_tags: List[str]) -> Dict[str, List[str]]:
        """
        è·å–æ‰€æœ‰æ¨¡å—çš„å…³é”®è¯åŒ¹é…æƒ…å†µ
        
        Args:
            all_tags: ç‰‡æ®µæ ‡ç­¾
            
        Returns:
            Dict: æ¯ä¸ªæ¨¡å—åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
        """
        matches = {}
        tags_text = " ".join(all_tags).lower()
        
        for module in self.four_modules:
            module_matches = []
            
            # æ£€æŸ¥æ¯ç§ç±»å‹çš„å…³é”®è¯
            if module in self.keyword_rules:
                keywords = self.keyword_rules[module]
                for kw in keywords:
                    if kw.lower() in tags_text:
                        module_matches.append(kw)
            
            matches[module] = module_matches
        
        return matches
    
    def _is_excluded_by_negative_keywords(self, tags_text: str) -> bool:
        """
        æ£€æŸ¥ç‰‡æ®µæ˜¯å¦è¢«è´Ÿé¢å…³é”®è¯æ’é™¤
        
        Args:
            tags_text: æ ‡ç­¾æ–‡æœ¬ï¼ˆå·²è½¬æ¢ä¸ºå°å†™ï¼‰
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥è¢«æ’é™¤
        """
        try:
            # åŠ è½½matching_rules.jsonä¸­çš„æ’é™¤é…ç½®
            import json
            config_file = "../config/matching_rules.json"
            # å¦‚æœç›¸å¯¹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ç»å¯¹è·¯å¾„
            if not os.path.exists(config_file):
                config_file = "config/matching_rules.json"
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    matching_rules = json.load(f)
            else:
                logger.warning("matching_rules.jsonä¸å­˜åœ¨ï¼Œè·³è¿‡æ’é™¤æ£€æŸ¥")
                return False
            
            # ğŸš« æ£€æŸ¥å…¨å±€æ’é™¤å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            if "GLOBAL_SETTINGS" in matching_rules:
                global_exclusion = matching_rules["GLOBAL_SETTINGS"].get("global_exclusion_keywords", [])
                for global_kw in global_exclusion:
                    if global_kw.lower() in tags_text:
                        logger.warning(f"ğŸš« å…¨å±€æ’é™¤å…³é”®è¯è¿‡æ»¤: '{global_kw}' åœ¨ '{tags_text}' ä¸­")
                        return True
            
            # 1. æ£€æŸ¥å…¨å±€æ’é™¤å…³é”®è¯
            if "GLOBAL_SETTINGS" in matching_rules:
                global_settings = matching_rules["GLOBAL_SETTINGS"]
                irrelevant_categories = global_settings.get("irrelevant_scene_categories", {})
                
                for category, keywords in irrelevant_categories.items():
                    for keyword in keywords:
                        if keyword.lower() in tags_text:
                            logger.warning(f"ğŸš¨ å…¨å±€æ’é™¤å‘½ä¸­ - {category}: '{keyword}' åœ¨ '{tags_text}' ä¸­")
                            return True
            
            # 2. æ£€æŸ¥å„æ¨¡å—çš„è´Ÿé¢å…³é”®è¯
            modules = ["ç—›ç‚¹", "è§£å†³æ–¹æ¡ˆå¯¼å…¥", "å–ç‚¹Â·æˆåˆ†&é…æ–¹", "ä¿ƒé”€æœºåˆ¶"]
            
            for module in modules:
                if module in matching_rules:
                    negative_keywords = matching_rules[module].get("negative_keywords", [])
                    
                    for neg_kw in negative_keywords:
                        if neg_kw.lower() in tags_text:
                            logger.warning(f"ğŸš« æ¨¡å—æ’é™¤å‘½ä¸­ - {module}: '{neg_kw}' åœ¨ '{tags_text}' ä¸­")
                            return True
            
            # 3. æ£€æŸ¥solution_introå’Œpain_pointçš„ç‰¹æ®Šæ’é™¤è§„åˆ™
            for special_module in ["solution_intro", "pain_point"]:
                if special_module in matching_rules:
                    negative_keywords = matching_rules[special_module].get("negative_keywords", [])
                    
                    for neg_kw in negative_keywords:
                        if neg_kw.lower() in tags_text:
                            logger.warning(f"ğŸš« ç‰¹æ®Šæ’é™¤å‘½ä¸­ - {special_module}: '{neg_kw}' åœ¨ '{tags_text}' ä¸­")
                            return True
            
            return False
            
        except Exception as e:
            logger.error(f"æ’é™¤å…³é”®è¯æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _classify_by_embedding_similarity(self, all_tags: List[str]) -> str:
        """
        ä½¿ç”¨embeddingç›¸ä¼¼åº¦è¿›è¡Œåˆ†ç±»ï¼ˆDeepSeekè¶…æ—¶æ—¶çš„fallbackï¼‰
        å¦‚æœembeddingæ¨¡å‹ä¸å¯ç”¨ï¼Œåˆ™ä½¿ç”¨å…³é”®è¯åˆ†ç±»
        
        Args:
            all_tags: æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            åˆ†ç±»ç»“æœ
        """
        # ğŸ”§ Embedding models are disabled. This method now directly falls back to keyword classification.
        logger.info("Embedding models are disabled. _classify_by_embedding_similarity falling back to keywords.")
        
        # Original logic when embedding_model is None:
        # if not self.embedding_model or not self.embedding_util:
        #     logger.info("Embeddingæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯åˆ†ç±»ä½œä¸ºfallback")
        #     keyword_result = self.classify_segment_by_tags(all_tags)
        #     return keyword_result if keyword_result else "å…¶ä»–"
        
        # Simplified fallback:
        keyword_result = self.classify_segment_by_tags(all_tags)
        return keyword_result if keyword_result else "å…¶ä»–"
            
        # The rest of the original method (embedding logic) is now unreachable 
        # because self.embedding_model will always be None.
        # Consider removing the unreachable code below in a future cleanup if this change is permanent.
        
        # if not all_tags:
        #     return "å…¶ä»–"
            
        # tags_text = " ".join(all_tags)
        # logger.info(f"ä½¿ç”¨embeddingç›¸ä¼¼åº¦åˆ†ç±»: {tags_text}")
        
        # # ğŸ”§ å°è¯•ä¸»æ¨¡å‹åˆ†ç±»
        # result = self._classify_with_model(all_tags, self.embedding_model, "ä¸»æ¨¡å‹")
        # if result != "æ¨¡å‹å¤±è´¥":
        #     return result
        
        # # ğŸ”§ ä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•fallbackæ¨¡å‹
        # if hasattr(self, 'fallback_model') and self.fallback_model:
        #     logger.warning("ä¸»æ¨¡å‹åˆ†ç±»å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨fallbackæ¨¡å‹")
        #     result = self._classify_with_model(all_tags, self.fallback_model, "fallbackæ¨¡å‹")
        #     if result != "æ¨¡å‹å¤±è´¥":
        #         return result
        
        # # ğŸ”§ æ‰€æœ‰embeddingæ¨¡å‹éƒ½å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»
        # logger.error("æ‰€æœ‰embeddingæ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»")
        # keyword_result = self.classify_segment_by_tags(all_tags)
        # return keyword_result if keyword_result else "å…¶ä»–"
    
    def _classify_with_model(self, all_tags: List[str], model, model_name: str) -> str:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œåˆ†ç±»
        
        Args:
            all_tags: æ ‡ç­¾åˆ—è¡¨
            model: SentenceTransformeræ¨¡å‹å®ä¾‹
            model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            åˆ†ç±»ç»“æœï¼Œå¤±è´¥æ—¶è¿”å›"æ¨¡å‹å¤±è´¥"
        """
        if not model or not self.embedding_util:
            logger.debug(f"{model_name}ä¸å¯ç”¨")
            return "æ¨¡å‹å¤±è´¥"
            
        if not all_tags:
            return "å…¶ä»–"
            
        tags_text = " ".join(all_tags)
        
        try:
            # å®šä¹‰å„ç±»åˆ«çš„ä»£è¡¨æ€§æ–‡æœ¬
            category_texts = {
                "ç—›ç‚¹æœºåˆ¶": "å®å®å“­é—¹ å©´å„¿ä¸é€‚ å–‚å…»å›°éš¾ ç¡çœ é—®é¢˜ å¥åº·æ‹…å¿§",
                "ä¿ƒé”€æœºåˆ¶": "å®å®å–å¥¶ç²‰å¼€å¿ƒ å®å®ç©è€å¼€å¿ƒ å®å®å¥”è·‘ æˆ·å¤–ç©è€ æ¸¸ä¹åœº å¤§ç¬‘ æ¬¢ä¹ æ´»åŠ›",
                "ç§‘æ™®æœºåˆ¶": "è¥å…»çŸ¥è¯† è‚²å„¿æ–¹æ³• å¥åº·æŒ‡å¯¼ ä¸“ä¸šå»ºè®® ç§‘å­¦å–‚å…»",
                "æƒ…æ„Ÿæœºåˆ¶": "æ¯çˆ± äº²æƒ… æ¸©é¦¨ é™ªä¼´ æˆé•¿ å…³çˆ±"
            }
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            embeddings = model.encode([tags_text] + list(category_texts.values()))
            similarities = self.embedding_util.pytorch_cos_sim(embeddings[0], embeddings[1:])
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç±»åˆ«
            max_similarity = float(similarities.max())
            max_index = int(similarities.argmax())
            
            categories = list(category_texts.keys())
            best_category = categories[max_index]
            
            # è®¾ç½®é˜ˆå€¼ï¼Œç›¸ä¼¼åº¦å¤ªä½åˆ™è¿”å›"å…¶ä»–"
            threshold = 0.4
            if max_similarity < threshold:
                logger.info(f"{model_name} ç›¸ä¼¼åº¦ {max_similarity:.3f} ä½äºé˜ˆå€¼ {threshold}ï¼Œè¿”å›å…¶ä»–")
                return "å…¶ä»–"
            
            logger.info(f"{model_name} åˆ†ç±»æˆåŠŸ: {tags_text} -> {best_category} (ç›¸ä¼¼åº¦: {max_similarity:.3f})")
            return best_category
            
        except Exception as e:
            logger.error(f"{model_name} åˆ†ç±»å¤±è´¥: {e}")
            return "æ¨¡å‹å¤±è´¥"
    
    def classify_segment_by_deepseek(self, all_tags: List[str]) -> str:
        """
        ğŸ”§ ä½¿ç”¨DeepSeekæ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼ˆç»Ÿä¸€prompté…ç½®ï¼‰
        """
        if not self.deepseek_analyzer or not self.deepseek_analyzer.is_available():
            logger.warning("DeepSeekåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨embedding fallback")
            return self._classify_by_embedding_similarity(all_tags)
        
        if not all_tags:
            logger.info("æ ‡ç­¾ä¸ºç©ºï¼ŒDeepSeekè·³è¿‡åˆ†ç±»ï¼Œè¿”å›å…¶ä»–")
            return "å…¶ä»–"
        
        tags_text = " ".join(all_tags)
        logger.info(f"ä½¿ç”¨DeepSeekåˆ†ç±»: {tags_text}")
        
        try:
            analyzer = self.deepseek_analyzer
            
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„prompté…ç½®
            try:
                from streamlit_app.utils.keyword_config import get_mapper_keywords
                keywords = get_mapper_keywords()
                
                # æ„å»ºåˆ†ç±»ç³»ç»Ÿprompt
                system_content = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¯å©´è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·æ ¹æ®è§†é¢‘æ ‡ç­¾ï¼Œå°†å†…å®¹åˆ†ç±»ä¸ºä»¥ä¸‹å››ç§æœºåˆ¶ä¹‹ä¸€ï¼š

1. ç—›ç‚¹æœºåˆ¶ï¼šå®å®å“­é—¹ã€ä¸é€‚ã€å–‚å…»å›°éš¾ç­‰è´Ÿé¢æƒ…å†µ
2. ä¿ƒé”€æœºåˆ¶ï¼šå®å®å¼€å¿ƒã€æ´»åŠ›ã€ç©è€ç­‰æ­£é¢åœºæ™¯  
3. ç§‘æ™®æœºåˆ¶ï¼šè¥å…»çŸ¥è¯†ã€è‚²å„¿æ–¹æ³•ã€ä¸“ä¸šæŒ‡å¯¼
4. æƒ…æ„Ÿæœºåˆ¶ï¼šæ¯çˆ±ã€äº²æƒ…ã€æ¸©é¦¨ã€é™ªä¼´

è¯·åªå›ç­”æœºåˆ¶åç§°ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œå›ç­”"å…¶ä»–"ã€‚"""
                
            except Exception as e:
                logger.warning(f"æ— æ³•å¯¼å…¥ç»Ÿä¸€å…³é”®è¯é…ç½®: {e}")
                system_content = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¯å©´è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚è¯·æ ¹æ®è§†é¢‘æ ‡ç­¾ï¼Œå°†å†…å®¹åˆ†ç±»ä¸ºä»¥ä¸‹å››ç§æœºåˆ¶ä¹‹ä¸€ï¼š

1. ç—›ç‚¹æœºåˆ¶ï¼šå®å®å“­é—¹ã€ä¸é€‚ã€å–‚å…»å›°éš¾ç­‰è´Ÿé¢æƒ…å†µ
2. ä¿ƒé”€æœºåˆ¶ï¼šå®å®å¼€å¿ƒã€æ´»åŠ›ã€ç©è€ç­‰æ­£é¢åœºæ™¯  
3. ç§‘æ™®æœºåˆ¶ï¼šè¥å…»çŸ¥è¯†ã€è‚²å„¿æ–¹æ³•ã€ä¸“ä¸šæŒ‡å¯¼
4. æƒ…æ„Ÿæœºåˆ¶ï¼šæ¯çˆ±ã€äº²æƒ…ã€æ¸©é¦¨ã€é™ªä¼´

è¯·åªå›ç­”æœºåˆ¶åç§°ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚å¦‚æœæ— æ³•ç¡®å®šï¼Œå›ç­”"å…¶ä»–"ã€‚"""
            
            messages = [
                {
                    "role": "system",
                    "content": system_content
                },
                {
                    "role": "user", 
                    "content": f"è¯·åˆ†æè¿™äº›è§†é¢‘æ ‡ç­¾ï¼š{tags_text}"
                }
            ]
            
            # ä½¿ç”¨çº¿ç¨‹å’Œè¶…æ—¶æœºåˆ¶
            result_container = {"result": None, "error": None}
            
            def call_deepseek():
                try:
                    response = analyzer._chat_completion(messages)
                    if response and "choices" in response and len(response["choices"]) > 0:
                        content = response["choices"][0]["message"]["content"].strip()
                        result_container["result"] = content
                    else:
                        result_container["error"] = "APIå“åº”æ ¼å¼æ— æ•ˆ"
                except Exception as e:
                    result_container["error"] = str(e)
            
            # å¯åŠ¨çº¿ç¨‹
            thread = threading.Thread(target=call_deepseek)
            thread.daemon = True
            thread.start()
            
            # ç­‰å¾…ç»“æœï¼Œè®¾ç½®è¶…æ—¶
            timeout = 10  # 10ç§’è¶…æ—¶
            thread.join(timeout)
            
            if thread.is_alive():
                # è¶…æ—¶ï¼Œä½¿ç”¨embedding fallback
                logger.warning(f"DeepSeek APIè°ƒç”¨è¶…æ—¶ï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»: {tags_text}")
                keyword_result = self.classify_segment_by_tags(all_tags)
                return keyword_result if keyword_result else "å…¶ä»–"
            
            if result_container["error"]:
                logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {result_container['error']}")
                logger.info("å›é€€åˆ°å…³é”®è¯åˆ†ç±»")
                keyword_result = self.classify_segment_by_tags(all_tags)
                return keyword_result if keyword_result else "å…¶ä»–"
            
            result = result_container["result"]
            if result:
                # éªŒè¯è¿”å›çš„åˆ†ç±»æ˜¯å¦æœ‰æ•ˆ
                valid_categories = ["ç—›ç‚¹æœºåˆ¶", "ä¿ƒé”€æœºåˆ¶", "ç§‘æ™®æœºåˆ¶", "æƒ…æ„Ÿæœºåˆ¶", "å…¶ä»–"]
                if any(cat in result for cat in valid_categories):
                    for cat in valid_categories:
                        if cat in result:
                            logger.info(f"DeepSeekåˆ†ç±»æˆåŠŸ: {tags_text} -> {cat}")
                            return cat
                else:
                    logger.warning(f"DeepSeekè¿”å›æ— æ•ˆåˆ†ç±»: {result}, å›é€€åˆ°å…³é”®è¯åˆ†ç±»")
                    keyword_result = self.classify_segment_by_tags(all_tags)
                    return keyword_result if keyword_result else "å…¶ä»–"
            else:
                logger.warning("DeepSeek APIå“åº”æ— æ•ˆï¼Œå›é€€åˆ°å…³é”®è¯åˆ†ç±»")
                keyword_result = self.classify_segment_by_tags(all_tags)
                return keyword_result if keyword_result else "å…¶ä»–"
                
        except Exception as e:
            logger.error(f"DeepSeekåˆ†ç±»å¤±è´¥: {e}")
            logger.info("å›é€€åˆ°å…³é”®è¯åˆ†ç±»")
            keyword_result = self.classify_segment_by_tags(all_tags)
            return keyword_result if keyword_result else "å…¶ä»–"
    
    def classify_segment(self, all_tags: List[str], segment_info: Optional[Dict[str, Any]] = None) -> str:
        """
        å¯¹ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼ˆå…³é”®è¯ä¼˜å…ˆï¼ŒDeepSeekå…œåº•ï¼‰ï¼Œå¹¶è®°å½•è¯¦ç»†å†³ç­–è¿‡ç¨‹
        
        Args:
            all_tags: ç‰‡æ®µçš„æ‰€æœ‰æ ‡ç­¾åˆ—è¡¨
            segment_info: ç‰‡æ®µè¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            
        Returns:
            str: åˆ†ç±»ç»“æœ
        """
        # ğŸ”§ æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.debug(f"å¼€å§‹åˆ†ç±»ç‰‡æ®µï¼Œæ ‡ç­¾: {all_tags}")
        
        # åˆå§‹åŒ–é€‰ç‰‡æ—¥å¿—
        try:
            from streamlit_app.modules.selection_logger import get_selection_logger
            selection_logger = get_selection_logger()
        except ImportError:
            selection_logger = None
        
        analysis_steps = []
        segment_name = segment_info.get("file_name", "unknown") if segment_info else "unknown"
        
        # ğŸš« ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥æ’é™¤å…³é”®è¯
        tags_text = " ".join(all_tags).lower()
        exclusion_result = self._detailed_exclusion_check(tags_text)
        
        if selection_logger:
            exclusion_step = selection_logger.log_exclusion_check(
                segment_name, all_tags, exclusion_result
            )
            analysis_steps.append(exclusion_step)
        
        if exclusion_result.get("is_excluded", False):
            logger.info(f"ğŸš« ç‰‡æ®µè¢«æ’é™¤å…³é”®è¯è¿‡æ»¤: '{' '.join(all_tags)}'")
            
            if selection_logger and segment_info:
                selection_logger.log_segment_analysis(
                    segment_info, analysis_steps, "å…¶ä»–", 
                    f"è¢«æ’é™¤å…³é”®è¯è¿‡æ»¤: {exclusion_result.get('exclusion_reasons', [])}"
                )
            
            return "å…¶ä»–"
        
        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šåŸºäºå…³é”®è¯çš„å¿«é€Ÿåˆ†ç±»
        keyword_matches = self._get_keyword_matches(all_tags)
        keyword_result = self.classify_segment_by_tags(all_tags)
        
        if selection_logger:
            keyword_step = selection_logger.log_keyword_classification(
                segment_name, all_tags, keyword_matches, keyword_result
            )
            analysis_steps.append(keyword_step)
        
        if keyword_result:
            logger.debug(f"å…³é”®è¯åˆ†ç±»æˆåŠŸ: {all_tags} -> {keyword_result}")
            
            if selection_logger and segment_info:
                selection_logger.log_segment_analysis(
                    segment_info, analysis_steps, keyword_result,
                    f"å…³é”®è¯åŒ¹é…æˆåŠŸ: {keyword_matches.get(keyword_result, [])}"
                )
            
            return keyword_result
        
        # ğŸ¤– ç¬¬ä¸‰æ­¥ï¼šå¦‚æœå…³é”®è¯åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨AIåˆ†ç±»
        logger.info(f"å…³é”®è¯è§„åˆ™æ— æ³•åˆ†ç±»æ ‡ç­¾ {all_tags}ï¼Œä½¿ç”¨DeepSeekæ¨¡å‹")
        
        ai_start_time = time.time()
        deepseek_result = self.classify_segment_by_deepseek(all_tags)
        ai_duration = time.time() - ai_start_time
        
        if selection_logger:
            ai_step = selection_logger.log_ai_classification(
                segment_name, all_tags, deepseek_result, 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                {"duration": ai_duration, "error": None}
            )
            analysis_steps.append(ai_step)
        
        logger.debug(f"DeepSeekåˆ†ç±»ç»“æœ: {all_tags} -> {deepseek_result}")
        
        if selection_logger and segment_info:
            if deepseek_result and deepseek_result != "å…¶ä»–":
                selection_logger.log_segment_analysis(
                    segment_info, analysis_steps, deepseek_result,
                    "AIåˆ†ç±»æˆåŠŸï¼Œå…³é”®è¯åˆ†ç±»å¤±è´¥"
                )
            else:
                selection_logger.log_segment_analysis(
                    segment_info, analysis_steps, "å…¶ä»–",
                    "å…³é”®è¯å’ŒAIåˆ†ç±»éƒ½æ— æ³•ç¡®å®šç±»åˆ«"
                )
        
        return deepseek_result
    
    def scan_video_pool(self, video_pool_path: str = "data/output/google_video/video_pool") -> List[Dict[str, Any]]:
        """
        æ‰«ævideo_poolç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
        
        Args:
            video_pool_path: video_poolç›®å½•è·¯å¾„
            
        Returns:
            List[Dict]: æ˜ å°„åçš„ç‰‡æ®µåˆ—è¡¨
        """
        mapped_segments = []
        
        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        if not os.path.exists(video_pool_path):
            logger.error(f"video_poolç›®å½•ä¸å­˜åœ¨: {video_pool_path}")
            return mapped_segments
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(video_pool_path, "*.json"))
        logger.info(f"åœ¨ {video_pool_path} ä¸­æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        if not json_files:
            logger.warning(f"åœ¨ {video_pool_path} ä¸­æœªæ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶")
            return mapped_segments
        
        # å¤„ç†æ¯ä¸ªJSONæ–‡ä»¶ï¼Œæ·»åŠ è¿›åº¦è·Ÿè¸ª
        for file_idx, json_file in enumerate(json_files):
            try:
                logger.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {file_idx + 1}/{len(json_files)}: {os.path.basename(json_file)}")
                
                # ğŸ”§ æ·»åŠ æ–‡ä»¶å¤§å°æ£€æŸ¥ï¼Œé¿å…å¤„ç†è¿‡å¤§çš„æ–‡ä»¶
                file_size = os.path.getsize(json_file)
                if file_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
                    logger.warning(f"è·³è¿‡è¿‡å¤§çš„æ–‡ä»¶: {json_file} ({file_size/1024/1024:.1f}MB)")
                    continue
                
                with open(json_file, 'r', encoding='utf-8') as f:
                    video_data = json.load(f)
                
                video_id = video_data.get('video_id', 'unknown')
                segments = video_data.get('segments', [])
                
                logger.info(f"å¤„ç†è§†é¢‘ {video_id}ï¼ŒåŒ…å« {len(segments)} ä¸ªç‰‡æ®µ")
                
                # ğŸ”§ é™åˆ¶æ¯ä¸ªæ–‡ä»¶å¤„ç†çš„ç‰‡æ®µæ•°é‡ï¼Œé¿å…è¿‡åº¦å¤„ç†
                max_segments_per_file = 100
                if len(segments) > max_segments_per_file:
                    logger.warning(f"æ–‡ä»¶ {json_file} åŒ…å« {len(segments)} ä¸ªç‰‡æ®µï¼Œåªå¤„ç†å‰ {max_segments_per_file} ä¸ª")
                    segments = segments[:max_segments_per_file]
                
                # å¤„ç†æ¯ä¸ªç‰‡æ®µ
                for seg_idx, segment in enumerate(segments):
                    try:
                        # ğŸ”§ æ·»åŠ ç‰‡æ®µå¤„ç†è¿›åº¦æ—¥å¿—
                        if seg_idx % 10 == 0:  # æ¯10ä¸ªç‰‡æ®µè®°å½•ä¸€æ¬¡
                            logger.debug(f"å¤„ç†ç‰‡æ®µè¿›åº¦: {seg_idx + 1}/{len(segments)}")
                        
                        # åŸºæœ¬ä¿¡æ¯æå–
                        file_path = segment.get('file_path', '')
                        all_tags = segment.get('all_tags', [])
                        quality_score = segment.get('quality_score', 0.9)
                        confidence = segment.get('confidence', 0.8)
                        file_name = segment.get('file_name', '')
                        
                        # ğŸ”§ æ·»åŠ analysis_methodå­—æ®µå¤„ç†ï¼Œä¸ºæ—§JSONæ–‡ä»¶æä¾›é»˜è®¤å€¼
                        analysis_method = segment.get('analysis_method', 'visual')  # é»˜è®¤ä¸ºvisual
                        transcription = segment.get('transcription', None)  # è¯­éŸ³è½¬å½•å†…å®¹
                        
                        # ğŸ”§ NEW: å…¼å®¹æ—§æ ¼å¼JSONæ–‡ä»¶ï¼Œä»å…¶ä»–å­—æ®µæ„å»ºall_tags
                        if not all_tags:
                            # ä»æ—§æ ¼å¼å­—æ®µæ„å»ºæ ‡ç­¾ - æ”¯æŒå­—ç¬¦ä¸²å’Œé€—å·åˆ†éš”æ ¼å¼
                            raw_fields = [
                                segment.get('object', ''),
                                segment.get('sence', ''),
                                segment.get('emotion', ''),
                                segment.get('brand_elements', '')
                            ]
                            
                            all_tags = []
                            for raw_field in raw_fields:
                                if not raw_field:
                                    continue
                                    
                                # å¤„ç†é€—å·åˆ†éš”çš„æƒ…å†µ
                                if ',' in raw_field:
                                    tags = raw_field.split(',')
                                else:
                                    tags = [raw_field]
                                
                                # æ¸…ç†å’Œæ·»åŠ æ ‡ç­¾
                                for tag in tags:
                                    clean_tag = tag.strip()
                                    if clean_tag and clean_tag not in all_tags:
                                        all_tags.append(clean_tag)
                            
                            logger.debug(f"ä»æ—§æ ¼å¼æ„å»ºæ ‡ç­¾: {file_name} -> {all_tags}")
                        
                        # è·³è¿‡ç©ºæ ‡ç­¾ç‰‡æ®µ
                        if not all_tags:
                            logger.debug(f"è·³è¿‡ç©ºæ ‡ç­¾ç‰‡æ®µ: {file_name}")
                            continue
                        
                        # ğŸ¯ NEW: è·³è¿‡äººè„¸ç‰¹å†™ç‰‡æ®µ
                        is_face_close_up = segment.get('is_face_close_up', False)
                        is_unusable = segment.get('unusable', False)
                        unusable_reason = segment.get('unusable_reason', '')
                        
                        if is_face_close_up or is_unusable:
                            logger.info(f"ğŸš« è·³è¿‡äººè„¸ç‰¹å†™/ä¸å¯ç”¨ç‰‡æ®µ: {file_name} (åŸå› : {unusable_reason})")
                            continue
                        
                        # ğŸ”§ æ·»åŠ è¶…æ—¶æ§åˆ¶çš„è§†é¢‘æ—¶é•¿æå–
                        duration = 0
                        if file_path and os.path.exists(file_path):
                            try:
                                duration = self.get_video_duration_ffprobe(file_path)
                            except Exception as e:
                                logger.warning(f"è·å–è§†é¢‘æ—¶é•¿å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                                duration = 0
                        else:
                            logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                        
                        # ğŸ”§ æ„å»ºç‰‡æ®µä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•
                        segment_info_for_logging = {
                            "file_name": file_name,
                            "duration": duration,
                            "all_tags": all_tags,
                            "combined_quality": quality_score * confidence,
                            "is_face_close_up": is_face_close_up
                        }
                        
                        # ğŸ”§ æ·»åŠ è¶…æ—¶æ§åˆ¶çš„åˆ†ç±»å¤„ç†
                        try:
                            category = self.classify_segment(all_tags, segment_info_for_logging)
                        except Exception as e:
                            logger.error(f"ç‰‡æ®µåˆ†ç±»å¤±è´¥: {all_tags}, é”™è¯¯: {e}")
                            category = "å…¶ä»–"
                        
                        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†
                        combined_quality = quality_score * confidence
                        
                        # æ„å»ºæ˜ å°„ç»“æœ
                        mapped_segment = {
                            "segment_id": f"{video_id}_{file_name}",
                            "file_path": file_path,
                            "file_name": file_name,  # ğŸ”§ ç¡®ä¿ä½¿ç”¨file_nameå­—æ®µå
                            "filename": file_name,   # ğŸ”§ æ·»åŠ filenameå­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
                            "all_tags": all_tags,
                            "category": category,    # ğŸ”§ ç¡®ä¿ä½¿ç”¨categoryå­—æ®µå
                            "classification": category,  # ğŸ”§ æ·»åŠ classificationå­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
                            "quality_score": quality_score,
                            "confidence": confidence,
                            "combined_quality": combined_quality,
                            "duration": duration,
                            "video_id": video_id,
                            "analysis_method": analysis_method,  # ğŸ”§ æ·»åŠ analysis_methodå­—æ®µ
                            "transcription": transcription,      # ğŸ”§ æ·»åŠ transcriptionå­—æ®µ
                            "success": True
                        }
                        
                        mapped_segments.append(mapped_segment)
                        logger.debug(f"æˆåŠŸæ˜ å°„ç‰‡æ®µ: {file_name} -> {category} (æ—¶é•¿: {duration:.2f}s)")
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†ç‰‡æ®µå¤±è´¥: {segment.get('file_name', segment.get('filename', 'unknown'))}, é”™è¯¯: {e}")
                        continue
                
                logger.info(f"æ–‡ä»¶ {os.path.basename(json_file)} å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len([s for s in mapped_segments if s['video_id'] == video_id])} ä¸ªç‰‡æ®µ")
                        
            except Exception as e:
                logger.error(f"å¤„ç†JSONæ–‡ä»¶å¤±è´¥: {json_file}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"æ˜ å°„å®Œæˆï¼Œå…±å¤„ç† {len(mapped_segments)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
        return mapped_segments
    
    def get_mapping_statistics(self, mapped_segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            mapped_segments: æ˜ å°„åçš„ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "total_segments": len(mapped_segments),
            "by_category": {},
            "by_video": {},
            "quality_stats": {
                "avg_quality": 0,
                "avg_confidence": 0,
                "avg_duration": 0,
                "total_duration": 0
            }
        }
        
        if not mapped_segments:
            return stats
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for module in self.four_modules + ["å…¶ä»–"]:
            module_segments = [s for s in mapped_segments if s["category"] == module]
            stats["by_category"][module] = {
                "count": len(module_segments),
                "total_duration": sum(s["duration"] for s in module_segments),
                "avg_quality": sum(s["combined_quality"] for s in module_segments) / len(module_segments) if module_segments else 0
            }
        
        # æŒ‰è§†é¢‘ç»Ÿè®¡
        video_counts = {}
        for segment in mapped_segments:
            video_id = segment["video_id"]
            if video_id not in video_counts:
                video_counts[video_id] = 0
            video_counts[video_id] += 1
        stats["by_video"] = video_counts
        
        # è´¨é‡ç»Ÿè®¡
        if mapped_segments:
            stats["quality_stats"]["avg_quality"] = sum(s["quality_score"] for s in mapped_segments) / len(mapped_segments)
            stats["quality_stats"]["avg_confidence"] = sum(s["confidence"] for s in mapped_segments) / len(mapped_segments)
            stats["quality_stats"]["avg_duration"] = sum(s["duration"] for s in mapped_segments) / len(mapped_segments)
            stats["quality_stats"]["total_duration"] = sum(s["duration"] for s in mapped_segments)
        
        return stats

    def _init_embedding_model_online(self):
        """åœ¨çº¿æ¨¡å¼ä¸‹åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        import time
        
        # åœ¨çº¿æ¨¡å¼çš„å¤šä¸ªfallbacké€‰é¡¹
        online_models = [
            'all-MiniLM-L6-v2',          # ä¸»é€‰æ‹©
            'all-mpnet-base-v2',         # å¤‡é€‰1
            'paraphrase-MiniLM-L6-v2'    # å¤‡é€‰2
        ]
        
        for attempt, model_name in enumerate(online_models, 1):
            logger.info(f"å°è¯•åœ¨çº¿æ¨¡å‹ {attempt}/{len(online_models)}: {model_name}")
            
            # æ¯ä¸ªæ¨¡å‹å°è¯•3æ¬¡
            for retry in range(3):
                try:
                    from sentence_transformers import SentenceTransformer, util
                    
                    if retry > 0:
                        wait_time = retry * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                        logger.info(f"ç¬¬{retry + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…{wait_time}ç§’...")
                        time.sleep(wait_time)
                    
                    # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                    self.embedding_model = SentenceTransformer(model_name, device='cpu')
                    self.embedding_util = util
                    logger.info(f"âœ… åœ¨çº¿æ¨¡å¼æˆåŠŸ: {model_name} (é‡è¯•{retry}æ¬¡)")
                    return True
                    
                except ImportError as e:
                    logger.error(f"âŒ æ— æ³•å¯¼å…¥sentence_transformers: {e}")
                    return False
                    
                except Exception as e:
                    logger.warning(f"åœ¨çº¿æ¨¡å‹ {model_name} ç¬¬{retry + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                    if "Connection" in str(e) or "timeout" in str(e).lower():
                        logger.debug("æ£€æµ‹åˆ°ç½‘ç»œé—®é¢˜ï¼Œç»§ç»­é‡è¯•...")
                        continue
                    else:
                        logger.debug("éç½‘ç»œé—®é¢˜ï¼Œè·³è¿‡é‡è¯•")
                        break
        
        # æ‰€æœ‰åœ¨çº¿é€‰é¡¹éƒ½å¤±è´¥ï¼Œä½¿ç”¨çº¯å…³é”®è¯æ¨¡å¼
        logger.error("âŒ æ‰€æœ‰åœ¨çº¿æ¨¡å‹éƒ½æ— æ³•åŠ è½½")
        logger.info("ğŸ”„ åˆ‡æ¢åˆ°çº¯å…³é”®è¯åˆ†ç±»æ¨¡å¼ï¼ˆæ— éœ€embeddingï¼‰")
        self.embedding_model = None
        self.embedding_util = None
        return False

# ç¼“å­˜çš„æ˜ å°„å‡½æ•°
@st.cache_data(ttl=3600, show_spinner=False)
def get_cached_mapping_results(video_pool_path: str) -> tuple:
    """
    ç¼“å­˜çš„æ˜ å°„ç»“æœè·å–å‡½æ•°
    
    Args:
        video_pool_path: video_poolç›®å½•è·¯å¾„
        
    Returns:
        tuple: (mapped_segments, statistics)
    """
    mapper = VideoSegmentMapper()
    mapped_segments = mapper.scan_video_pool(video_pool_path)
    statistics = mapper.get_mapping_statistics(mapped_segments)
    return mapped_segments, statistics 