"""
è§†é¢‘ç‰‡æ®µæ˜ å°„æ¨¡å— - çº¯AIåˆ†ç±»ç‰ˆæœ¬
ä½¿ç”¨DeepSeek AIè¿›è¡Œæ™ºèƒ½åˆ†ç±»ï¼Œç§»é™¤æ‰€æœ‰å…³é”®è¯åŒ¹é…æœºåˆ¶
"""

import logging
import json
import os
import glob
import subprocess
from typing import Dict, List, Optional, Any
from pathlib import Path
import streamlit as st

from modules.selection_logger import get_selection_logger
from utils.path_utils import get_project_root
from modules.ai_analyzers import DeepSeekAnalyzer

logger = logging.getLogger(__name__)

def resolve_video_pool_path(relative_path: str = "data/output/google_video/video_pool") -> str:
    """è§£ævideo_poolçš„ç»å¯¹è·¯å¾„"""
    try:
        project_root = get_project_root()
        resolved_path = os.path.join(project_root, relative_path)
        return resolved_path
    except Exception as e:
        logger.warning(f"è§£æè·¯å¾„å¤±è´¥ï¼Œä½¿ç”¨åŸè·¯å¾„: {e}")
        return relative_path

class VideoSegmentMapper:
    """ğŸ¯ è§†é¢‘ç‰‡æ®µæ˜ å°„å™¨ï¼Œä½¿ç”¨DeepSeek AIè¿›è¡Œæ™ºèƒ½åˆ†ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ˜ å°„å™¨"""
        self.four_modules = ["ç—›ç‚¹", "è§£å†³æ–¹æ¡ˆå¯¼å…¥", "å–ç‚¹Â·æˆåˆ†&é…æ–¹", "ä¿ƒé”€æœºåˆ¶"]
        
        # åˆå§‹åŒ–DeepSeekåˆ†æå™¨
        try:
            self.deepseek_analyzer = DeepSeekAnalyzer()
            logger.info("DeepSeekåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"DeepSeekåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.deepseek_analyzer = None
    
    def get_video_duration_ffprobe(self, file_path: str) -> float:
        """ğŸ¯ ä½¿ç”¨ffprobeè·å–è§†é¢‘æ—¶é•¿"""
        try:
            cmd = ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', file_path]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10, check=True)
            data = json.loads(result.stdout)
            return float(data['format']['duration'])
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘æ—¶é•¿å¤±è´¥: {str(e)}")
            return 0.0
    
    def classify_segment_by_deepseek_ai(self, all_tags: List[str], segment_info: Optional[Dict[str, Any]] = None) -> str:
        """ğŸ¯ ä½¿ç”¨DeepSeek AIè¿›è¡Œè§†é¢‘ç‰‡æ®µæ™ºèƒ½åˆ†ç±»"""
        if not self.deepseek_analyzer or not self.deepseek_analyzer.is_available():
            logger.warning("DeepSeekåˆ†æå™¨ä¸å¯ç”¨")
            return "å…¶ä»–"
        
        if not all_tags:
            return "å…¶ä»–"
        
        tags_text = " ".join(all_tags)
        logger.info(f"ğŸ¯ ä½¿ç”¨DeepSeek AIæ™ºèƒ½åˆ†ç±»: {tags_text}")
        
        try:
            system_prompt = self._build_ai_classification_prompt()
            
            # å‡†å¤‡ç”¨æˆ·è¾“å…¥å†…å®¹
            user_content_parts = [f"è§†é¢‘ç‰‡æ®µæ ‡ç­¾: {', '.join(all_tags)}"]
            
            if segment_info:
                transcription = segment_info.get('transcription')
                if transcription and transcription.strip():
                    user_content_parts.append(f"è¯­éŸ³è½¬å½•å†…å®¹: {transcription.strip()}")
                
                duration = segment_info.get('duration')
                if duration:
                    user_content_parts.append(f"ç‰‡æ®µæ—¶é•¿: {duration:.1f}ç§’")
            
            user_content = "\n".join(user_content_parts)
            user_content += "\n\nè¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯è¿›è¡Œæ¨¡å—åˆ†ç±»ï¼Œåªå›ç­”æ¨¡å—åç§°ã€‚"
            
            response = self.deepseek_analyzer._chat_completion([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ])
            
            if response and "choices" in response and len(response["choices"]) > 0:
                content = response["choices"][0]["message"]["content"].strip()
                cleaned_result = self._extract_module_from_ai_response(content)
                
                if cleaned_result in self.four_modules:
                    logger.info(f"âœ… DeepSeek AIåˆ†ç±»æˆåŠŸ: {tags_text} -> {cleaned_result}")
                    return cleaned_result
                else:
                    logger.warning(f"âš ï¸ DeepSeekè¿”å›æ— æ•ˆåˆ†ç±»: {content}")
                    return "å…¶ä»–"
            else:
                logger.warning("âš ï¸ DeepSeek APIå“åº”æ— æ•ˆ")
                return "å…¶ä»–"
                
        except Exception as e:
            logger.error(f"âŒ DeepSeek AIåˆ†ç±»å¤±è´¥: {e}")
            return "å…¶ä»–"
    
    def _build_ai_classification_prompt(self) -> str:
        """æ„å»ºAIåˆ†ç±»æç¤ºè¯"""
        return """ğŸ¯ ä½ æ˜¯ä¸“ä¸šçš„æ¯å©´è§†é¢‘å†…å®¹åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®è§†é¢‘æ ‡ç­¾å°†å†…å®¹åˆ†ç±»åˆ°ä»¥ä¸‹å››ä¸ªä¸šåŠ¡æ¨¡å—ä¹‹ä¸€ï¼š

## ğŸ¯ ç—›ç‚¹
- **æ ¸å¿ƒç‰¹å¾**: è¯†åˆ«å®å®æˆ–å¦ˆå¦ˆé‡åˆ°çš„é—®é¢˜ã€å›°æ‰°ã€ä¸é€‚çŠ¶å†µ
- **å…¸å‹æ ‡ç­¾**: å“­é—¹ã€ç”Ÿç—…ã€ä¸é€‚ã€ç„¦è™‘ã€ä¾¿ç§˜ã€è…¹æ³»ã€è¿‡æ•ã€åå¥¶ã€æ‹’é£Ÿã€ç¡çœ å›°éš¾
- **æƒ…ç»ªç‰¹å¾**: è´Ÿé¢æƒ…ç»ªå ä¸»å¯¼ï¼Œå¦‚æ‚²ä¼¤ã€ç”Ÿæ°”ã€ç„¦è™‘
- **åœºæ™¯ç‰¹å¾**: åŒ»é™¢ã€è¯Šæ‰€ã€é—®é¢˜å¤„ç†åœºæ™¯
- **è¯†åˆ«é‡ç‚¹**: å¼ºè°ƒé—®é¢˜å­˜åœ¨ï¼Œä½“ç°éœ€æ±‚ç—›ç‚¹

## ğŸ¯ è§£å†³æ–¹æ¡ˆå¯¼å…¥
- **æ ¸å¿ƒç‰¹å¾**: å±•ç¤ºå…·ä½“çš„è§£å†³æ–¹æ¡ˆã€æ“ä½œè¿‡ç¨‹ã€ä½¿ç”¨æ–¹æ³•
- **å…¸å‹æ ‡ç­¾**: å†²å¥¶ã€å–‚å…»ã€å¥¶ç“¶ã€å‡†å¤‡ã€æ“ä½œã€ä½¿ç”¨ã€æ‰‹æœºã€å­¦ä¹ 
- **æƒ…ç»ªç‰¹å¾**: ä¸­æ€§æˆ–è½»å¾®æ­£é¢ï¼Œä¸“æ³¨äºè¿‡ç¨‹
- **åœºæ™¯ç‰¹å¾**: å¨æˆ¿ã€æ“ä½œå°ã€å­¦ä¹ ç¯å¢ƒã€å®æ“åœºæ™¯
- **è¯†åˆ«é‡ç‚¹**: å¼ºè°ƒè¡ŒåŠ¨å’Œè§£å†³è¿‡ç¨‹ï¼Œå±•ç¤ºå¦‚ä½•è§£å†³é—®é¢˜

## ğŸ¯ å–ç‚¹Â·æˆåˆ†&é…æ–¹
- **æ ¸å¿ƒç‰¹å¾**: çªå‡ºäº§å“ç‰¹ç‚¹ã€è¥å…»æˆåˆ†ã€ç§‘å­¦é…æ–¹ã€ä¸“ä¸šä¼˜åŠ¿
- **å…¸å‹æ ‡ç­¾**: A2å¥¶æºã€DHAã€HMOã€æˆåˆ†ã€é…æ–¹ã€è¥å…»ã€ç§‘æŠ€ã€ä¸“åˆ©ã€ä¸“ä¸šã€æƒå¨
- **æƒ…ç»ªç‰¹å¾**: ä¸“ä¸šã€ç§‘å­¦ã€ä¿¡èµ–æ„Ÿ
- **åœºæ™¯ç‰¹å¾**: å®éªŒå®¤ã€ä¸“ä¸šèƒŒæ™¯ã€äº§å“å±•ç¤ºã€ç§‘ç ”ç¯å¢ƒ
- **è¯†åˆ«é‡ç‚¹**: å¼ºè°ƒäº§å“ä¼˜åŠ¿å’Œç§‘å­¦ä»·å€¼ï¼Œä½“ç°ä¸“ä¸šæ€§

## ğŸ¯ ä¿ƒé”€æœºåˆ¶
- **æ ¸å¿ƒç‰¹å¾**: å±•ç¤ºäº§å“æ•ˆæœã€å®å®å¥åº·æˆé•¿ã€æ¨å¹¿æ¿€åŠ±
- **å…¸å‹æ ‡ç­¾**: å¼€å¿ƒã€å¥åº·ã€æ´»åŠ›ã€å¿«ä¹æˆé•¿ã€æ¨èã€ä¼˜æƒ ã€æ´»åŠ¨ã€ç¬‘å®¹ã€èŒå£®æˆé•¿
- **æƒ…ç»ªç‰¹å¾**: ç§¯ææ­£é¢ï¼Œå……æ»¡æ´»åŠ›
- **åœºæ™¯ç‰¹å¾**: æˆ·å¤–ã€æ¸¸ä¹åœºã€é˜³å…‰ã€æ¸©é¦¨å®¶åº­ã€å¿«ä¹æ—¶å…‰
- **è¯†åˆ«é‡ç‚¹**: å¼ºè°ƒä½¿ç”¨æ•ˆæœå’Œæ¨å¹¿ä»·å€¼ï¼Œæ¿€å‘è´­ä¹°æ¬²æœ›

âš ï¸ **é‡è¦è¯´æ˜**:
- åªå›ç­”å››ä¸ªæ¨¡å—åç§°ä¹‹ä¸€ï¼šç—›ç‚¹ã€è§£å†³æ–¹æ¡ˆå¯¼å…¥ã€å–ç‚¹Â·æˆåˆ†&é…æ–¹ã€ä¿ƒé”€æœºåˆ¶
- å¦‚æœæ— æ³•ç¡®å®šï¼Œå›ç­”'å…¶ä»–'
- ä¸è¦æä¾›è§£é‡Šï¼Œåªè¦æ¨¡å—åç§°
- ä¼˜å…ˆè€ƒè™‘æ ‡ç­¾çš„ä¸»è¦å€¾å‘å’Œæ•´ä½“è¯­ä¹‰"""
    
    def _extract_module_from_ai_response(self, ai_response: str) -> str:
        """ä»AIå“åº”ä¸­æå–æ¨¡å—åç§°"""
        if not ai_response:
            return "å…¶ä»–"
        
        cleaned = ai_response.strip()
        
        # ç›´æ¥åŒ¹é…æ¨¡å—åç§°
        for module in self.four_modules:
            if module in cleaned:
                return module
        
        # åŒ¹é…ç®€å†™
        mapping = {
            "ç—›ç‚¹": "ç—›ç‚¹",
            "è§£å†³æ–¹æ¡ˆ": "è§£å†³æ–¹æ¡ˆå¯¼å…¥",
            "å–ç‚¹": "å–ç‚¹Â·æˆåˆ†&é…æ–¹", 
            "ä¿ƒé”€": "ä¿ƒé”€æœºåˆ¶"
        }
        
        for key, value in mapping.items():
            if key in cleaned:
                return value
        
        return "å…¶ä»–"
    
    def classify_segment(self, all_tags: List[str], segment_info: Optional[Dict[str, Any]] = None) -> str:
        """ğŸ¯ å¯¹ç‰‡æ®µè¿›è¡ŒAIåˆ†ç±»"""
        selection_logger = get_selection_logger()
        
        # ä½¿ç”¨DeepSeek AIè¿›è¡Œåˆ†ç±»
        category = self.classify_segment_by_deepseek_ai(all_tags, segment_info)
        
        log_reason = "AIåˆ†ç±»æˆåŠŸ" if category != "å…¶ä»–" else "AIåˆ†ç±»æ— æ³•ç¡®å®šç±»åˆ«"
        
        selection_logger.log_step(
            step_type="ai_classification",
            input_tags=all_tags,
            result=category
        )
        
        selection_logger.log_final_result(
            final_category=category,
            reason=log_reason,
            segment_info=segment_info
        )
        
        return category
    
    def scan_video_pool(self, video_pool_path: str = "data/output/google_video/video_pool") -> List[Dict[str, Any]]:
        """ğŸ¯ ä½¿ç”¨AIåˆ†ç±»æ‰«ævideo_poolç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶"""
        mapped_segments = []
        seen_segment_ids = set()
        
        resolved_path = resolve_video_pool_path(video_pool_path)
        logger.info(f"ğŸ¯ ä½¿ç”¨AIæ™ºèƒ½åˆ†ç±»æ‰«æ: {resolved_path}")
        
        if not os.path.exists(resolved_path):
            logger.error(f"video_poolç›®å½•ä¸å­˜åœ¨: {resolved_path}")
            return mapped_segments
        
        json_files = glob.glob(os.path.join(resolved_path, "*.json"))
        logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        for file_idx, json_file in enumerate(json_files):
            try:
                logger.info(f"å¤„ç†æ–‡ä»¶ {file_idx + 1}/{len(json_files)}: {os.path.basename(json_file)}")
                
                with open(json_file, 'r', encoding='utf-8') as f:
                    video_data = json.load(f)
                
                video_id = video_data.get('video_id', 'unknown')
                segments = video_data.get('segments', [])
                
                for seg_idx, segment in enumerate(segments):
                    try:
                        # æå–åŸºæœ¬ä¿¡æ¯
                        file_path = segment.get('file_path', '')
                        all_tags = segment.get('all_tags', [])
                        quality_score = segment.get('quality_score', 0.9)
                        confidence = segment.get('confidence', 0.8)
                        file_name = segment.get('file_name', '')
                        analysis_method = segment.get('analysis_method', 'visual')
                        transcription = segment.get('transcription', None)
                        
                        # å…¼å®¹æ—§æ ¼å¼
                        if not all_tags:
                            raw_fields = [
                                segment.get('object', ''),
                                segment.get('scene', ''),
                                segment.get('emotion', ''),
                                segment.get('brand_elements', '')
                            ]
                            
                            all_tags = []
                            for raw_field in raw_fields:
                                if raw_field:
                                    if ',' in raw_field:
                                        tags = raw_field.split(',')
                                    else:
                                        tags = [raw_field]
                                
                                    for tag in tags:
                                        clean_tag = tag.strip()
                                        if clean_tag and clean_tag not in all_tags:
                                            all_tags.append(clean_tag)
                            
                        if not all_tags:
                            continue
                        
                        # è·³è¿‡äººè„¸ç‰¹å†™å’Œä¸å¯ç”¨ç‰‡æ®µ
                        if segment.get('is_face_close_up', False) or segment.get('unusable', False):
                            continue
                        
                        # è·å–è§†é¢‘æ—¶é•¿
                        duration = 0
                        if file_path and os.path.exists(file_path):
                                duration = self.get_video_duration_ffprobe(file_path)
                        
                        # æ—¶é•¿è¿‡æ»¤
                        if duration > 10:
                            continue
                        
                        # æ„å»ºç‰‡æ®µä¿¡æ¯
                        segment_info_for_classification = {
                            "file_name": file_name,
                            "duration": duration,
                            "all_tags": all_tags,
                            "transcription": transcription,
                            "analysis_method": analysis_method
                        }
                        
                        # AIåˆ†ç±»
                        category = self.classify_segment(all_tags, segment_info_for_classification)
                        
                        # å»é‡æ£€æŸ¥
                        unique_id = f"{video_id}::{file_name}"
                        if unique_id in seen_segment_ids:
                            continue
                        
                        # æ„å»ºæ˜ å°„ç»“æœ
                        mapped_segment = {
                            "segment_id": f"{video_id}_{file_name}",
                            "file_path": file_path,
                            "category": category,
                            "all_tags": all_tags,
                            "quality_score": quality_score,
                            "confidence": confidence,
                            "duration": duration,
                            "transcription": transcription,
                            "analysis_method": analysis_method
                        }
                        
                        mapped_segments.append(mapped_segment)
                        seen_segment_ids.add(unique_id)
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†ç‰‡æ®µå¤±è´¥: {str(e)}")
                        continue
                        
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
                continue
        
        return mapped_segments
    
    def get_mapping_statistics(self, mapped_segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è·å–æ˜ å°„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_segments": len(mapped_segments),
            "by_category": {},
            "by_video": {},
            "quality_stats": {"avg_quality": 0, "avg_confidence": 0, "avg_duration": 0, "total_duration": 0}
        }
        
        if not mapped_segments:
            return stats
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        for module in self.four_modules + ["å…¶ä»–"]:
            module_segments = [s for s in mapped_segments if s["category"] == module]
            stats["by_category"][module] = {
                "count": len(module_segments),
                "total_duration": sum(s["duration"] for s in module_segments),
                "avg_quality": sum(s["combined_quality"] for s in module_segments) / len(module_segments) if module_segments else 0
            }
        
        # æŒ‰è§†é¢‘ç»Ÿè®¡
        video_counts = {}
        for segment in mapped_segments:
            video_id = segment["video_id"]
            if video_id not in video_counts:
                video_counts[video_id] = 0
            video_counts[video_id] += 1
        stats["by_video"] = video_counts
        
        # è´¨é‡ç»Ÿè®¡
        if mapped_segments:
            stats["quality_stats"] = {
                "avg_quality": sum(s["combined_quality"] for s in mapped_segments) / len(mapped_segments),
                "avg_confidence": sum(s["confidence"] for s in mapped_segments) / len(mapped_segments),
                "avg_duration": sum(s["duration"] for s in mapped_segments) / len(mapped_segments),
                "total_duration": sum(s["duration"] for s in mapped_segments)
            }
        
        return stats

# ç¼“å­˜çš„æ˜ å°„å‡½æ•°
@st.cache_data(ttl=3600, show_spinner=False)
def get_cached_mapping_results(video_pool_path: str) -> tuple:
    """ç¼“å­˜çš„æ˜ å°„ç»“æœè·å–å‡½æ•°"""
    mapper = VideoSegmentMapper()
    mapped_segments = mapper.scan_video_pool(video_pool_path)
    statistics = mapper.get_mapping_statistics(mapped_segments)
    return mapped_segments, statistics 