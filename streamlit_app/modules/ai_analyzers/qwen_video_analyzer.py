"""
åƒé—®2.5è§†è§‰åˆ†æå™¨

ä¸“é—¨å¤„ç†åƒé—®2.5å¤šæ¨¡æ€è§†é¢‘åˆ†æåŠŸèƒ½çš„æ¨¡å—ï¼ŒåŒ…å«è¯­éŸ³è½¬å½•ä¿åº•æœºåˆ¶
åº”ç”¨äº†qwen_optimization_guide.mdä¸­çš„å…¨éƒ¨ä¼˜åŒ–ç­–ç•¥
"""

import os
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from collections import Counter
from http import HTTPStatus
import re

# å¯¼å…¥éŸ³é¢‘åˆ†æå™¨
from .dashscope_audio_analyzer import DashScopeAudioAnalyzer
from .deepseek_analyzer import DeepSeekAnalyzer

logger = logging.getLogger(__name__)


class QwenVideoAnalyzer:
    """åƒé—®2.5è§†è§‰åˆ†æå™¨ï¼ˆåŒ…å«è¯­éŸ³è½¬å½•ä¿åº•æœºåˆ¶ + å…¨é¢ä¼˜åŒ–ç­–ç•¥ï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–åƒé—®2.5åˆ†æå™¨
        
        Args:
            api_key: DashScope APIå¯†é’¥
        """
        self.api_key = api_key or os.environ.get("DASHSCOPE_API_KEY")
        self.analyzer = None
        
        # åˆå§‹åŒ–éŸ³é¢‘åˆ†æå™¨ - è¯­éŸ³è½¬å½•ä¿åº•æœºåˆ¶
        self.audio_analyzer = DashScopeAudioAnalyzer(api_key=self.api_key)
        
        # åˆå§‹åŒ–DeepSeekåˆ†æå™¨ - æ–‡æœ¬åˆ†æ
        self.deepseek_analyzer = DeepSeekAnalyzer()
        
        if not self.api_key:
            logger.warning("æœªè®¾ç½®DASHSCOPE_API_KEYï¼Œåƒé—®2.5åˆ†æå™¨ä¸å¯ç”¨")
        else:
            self._initialize_analyzer()
    
    def _initialize_analyzer(self):
        """åˆå§‹åŒ–åƒé—®2.5åˆ†æå™¨"""
        try:
            import dashscope
            dashscope.api_key = self.api_key
            self.analyzer = True
            logger.info("åƒé—®2.5è§†è§‰åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            logger.error(f"æ— æ³•å¯¼å…¥DashScope: {str(e)}")
            self.analyzer = None
        except Exception as e:
            logger.error(f"åƒé—®2.5åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.analyzer = None
            
        # ğŸ”§ ä¼˜åŒ–çš„æ¨¡å‹å‚æ•°é…ç½®
        self.model_config = {
            'model': 'qwen-vl-plus-latest',  # ä½¿ç”¨æœ€æ–°æ¨¡å‹
            'temperature': 0.1,              # é™ä½éšæœºæ€§
            'top_p': 0.8,                   # æ§åˆ¶ç”Ÿæˆè´¨é‡
            'max_tokens': 1500,             # å¢åŠ è¾“å‡ºé•¿åº¦
            'seed': 1234                    # ç¡®ä¿å¯é‡å¤æ€§
        }
        
        # ğŸ”§ è´¨é‡æ§åˆ¶å‚æ•°
        self.quality_config = {
            'min_quality_threshold': 0.6,   # è´¨é‡åˆ†é˜ˆå€¼
            'max_retry_count': 1,            # æœ€å¤§é‡è¯•æ¬¡æ•° (ä»2æ”¹ä¸º0ä»¥æé«˜é€Ÿåº¦)
            'confidence_threshold': 0.6      # ç½®ä¿¡åº¦é˜ˆå€¼
        }
        
        logger.info(f"åƒé—®2.5åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {self.model_config['model']}")
    
    def is_available(self) -> bool:
        """æ£€æŸ¥åˆ†æå™¨æ˜¯å¦å¯ç”¨"""
        return self.analyzer is not None and self.api_key is not None
    
    def analyze_video_segment(
        self,
        video_path: str,
        tag_language: str = "ä¸­æ–‡",
        frame_rate: float = 2.0
    ) -> Dict[str, Any]:
        """
        è§†é¢‘ç‰‡æ®µåˆ†æï¼ˆåŒ…å«è¯­éŸ³è½¬å½•ä¿åº•æœºåˆ¶ + å…¨é¢ä¼˜åŒ–ç­–ç•¥ï¼‰
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            tag_language: æ ‡ç­¾è¯­è¨€
            frame_rate: å¸§ç‡
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info(f"ğŸ¯ å¼€å§‹åˆ†æè§†é¢‘: {video_path}")

        if not self.is_available():
            return self._get_default_result("åƒé—®2.5åˆ†æå™¨ä¸å¯ç”¨")
        
        if not os.path.exists(video_path):
            return self._get_default_result(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        try:
            # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šå°è¯•è§†è§‰åˆ†æï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            logger.info("ğŸ¯ å¼€å§‹è§†è§‰åˆ†æ...")
            visual_result = self._analyze_with_retry(video_path, frame_rate, tag_language)
            
            if visual_result and visual_result.get('success'):
                # ğŸ” æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨è¯­éŸ³è½¬å½•ä¿åº•æœºåˆ¶
                if self._needs_audio_fallback(visual_result):
                    logger.warning("ğŸ¤ è§†è§‰åˆ†æå­˜åœ¨æœªè¯†åˆ«å†…å®¹ï¼Œå¯ç”¨DeepSeekéŸ³é¢‘è½¬å½•å…œåº•åˆ†æ...")
                    
                    # ğŸ¯ ä½¿ç”¨é’ˆå¯¹æ€§åˆ†æè€Œä¸æ˜¯å®Œæ•´é‡æ–°åˆ†æ
                    enhanced_result = self._targeted_audio_fallback_analysis(video_path, visual_result, tag_language)
                    
                    if enhanced_result.get("success"):
                        logger.info("ğŸ¯ é’ˆå¯¹æ€§DeepSeekè¯­éŸ³å…œåº•åˆ†æå®Œæˆ")
                        return enhanced_result
                    else:
                        logger.warning("ğŸ¯ é’ˆå¯¹æ€§åˆ†æå¤±è´¥ï¼Œè¿”å›åŸå§‹visualç»“æœ")
                        return visual_result
                
                return visual_result
            else:
                # ğŸ¤ è§†è§‰åˆ†æå¤±è´¥ï¼Œç›´æ¥å¯ç”¨è¯­éŸ³è½¬å½•ä¿åº•
                logger.warning("ğŸ¤ Qwenè§†è§‰åˆ†æå¤±è´¥ï¼Œå¯ç”¨DeepSeekéŸ³é¢‘è½¬å½•å…œåº•åˆ†æ...")
                return self._audio_fallback_analysis(video_path, tag_language)
                
        except Exception as e:
            logger.error(f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            # ğŸ¤ å¼‚å¸¸æƒ…å†µä¸‹å°è¯•è¯­éŸ³è½¬å½•ä¿åº•
            try:
                return self._audio_fallback_analysis(video_path, tag_language)
            except Exception as fallback_error:
                logger.error(f"è¯­éŸ³è½¬å½•ä¿åº•ä¹Ÿå¤±è´¥: {str(fallback_error)}")
                return self._get_default_result(f"åˆ†æå¤±è´¥: {str(e)}")
    
    def _analyze_with_retry(self, video_path: str, frame_rate: float, tag_language: str) -> Dict[str, Any]:
        """
        ğŸ”§ å¸¦é‡è¯•æœºåˆ¶çš„è§†è§‰åˆ†æ
        """
        max_retry = self.quality_config['max_retry_count']
        
        for attempt in range(max_retry + 1):
            try:
                # é€‰æ‹©æç¤ºè¯ï¼ˆé‡è¯•æ—¶ä½¿ç”¨å¢å¼ºç‰ˆï¼‰
                if attempt == 0:
                    prompt = self._build_professional_prompt(tag_language)
                else:
                    prompt = self._build_enhanced_retry_prompt(tag_language)
                    logger.info(f"ğŸ”„ ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œä½¿ç”¨å¢å¼ºæç¤ºè¯")
                
                # æ‰§è¡Œåˆ†æ
                visual_result = self._analyze_video_file(video_path, frame_rate, prompt)
                
                if visual_result and 'analysis' in visual_result:
                    # è§£æç»“æœ
                    analysis_result = self._parse_analysis_result(
                        visual_result['analysis'], tag_language
                    )
                    analysis_result["success"] = True
                    analysis_result["quality_score"] = visual_result.get('quality_score', 0.8)
                    analysis_result["analysis_method"] = "visual"
                    analysis_result["retry_count"] = attempt
                    
                    # ğŸ¯ NEW: æ£€æµ‹äººè„¸ç‰¹å†™
                    face_close_up_detected = self._detect_face_close_up(analysis_result, video_path)
                    if face_close_up_detected:
                        logger.warning(f"ğŸš« æ£€æµ‹åˆ°äººè„¸ç‰¹å†™ç‰‡æ®µï¼Œæ ‡è®°ä¸ºä¸å¯ç”¨: {video_path}")
                        analysis_result["is_face_close_up"] = True
                        analysis_result["unusable"] = True
                        analysis_result["unusable_reason"] = "äººè„¸ç‰¹å†™ç‰‡æ®µ"
                        # é™ä½è´¨é‡åˆ†ï¼Œç¡®ä¿åœ¨åŒ¹é…æ—¶è¢«è¿‡æ»¤
                        analysis_result["quality_score"] = 0.1
                    else:
                        analysis_result["is_face_close_up"] = False
                        analysis_result["unusable"] = False
                    
                    # æ£€æŸ¥è´¨é‡
                    if analysis_result["quality_score"] >= self.quality_config['min_quality_threshold']:
                        logger.info(f"âœ… åˆ†ææˆåŠŸï¼Œè´¨é‡åˆ†: {analysis_result['quality_score']:.2f}")
                        return analysis_result
                    elif attempt < max_retry:
                        logger.warning(f"âš ï¸ è´¨é‡åˆ†è¿‡ä½ ({analysis_result['quality_score']:.2f})ï¼Œå‡†å¤‡é‡è¯•...")
                        continue
                    else:
                        # æœ€åä¸€æ¬¡é‡è¯•ï¼Œè¿›è¡Œåå¤„ç†ä¼˜åŒ–
                        analysis_result = self._enhance_poor_result(analysis_result, video_path)
                        logger.info(f"ğŸ”§ åº”ç”¨åå¤„ç†ä¼˜åŒ–ï¼Œæœ€ç»ˆè´¨é‡åˆ†: {analysis_result['quality_score']:.2f}")
                        return analysis_result
                else:
                    if attempt < max_retry:
                        logger.warning(f"âš ï¸ åˆ†æè¿”å›ç©ºç»“æœï¼Œå‡†å¤‡é‡è¯•...")
                        continue
                    else:
                        return self._get_default_result("é‡è¯•åä»æ— æ³•è·å–åˆ†æç»“æœ")
                        
            except Exception as e:
                if attempt < max_retry:
                    logger.warning(f"âš ï¸ åˆ†æå¼‚å¸¸: {str(e)}ï¼Œå‡†å¤‡é‡è¯•...")
                    continue
                else:
                    logger.error(f"âŒ é‡è¯•{max_retry}æ¬¡åä»å¤±è´¥: {str(e)}")
                    return self._get_default_result(f"é‡è¯•å¤±è´¥: {str(e)}")
        
        return self._get_default_result("é‡è¯•æœºåˆ¶å¼‚å¸¸")
    
    def _build_professional_prompt(self, tag_language: str) -> str:
        """
        ğŸ”§ æ„å»ºåŸºäºé…ç½®æ–‡ä»¶çš„è§†è§‰åˆ†ææç¤ºè¯
        """
        try:
            from streamlit_app.utils.keyword_config import sync_prompt_templates
            templates = sync_prompt_templates()
            return templates.get("qwen_visual", "")
        except Exception as e:
            logger.warning(f"æ— æ³•å¯¼å…¥ç»Ÿä¸€promptæ¨¡æ¿ï¼Œä½¿ç”¨å…œåº•é…ç½®: {e}")
            return self._get_fallback_visual_prompt()
    
    def _build_enhanced_retry_prompt(self, tag_language: str) -> str:
        """
        ğŸ”§ æ„å»ºåŸºäºé…ç½®æ–‡ä»¶çš„é‡è¯•æç¤ºè¯
        """
        try:
            from streamlit_app.utils.keyword_config import sync_prompt_templates
            templates = sync_prompt_templates()
            return templates.get("qwen_retry", "")
        except Exception as e:
            logger.warning(f"æ— æ³•å¯¼å…¥ç»Ÿä¸€é‡è¯•promptæ¨¡æ¿ï¼Œä½¿ç”¨å…œåº•é…ç½®: {e}")
            return self._get_fallback_retry_prompt()
    
    def _get_fallback_visual_prompt(self) -> str:
        """å…œåº•è§†è§‰åˆ†æprompt"""
        return """ä½ æ˜¯æ¯å©´äº§å“**è§†è§‰è¯†åˆ«ä¸“å®¶**ï¼Œè¯·**åªçœ‹ç”»é¢**æå–å…³é”®ä¿¡æ¯ã€‚

â€”â€” **å¼ºåˆ¶è¯†åˆ«å­—æ®µ** â€”â€”
object:        å¥¶ç²‰ç½ã€å¥¶ç“¶ã€å®å®ã€å¦ˆå¦ˆã€å©´å„¿ç”¨å“ã€è¥å…»è¡¨
sence:         å¨æˆ¿ã€å®¢å…ã€åŒ»é™¢ã€ç—…æˆ¿ã€æˆ·å¤–ã€å…¬å›­ã€æ¸¸ä¹åœº
emotion:       [å¿«ä¹ / å…´å¥‹ / æ¸©é¦¨ / ç„¦è™‘ / ç—›è‹¦]  â† åªèƒ½é€‰è¿™ 5 ä¸ª
brand_elements:å¯èµ‹ã€Wyethã€illumaã€A2ã€ATWOã€HMOã€DHA
confidence:    0.0-1.0

â€”â€” **ç—›ç‚¹ä¿¡å·**ï¼ˆè‹¥å‡ºç°è¯·ä¸€å®šå†™åˆ° object æˆ– senceï¼‰ â€”â€”
å®å®å“­ã€è¾“æ¶²ç®¡ã€åŒ»é™¢ã€ç—…åºŠã€å‘çƒ§ã€å¤œé†’ã€çˆ¶æ¯ç„¦è™‘

â€”â€” **æ´»åŠ›ä¿¡å·**ï¼ˆåˆ¤ä¿ƒé”€ç»“å°¾ç”¨ï¼‰ â€”â€”
å®å®å¥”è·‘ã€è·³è·ƒã€æ»‘æ¢¯ã€è¹¦åºŠã€æˆ·å¤–ç©è€ã€å…¬å›­å¬‰æˆ

â€”â€” **è¾“å‡ºè¦æ±‚** â€”â€”
å¿…é¡»ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯è¡Œä¸€ä¸ªå­—æ®µï¼š
object: [è¯†åˆ«åˆ°çš„ç‰©ä½“ï¼Œé€—å·åˆ†éš”]
sence: [è¯†åˆ«åˆ°çš„åœºæ™¯ï¼Œé€—å·åˆ†éš”]
emotion: [è¯†åˆ«åˆ°çš„æƒ…ç»ªï¼Œåªèƒ½ä»5ä¸ªé€‰é¡¹ä¸­é€‰]
brand_elements: [è¯†åˆ«åˆ°çš„å“ç‰Œï¼Œé€—å·åˆ†éš”]
confidence: [ç½®ä¿¡åº¦0.0-1.0]

æ³¨æ„ï¼š
1. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜æ–‡å­—
2. çœ‹ä¸æ¸…çš„å­—æ®µç•™ç©ºï¼Œä½†ä¿ç•™å­—æ®µå
3. æ¯ä¸ªå­—æ®µéƒ½å¿…é¡»å­˜åœ¨

è¯·å¼€å§‹åˆ†æç”»é¢ï¼š"""
    
    def _get_fallback_retry_prompt(self) -> str:
        """å…œåº•é‡è¯•prompt"""
        return """ä½ æ˜¯æ¯å©´è§†è§‰ä¸“å®¶ï¼Œé‡æ–°**æ·±åº¦æ”¾å¤§**ç”»é¢ï¼Œè¡¥æŠ“é—æ¼ä¿¡æ¯ã€‚

â€”â€” **å…³é”®è¡¥æŠ“** â€”â€”
â€¢ pain_signals: å®å®å“­ã€è¾“æ¶²ç®¡ã€åŒ»é™¢ã€å‘çƒ§ã€å¤œé†’  
â€¢ vitality_signals: è·‘ã€è·³ã€æ»‘æ¢¯ã€è¹¦åºŠã€æˆ·å¤–ã€å…¬å›­  
â€¢ brand logo / è¥å…»æˆåˆ†è¡¨ / åˆ†å­ç»“æ„

â€”â€” **è¾“å‡ºè¦æ±‚** â€”â€”
å¿…é¡»ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯è¡Œä¸€ä¸ªå­—æ®µï¼š
object: [è¯†åˆ«åˆ°çš„ç‰©ä½“ï¼Œé€—å·åˆ†éš”]
sence: [è¯†åˆ«åˆ°çš„åœºæ™¯ï¼Œé€—å·åˆ†éš”]
emotion: [å¿«ä¹/å…´å¥‹/æ¸©é¦¨/ç„¦è™‘/ç—›è‹¦ä¸­é€‰ä¸€ä¸ª]
brand_elements: [è¯†åˆ«åˆ°çš„å“ç‰Œï¼Œé€—å·åˆ†éš”]
confidence: [ç½®ä¿¡åº¦0.0-1.0]

æ³¨æ„ï¼šçœ‹ä¸æ¸…çš„å­—æ®µç•™ç©ºï¼Œä½†ä¿ç•™å­—æ®µåã€‚

è¯·å†æ¬¡ç²¾å‡†è¯†åˆ«ï¼š"""
    
    def _get_fallback_audio_prompt(self, transcription: str) -> str:
        """å…œåº•éŸ³é¢‘åˆ†æprompt"""
        return f"""åˆ†ææ¯å©´çŸ­ç‰‡è¯­éŸ³è½¬å½•ï¼Œæå–äº§å“ä¸åœºæ™¯å…³é”®è¯ã€‚
è¯­éŸ³å†…å®¹:
{transcription}

å¿…æŠ“ä¿¡æ¯:
object:        å¥¶ç²‰ã€å¥¶ç“¶ã€å®å®ã€å¦ˆå¦ˆã€åŒ»é™¢ã€æ¸¸ä¹åœº
sence:         å†²å¥¶ã€æŒ‡å¯¼ã€æŠ¤ç†ã€æˆ·å¤–ç©è€ã€åŒ»é™¢åœºæ™¯
emotion:       [å¿«ä¹ / å…´å¥‹ / æ¸©é¦¨ / ç„¦è™‘ / ç—›è‹¦]  (é™é€‰5ä¸ª)
brand_elements:å¯èµ‹ã€Wyethã€illumaã€A2ã€ATWOã€HMOã€DHA
confidence:    0.0-1.0

é‡ç‚¹å…³æ³¨:
- ç—›ç‚¹è¯: å“­é—¹ã€å‘çƒ§ã€æ‹‰è‚šå­ã€å¤œé†’ã€ç”Ÿç—…ã€ç„¦è™‘
- ä¿ƒé”€ä¿¡å·: å®å®å¼€å¿ƒã€å¿«ä¹æˆé•¿ã€æ´»åŠ›æ»¡æ»¡ã€å¥åº·æˆé•¿ã€çˆ±ç¬‘ã€ç²¾ç¥é¥±æ»¡ã€æœºçµå¯çˆ±ã€èªæ˜æ´»æ³¼

è¾“å‡ºè¦æ±‚:
1. å…¨ä¸­æ–‡å°å†™ (å“ç‰Œåä¿ç•™å¤§å°å†™)
2. é€—å·åˆ†éš”å•è¯/çŸ­è¯­ (æ— æ‹¬å·/å¼•å·)
3. confidence < 0.6 ä¸è¾“å‡º
4. ä»…åŸºäºè¯­éŸ³ï¼Œä¸è‡†æµ‹ç”»é¢

æŒ‰æ­¤æ ¼å¼ç”Ÿæˆ: object, sence, emotion, brand_elements, confidence."""
    
    def _get_default_result(self, error_msg: str) -> Dict[str, Any]:
        """è·å–é»˜è®¤é”™è¯¯ç»“æœ"""
        return {
            "success": False,
            "error": error_msg,
            "object": "",
            "sence": "",
            "emotion": "",
            "brand_elements": "",
            "confidence": 0.0,
            "all_tags": []
        }
        
    def _analyze_video_file(self, video_path: str, frame_rate: float, prompt: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘æ–‡ä»¶"""
        try:
            import cv2
            import dashscope
            
            # æå–å¸§
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return None
            
            # è·å–è§†é¢‘æ—¶é•¿
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            # ğŸ”§ æä¿å®ˆçš„å¸§é‡‡æ ·ç­–ç•¥
            frames = self._ultra_conservative_frame_sampling(cap, duration)
            cap.release()
            
            if not frames:
                return None
            
            # è°ƒç”¨åƒé—®API
            messages = [
                {
                    'role': 'user',
                    'content': [
                        {'text': prompt},
                        *[{'image': frame_data} for frame_data in frames]
                    ]
                }
            ]
            
            response = dashscope.MultiModalConversation.call(
                model=self.model_config['model'],
                messages=messages,
                temperature=self.model_config['temperature'],
                top_p=self.model_config['top_p'],
                max_tokens=self.model_config['max_tokens']
            )
            
            if response.status_code == HTTPStatus.OK:
                # ğŸ”§ ä¿®å¤ï¼šå¤„ç†contentå¯èƒ½æ˜¯listçš„æƒ…å†µ
                content = response.output.choices[0].message.content
                if isinstance(content, list):
                    # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼Œæå–textéƒ¨åˆ†
                    result_text = ""
                    for item in content:
                        if isinstance(item, dict) and 'text' in item:
                            result_text += item['text']
                        elif isinstance(item, str):
                            result_text += item
                else:
                    result_text = str(content) if content else ""
                
                logger.info(f"ğŸ” åƒé—®APIè¿”å›å†…å®¹: {result_text[:200]}...")
                quality_score = self._assess_result_quality(result_text)
                return {'analysis': result_text, 'quality_score': quality_score}
            else:
                logger.error(f"åƒé—®APIè°ƒç”¨å¤±è´¥: {response.message}")
                return None
                
        except Exception as e:
            logger.error(f"è§†é¢‘æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}")
            return None
    
    def _ultra_conservative_frame_sampling(self, cap, duration: float) -> List[str]:
        """
        ğŸ”§ æä¿å®ˆçš„å¸§é‡‡æ ·ç­–ç•¥ - ç¬¦åˆAPIæœ€ä¸¥æ ¼è¦æ±‚
        """
        import cv2
        import base64
        from io import BytesIO
        from PIL import Image
        
        frames = []
        
        # ğŸ”§ æä¿å®ˆç­–ç•¥ - åªé‡‡æ ·1å¸§ï¼Œé¿å…å†…å®¹é•¿åº¦è¶…é™
        cap.set(cv2.CAP_PROP_POS_MSEC, duration * 0.5 * 1000)
        ret, frame = cap.read()
        
        if ret:
            try:
                # è½¬æ¢ä¸ºPILå›¾åƒ
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                
                # ğŸ”§ æåº¦å‹ç¼©å›¾åƒ
                processed_image = self._ultra_compress_image(pil_image)
                
                # è½¬æ¢ä¸ºbase64
                buffered = BytesIO()
                processed_image.save(buffered, format='JPEG', quality=60)
                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                frames.append(f"data:image/jpeg;base64,{img_base64}")
                
                logger.info(f"ğŸ”§ æä¿å®ˆå¸§é‡‡æ ·å®Œæˆï¼Œå›¾åƒå°ºå¯¸: {processed_image.size}")
                
            except Exception as e:
                logger.warning(f"å¤„ç†å¸§æ—¶å‡ºé”™: {str(e)}")
        
        return frames
    
    def _ultra_compress_image(self, pil_image):
        """
        ğŸ”§ æåº¦å‹ç¼©å›¾åƒ - ç¬¦åˆAPIæœ€ä¸¥æ ¼è¦æ±‚
        """
        from PIL import Image
        
        # ğŸ”§ æä¿å®ˆçš„å°ºå¯¸ç­–ç•¥ - 224x224 (8x28 = 224)
        target_width = 224   # 28çš„å€æ•°
        target_height = 224  # 28çš„å€æ•°
        
        # å¼ºåˆ¶è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
        compressed_image = pil_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        return compressed_image
    
    def _assess_result_quality(self, result_text: str) -> float:
        """
        ğŸ”§ ä¼˜åŒ–çš„ç»“æœè´¨é‡è¯„ä¼°
        """
        if not result_text or len(result_text.strip()) < 10:
            return 0.0
        
        quality_score = 0.5  # åŸºç¡€åˆ†
        
        # å¿…è¦å­—æ®µå®Œæ•´æ€§ (30%)
        required_fields = ['object:', 'sence:', 'emotion:', 'brand_elements:', 'confidence:']
        field_count = sum(1 for field in required_fields if field in result_text)
        quality_score += (field_count / len(required_fields)) * 0.3
        
        # é¿å…"æ— "å›ç­” (15%)
        if 'æ— ' not in result_text or result_text.count('æ— ') <= 1:
            quality_score += 0.15
        
        # å†…å®¹ä¸°å¯Œåº¦ (5%)
        if len(result_text) > 100:
            quality_score += 0.05
        
        return min(quality_score, 1.0)
    
    def _enhance_poor_result(self, result: Dict[str, Any], video_path: str) -> Dict[str, Any]:
        """
        ğŸ”§ æ™ºèƒ½åå¤„ç†ä¼˜åŒ– - å¢å¼ºä½è´¨é‡ç»“æœ
        """
        logger.info("ğŸ”§ å¼€å§‹åå¤„ç†ä¼˜åŒ–...")
        
        enhanced_result = result.copy()
        video_info = self._extract_video_info(video_path)
        
        # åŸºäºè§†é¢‘ç‰¹å¾æ¨æ–­
        if enhanced_result.get('object') in ['æ— ', 'ç”»é¢ä¸æ¸…æ™°', '']:
            if video_info.get('duration', 0) < 5:
                enhanced_result['object'] = 'ç–‘ä¼¼äº§å“å±•ç¤º'
            else:
                enhanced_result['object'] = 'ç–‘ä¼¼äººç‰©æ´»åŠ¨'
        
        if enhanced_result.get('sence') in ['æ— ', 'ç”»é¢ä¸æ¸…æ™°', '']:
            enhanced_result['sence'] = 'ç–‘ä¼¼å®¤å†…ç¯å¢ƒ'
        
        if enhanced_result.get('emotion') in ['æ— ', 'ç”»é¢ä¸æ¸…æ™°', '']:
            enhanced_result['emotion'] = 'ç–‘ä¼¼æ¸©é¦¨æ°›å›´'
        
        if enhanced_result.get('brand_elements') in ['æ— ', 'ç”»é¢ä¸æ¸…æ™°', '']:
            enhanced_result['brand_elements'] = 'ç–‘ä¼¼å“ç‰Œè¦ç´ '
        
        # é‡å»ºall_tags
        enhanced_result['all_tags'] = self._rebuild_tags(enhanced_result)
        
        # æå‡è´¨é‡åˆ†
        enhanced_result['quality_score'] = min(enhanced_result.get('quality_score', 0.0) + 0.1, 0.7)
        
        logger.info(f"ğŸ”§ åå¤„ç†ä¼˜åŒ–å®Œæˆï¼Œè´¨é‡åˆ†: {enhanced_result['quality_score']:.2f}")
        return enhanced_result
    
    def _extract_video_info(self, video_path: str) -> Dict[str, Any]:
        """æå–è§†é¢‘åŸºæœ¬ä¿¡æ¯"""
        try:
            import cv2
            cap = cv2.VideoCapture(video_path)
            if cap.isOpened():
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                duration = frame_count / fps if fps > 0 else 0
                cap.release()
                return {'duration': duration, 'fps': fps, 'frame_count': frame_count}
        except:
            pass
        return {'duration': 0, 'fps': 0, 'frame_count': 0}
    
    def _rebuild_tags(self, result: Dict[str, Any]) -> List[str]:
        """é‡å»ºall_tagså­—æ®µ"""
        all_tags = []
        for field in ['object', 'sence', 'emotion', 'brand_elements']:
            value = result.get(field, '')
            if value and value not in ['æ— ', 'ç”»é¢ä¸æ¸…æ™°', 'ç–‘ä¼¼å“ç‰Œè¦ç´ ', 'ç–‘ä¼¼å®¤å†…ç¯å¢ƒ', 'ç–‘ä¼¼æ¸©é¦¨æ°›å›´', 'ç–‘ä¼¼äº§å“å±•ç¤º', 'ç–‘ä¼¼äººç‰©æ´»åŠ¨']:
                tags = [tag.strip() for tag in value.split(',') if tag.strip()]
                all_tags.extend(tags)
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_tags = []
        for tag in all_tags:
            tag_clean = tag.replace('ç–‘ä¼¼', '').strip()
            if tag_clean and tag_clean not in unique_tags:
                unique_tags.append(tag_clean)
        
        return unique_tags
    
    def _parse_analysis_result(self, analysis_text, tag_language: str) -> Dict[str, Any]:
        """è§£æåˆ†æç»“æœï¼Œä½¿ç”¨ç®€åŒ–çš„5å­—æ®µæ ¼å¼"""
        
        # ğŸ”§ é‡ç”¨çš„æ ¼å¼æ¸…ç†å‡½æ•°
        def clean_field_value(value: str) -> str:
            """æ¸…ç†å­—æ®µå€¼ï¼Œç¡®ä¿è¾“å‡ºç®€æ´çš„å•è¯çŸ­è¯­"""
            if not value:
                return ''
            
            # åŸºç¡€æ¸…ç†
            cleaned = value.strip()
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šç§»é™¤å­—æ®µæ ‡è¯†ç¬¦å¹²æ‰°
            field_markers = ['object:', 'sence:', 'emotion:', 'brand_elements:', 'confidence:']
            for marker in field_markers:
                if marker in cleaned:
                    # å¦‚æœåœ¨å¼€å¤´ï¼Œç§»é™¤å®ƒ
                    if cleaned.startswith(marker):
                        cleaned = cleaned[len(marker):].strip()
                    else:
                        # å¦‚æœåœ¨ä¸­é—´ï¼Œåªå–å‰é¢éƒ¨åˆ†
                        cleaned = cleaned.split(marker)[0].strip()
            
            # å»é™¤Markdownå’Œç‰¹æ®Šç¬¦å·
            cleaned = cleaned.replace('**', '').replace('*', '')
            cleaned = cleaned.replace('- ', '').replace('+ ', '')
            cleaned = cleaned.replace('# ', '').replace('[', '').replace(']', '')
            cleaned = cleaned.replace('(', '').replace(')', '')
            cleaned = cleaned.replace('"', '').replace("'", '')
            cleaned = cleaned.replace('ï¼š', ':').replace('ã€‚', '').replace('ï¼›', '')
            
            # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            cleaned = re.sub(r'[,ï¼Œ]+', ',', cleaned)
            cleaned = cleaned.strip(',')
            
            # è¿‡æ»¤æ— æ„ä¹‰å†…å®¹
            meaningless = ['æ— ', 'ä¸ç¡®å®š', 'ç”»é¢ä¸æ¸…æ™°', 'è§£æå¤±è´¥', 
                          '', 'none', 'n/a', 'ä¸æ˜ç¡®', 'ç›¸å…³']
            if cleaned.lower() in meaningless:
                return ''
            
            # ğŸ”§ æ–°å¢ï¼šè¿‡æ»¤çº¯æ•°å­—å†…å®¹ï¼ˆconfidenceå€¼è¯¯å…¥å…¶ä»–å­—æ®µï¼‰
            if re.match(r'^[0-9.]+$', cleaned):
                return ''
            
            # ğŸ”§ å¼ºåŒ–ï¼šè¿‡æ»¤åŒ…å«çº¯æ•°å­—çš„ç‰‡æ®µï¼ˆå¦‚"0.5"è¯¯å…¥å“ç‰Œå­—æ®µï¼‰
            if re.match(r'^[0-9]+\.?[0-9]*$', cleaned):
                return ''
            
            # ğŸ”§ æ–°å¢ï¼šè¿‡æ»¤è¿‡çŸ­çš„æ— æ„ä¹‰å†…å®¹ï¼ˆ1-2å­—ç¬¦ï¼‰
            if len(cleaned) <= 2 and cleaned.lower() in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.']:
                return ''
            
            return cleaned
        
        try:
            result = {
                'object': '',
                'sence': '', 
                'emotion': '',
                'brand_elements': '',
                'confidence': 0.8
            }
            
            logger.info(f"ğŸ¯ å¼€å§‹è§£æç®€åŒ–åˆ†ææ–‡æœ¬:")
            logger.info(f"åŸå§‹æ–‡æœ¬: {analysis_text}")
            
            # ğŸ”§ æ”¯æŒä¸¤ç§æ ¼å¼ï¼šæ ‡å‡†å­—æ®µæ ¼å¼ å’Œ ç®€å•æ–‡æœ¬æ ¼å¼
            lines = analysis_text.strip().split('\n')
            has_field_markers = any(':' in line for line in lines)
            
            if has_field_markers:
                # æ ¼å¼1ï¼šæ ‡å‡†å­—æ®µæ ¼å¼ï¼ˆobject: xxxï¼‰
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                        
                    logger.debug(f"å¤„ç†è¡Œ: '{line}'")
                    
                    # æ£€æŸ¥æ¯ä¸€è¡Œæ˜¯å¦åŒ…å«å­—æ®µæ ‡è¯†ç¬¦
                    if line.lower().startswith('object:'):
                        raw_value = line[7:].strip()
                        result['object'] = clean_field_value(raw_value)
                        logger.debug(f"æå–object: '{raw_value}' -> '{result['object']}'")
                        
                    elif line.lower().startswith('sence:'):
                        raw_value = line[6:].strip()
                        result['sence'] = clean_field_value(raw_value)
                        logger.debug(f"æå–sence: '{raw_value}' -> '{result['sence']}'")
                        
                    elif line.lower().startswith('emotion:'):
                        raw_value = line[8:].strip()
                        result['emotion'] = clean_field_value(raw_value)
                        logger.debug(f"æå–emotion: '{raw_value}' -> '{result['emotion']}'")
                        
                    elif line.lower().startswith('brand_elements:'):
                        raw_value = line[15:].strip()
                        # ğŸ”§ ç‰¹æ®Šå¤„ç†brand_elementsï¼šè¿‡æ»¤æ•°å­—æ±¡æŸ“
                        cleaned_brand = clean_field_value(raw_value)
                        if cleaned_brand:
                            # è¿›ä¸€æ­¥æ¸…ç†ï¼šç§»é™¤çº¯æ•°å­—ç‰‡æ®µ
                            brand_parts = [part.strip() for part in cleaned_brand.split(',')]
                            clean_parts = [part for part in brand_parts if part and not re.match(r'^[0-9]+\.?[0-9]*$', part)]
                            result['brand_elements'] = ','.join(clean_parts) if clean_parts else ''
                        else:
                            result['brand_elements'] = ''
                        logger.debug(f"æå–brand_elements: '{raw_value}' -> '{result['brand_elements']}'")
                        
                    elif line.lower().startswith('confidence:'):
                        confidence_text = line[11:].strip()
                        try:
                            confidence_match = re.search(r'([0-9.]+)', confidence_text)
                            if confidence_match:
                                result['confidence'] = float(confidence_match.group(1))
                                logger.debug(f"æå–confidence: '{confidence_text}' -> {result['confidence']}")
                        except:
                            result['confidence'] = 0.8
            else:
                # æ ¼å¼2ï¼šç®€å•æ–‡æœ¬æ ¼å¼ - æ™ºèƒ½è§£æé€—å·åˆ†éš”çš„å†…å®¹
                logger.info("ğŸ”§ æ£€æµ‹åˆ°ç®€å•æ–‡æœ¬æ ¼å¼ï¼Œå¯ç”¨æ™ºèƒ½è§£æ")
                full_text = analysis_text.replace('\n', ' ').strip()
                
                # åˆ†å‰²ä¸ºtokens
                tokens = [token.strip() for token in full_text.split('ã€') if token.strip()]
                if not tokens:
                    tokens = [token.strip() for token in full_text.split(',') if token.strip()]
                
                # æ™ºèƒ½åˆ†ç±»é…ç½®
                from streamlit_app.utils.keyword_config import get_keyword_config
                try:
                    keywords_config = get_keyword_config()
                    
                    # è·å–é…ç½®è¯æ±‡ - ä½¿ç”¨æ­£ç¡®çš„é…ç½®ç»“æ„
                    from streamlit_app.utils.keyword_config import get_visual_objects, get_scenes, get_emotions, get_brands
                    
                    visual_objects = get_visual_objects()
                    visual_scenes = get_scenes() 
                    emotions = get_emotions()
                    brands = get_brands()
                    
                    # æ™ºèƒ½åˆ†ç±»
                    detected_objects = []
                    detected_scenes = []
                    detected_emotions = []
                    detected_brands = []
                    
                    for token in tokens:
                        cleaned_token = clean_field_value(token)
                        if not cleaned_token:
                            continue
                            
                        # å“ç‰Œä¼˜å…ˆçº§æœ€é«˜
                        if any(brand.lower() in cleaned_token.lower() for brand in brands):
                            detected_brands.append(cleaned_token)
                        # æƒ…ç»ªåŒ¹é…
                        elif any(emotion in cleaned_token for emotion in emotions):
                            detected_emotions.append(cleaned_token)
                        # åœºæ™¯åŒ¹é…
                        elif any(scene in cleaned_token for scene in visual_scenes):
                            detected_scenes.append(cleaned_token)
                        # ç‰©ä½“åŒ¹é…
                        elif any(obj in cleaned_token for obj in visual_objects):
                            detected_objects.append(cleaned_token)
                        else:
                            # å…œåº•ï¼šæ ¹æ®å…³é”®è¯ç‰¹å¾åˆ¤æ–­
                            if any(keyword in cleaned_token for keyword in ['å¥¶ç²‰', 'å¥¶ç“¶', 'å®å®', 'å¦ˆå¦ˆ', 'å©´å„¿', 'ç”¨å“']):
                                detected_objects.append(cleaned_token)
                            elif any(keyword in cleaned_token for keyword in ['å¨æˆ¿', 'å®¢å…', 'æˆ·å¤–', 'å…¬å›­', 'åŒ»é™¢', 'æ¸¸ä¹åœº']):
                                detected_scenes.append(cleaned_token)
                    
                    # å¡«å……ç»“æœ - ğŸ”§ å¢å¼ºæ•°å­—æ±¡æŸ“æ¸…ç†
                    result['object'] = ','.join(detected_objects) if detected_objects else ''
                    result['sence'] = ','.join(detected_scenes) if detected_scenes else ''
                    result['emotion'] = ','.join(detected_emotions) if detected_emotions else ''
                    # ğŸ”§ ç‰¹æ®Šå¤„ç†å“ç‰Œï¼šè¿‡æ»¤æ•°å­—æ±¡æŸ“
                    clean_brands = [brand for brand in detected_brands if brand and not re.match(r'^[0-9]+\.?[0-9]*$', brand)]
                    result['brand_elements'] = ','.join(clean_brands) if clean_brands else ''
                    
                    logger.info(f"ğŸ”§ æ™ºèƒ½è§£æç»“æœ:")
                    logger.info(f"   åŸå§‹tokens: {tokens}")
                    logger.info(f"   ç‰©ä½“: {detected_objects}")
                    logger.info(f"   åœºæ™¯: {detected_scenes}")
                    logger.info(f"   æƒ…ç»ª: {detected_emotions}")
                    logger.info(f"   å“ç‰Œ: {detected_brands}")
                    
                except Exception as e:
                    logger.warning(f"æ™ºèƒ½è§£æå¤±è´¥ï¼Œä½¿ç”¨å…œåº•ç­–ç•¥: {e}")
                    # å…œåº•ï¼šç›´æ¥å°†æ‰€æœ‰å†…å®¹æ”¾å…¥objectå­—æ®µ
                    result['object'] = clean_field_value(full_text)
            
            # ğŸ”§ åˆ›å»ºall_tags - åŒ…å«æ‰€æœ‰æœ‰æ„ä¹‰çš„å†…å®¹ï¼ˆå¼ºåŒ–æ•°å­—è¿‡æ»¤ï¼‰
            all_tags = []
            for field_name, value in result.items():
                if field_name == 'confidence':
                    continue
                if value:  # åªè¦ä¸ä¸ºç©ºå°±å¤„ç†
                    # åˆ†å‰²é€—å·åˆ†éš”çš„æ ‡ç­¾
                    tags = [tag.strip() for tag in value.split(',') if tag.strip()]
                    for tag in tags:
                        cleaned_tag = clean_field_value(tag)
                        # ğŸ”§ å¼ºåŒ–æ•°å­—è¿‡æ»¤ï¼šå†æ¬¡æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—
                        if cleaned_tag and not re.match(r'^[0-9]+\.?[0-9]*$', cleaned_tag):
                            all_tags.append(cleaned_tag)
            
            # å»é‡å¹¶è¿‡æ»¤
            result['all_tags'] = list(set(filter(None, all_tags)))
            
            logger.info(f"ğŸ¯ ç®€åŒ–è§£ææœ€ç»ˆç»“æœ:")
            logger.info(f"   ç‰©ä½“: '{result['object']}'")
            logger.info(f"   åœºæ™¯: '{result['sence']}'")
            logger.info(f"   æƒ…ç»ª: '{result['emotion']}'")
            logger.info(f"   å“ç‰Œ: '{result['brand_elements']}'")
            logger.info(f"   ç½®ä¿¡åº¦: {result['confidence']}")
            logger.info(f"   å…¨éƒ¨æ ‡ç­¾: {result['all_tags']}")
            
            return result
            
        except Exception as e:
            logger.error(f"è§£æåˆ†æç»“æœå¤±è´¥: {str(e)}")
            return {
                'object': '',
                'sence': '',
                'emotion': '',
                'brand_elements': '',
                'confidence': 0.1,
                'all_tags': []
            }
    
    def _needs_audio_fallback(self, visual_result: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¯ç”¨è¯­éŸ³è½¬å½•ä¿åº•æœºåˆ¶"""
        all_tags = visual_result.get('all_tags', [])
        
        # æƒ…å†µ1ï¼šall_tagså®Œå…¨ä¸ºç©º
        if not all_tags:
            return True
        
        # æƒ…å†µ2ï¼šall_tagsåªåŒ…å«æ— æ„ä¹‰å†…å®¹
        meaningless_tags = {'ä¸ç¡®å®š', 'æ— ', 'ç”»é¢ä¸æ¸…æ™°', 'è§£æå¤±è´¥', ''}
        if all(tag in meaningless_tags for tag in all_tags):
            return True
        
        # æƒ…å†µ3ï¼šè´¨é‡åˆ†è¿‡ä½
        quality_score = visual_result.get('quality_score', 1.0)
        if quality_score < 0.5:
            return True
        
        # æƒ…å†µ4ï¼šå…³é”®å­—æ®µä¸ºç©ºæ—¶å¯ç”¨éŸ³é¢‘å…œåº•
        object_empty = not visual_result.get('object') or visual_result.get('object') == ''
        brand_empty = not visual_result.get('brand_elements') or visual_result.get('brand_elements') == ''
        scene_empty = not visual_result.get('sence') or visual_result.get('sence') == ''
        
        # å¦‚æœç‰©ä½“å’Œå“ç‰Œéƒ½ä¸ºç©ºï¼Œå¯ç”¨éŸ³é¢‘å…œåº•
        if object_empty and brand_empty:
            logger.info("ğŸ¤ æ£€æµ‹åˆ°å…³é”®å­—æ®µä¸ºç©º(ç‰©ä½“+å“ç‰Œ)ï¼Œå¯ç”¨éŸ³é¢‘å…œåº•åˆ†æ")
            return True
        
        # å¦‚æœç‰©ä½“ã€åœºæ™¯ã€å“ç‰Œä¸‰è€…ä¸­æœ‰ä¸¤ä¸ªä¸ºç©ºï¼Œä¹Ÿå¯ç”¨éŸ³é¢‘åˆ†æ
        empty_count = sum([object_empty, brand_empty, scene_empty])
        if empty_count >= 2:
            logger.info(f"ğŸ¤ æ£€æµ‹åˆ°{empty_count}ä¸ªå…³é”®å­—æ®µä¸ºç©ºï¼Œå¯ç”¨éŸ³é¢‘å…œåº•åˆ†æ")
            return True
        
        return False
    
    def _get_targeted_analysis_prompt(self, transcription: str, visual_result: Dict[str, Any]) -> str:
        """
        ğŸ¯ ç”Ÿæˆé’ˆå¯¹æ€§åˆ†æprompt - åªåˆ†ævisualç¼ºå¤±çš„å­—æ®µ
        """
        # åˆ†ævisualç»“æœä¸­çš„ç©ºå­—æ®µ
        missing_fields = []
        field_analysis = {}
        
        object_empty = not visual_result.get('object') or visual_result.get('object') == ''
        scene_empty = not visual_result.get('sence') or visual_result.get('sence') == ''
        emotion_empty = not visual_result.get('emotion') or visual_result.get('emotion') == ''
        brand_empty = not visual_result.get('brand_elements') or visual_result.get('brand_elements') == ''
        
        if object_empty:
            missing_fields.append("object")
            field_analysis["object"] = "å¥¶ç²‰ã€å¥¶ç“¶ã€å®å®ã€å¦ˆå¦ˆã€åŒ»é™¢ã€æ¸¸ä¹åœº"
        
        if scene_empty:
            missing_fields.append("sence")
            field_analysis["sence"] = "å†²å¥¶ã€æŒ‡å¯¼ã€æŠ¤ç†ã€æˆ·å¤–ç©è€ã€åŒ»é™¢åœºæ™¯"
        
        if emotion_empty:
            missing_fields.append("emotion")
            field_analysis["emotion"] = "[å¿«ä¹ / å…´å¥‹ / æ¸©é¦¨ / ç„¦è™‘ / ç—›è‹¦]"
        
        if brand_empty:
            missing_fields.append("brand_elements")
            field_analysis["brand_elements"] = "å¯èµ‹ã€Wyethã€illumaã€A2ã€ATWOã€HMOã€DHA"
        
        # æ„å»ºé’ˆå¯¹æ€§prompt
        prompt_parts = [
            f"ğŸ¯ é’ˆå¯¹æ€§åˆ†ææ¯å©´vlogè¯­éŸ³è½¬å½•ï¼Œåªè¡¥å……visualåˆ†æç¼ºå¤±çš„{len(missing_fields)}ä¸ªå­—æ®µã€‚",
            f"",
            f"è¯­éŸ³å†…å®¹:",
            f"{transcription}",
            f"",
            f"ğŸ“‹ Visualåˆ†æå·²æœ‰ç»“æœ:",
        ]
        
        # æ˜¾ç¤ºå·²æœ‰ç»“æœ
        if not object_empty:
            prompt_parts.append(f"object: {visual_result.get('object', '')} âœ…")
        if not scene_empty:
            prompt_parts.append(f"sence: {visual_result.get('sence', '')} âœ…")
        if not emotion_empty:
            prompt_parts.append(f"emotion: {visual_result.get('emotion', '')} âœ…")
        if not brand_empty:
            prompt_parts.append(f"brand_elements: {visual_result.get('brand_elements', '')} âœ…")
        
        prompt_parts.extend([
            f"",
            f"ğŸ¯ ä»…éœ€åˆ†æä»¥ä¸‹{len(missing_fields)}ä¸ªç¼ºå¤±å­—æ®µ:",
        ])
        
        # åªåˆ—å‡ºéœ€è¦åˆ†æçš„å­—æ®µ
        for field in missing_fields:
            prompt_parts.append(f"{field}: {field_analysis[field]}")
        
        prompt_parts.extend([
            f"",
            f"ğŸ“ è¾“å‡ºè¦æ±‚:",
            f"1. ä»…è¾“å‡ºç¼ºå¤±å­—æ®µï¼Œä¸è¦é‡å¤å·²æœ‰ç»“æœ",
            f"2. å…¨ä¸­æ–‡å°å†™(å“ç‰Œåä¿ç•™å¤§å°å†™)",
            f"3. é€—å·åˆ†éš”å•è¯/çŸ­è¯­",
            f"4. confidence < 0.6 çš„å­—æ®µè¾“å‡ºä¸ºç©º",
            f"5. åŸºäºè¯­éŸ³å†…å®¹ï¼Œä¸è‡†æµ‹ç”»é¢",
            f"",
            f"è¾“å‡ºæ ¼å¼ - åªåŒ…å«ç¼ºå¤±å­—æ®µ:",
        ])
        
        # åŠ¨æ€ç”Ÿæˆè¾“å‡ºæ ¼å¼
        for field in missing_fields:
            prompt_parts.append(f"{field}: <åˆ†æç»“æœ>")
        
        return "\n".join(prompt_parts)
    
    def _audio_fallback_analysis(self, video_path: str, tag_language: str = "ä¸­æ–‡") -> Dict[str, Any]:
        """è¯­éŸ³è½¬å½•ä¿åº•åˆ†ææœºåˆ¶"""
        try:
            logger.info("ğŸ¤ å¼€å§‹è¯­éŸ³è½¬å½•åˆ†æ...")
            
            # æ­¥éª¤1ï¼šæå–éŸ³é¢‘å¹¶è½¬å½•
            transcription = self._extract_and_transcribe_audio(video_path)
            
            if not transcription or transcription.strip() == "":
                logger.warning("ğŸ¤ è¯­éŸ³è½¬å½•ç»“æœä¸ºç©º")
                return self._get_default_result("è¯­éŸ³è½¬å½•ç»“æœä¸ºç©º")
            
            logger.info(f"ğŸ¤ è½¬å½•æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(transcription)} å­—ç¬¦")
            
            # æ­¥éª¤2ï¼šä½¿ç”¨DeepSeekåˆ†æè½¬å½•æ–‡æœ¬
            audio_analysis = self._analyze_transcription_with_deepseek(transcription, tag_language)
            
            if audio_analysis.get("success"):
                audio_analysis["analysis_method"] = "audio_only"
                audio_analysis["transcription"] = transcription
                logger.info("ğŸ¤ è¯­éŸ³è½¬å½•åˆ†ææˆåŠŸ")
                return audio_analysis
            else:
                logger.warning("ğŸ¤ DeepSeekæ–‡æœ¬åˆ†æå¤±è´¥")
                return self._get_default_result("DeepSeekæ–‡æœ¬åˆ†æå¤±è´¥")
                
        except Exception as e:
            logger.error(f"è¯­éŸ³è½¬å½•ä¿åº•åˆ†æå¤±è´¥: {str(e)}")
            return self._get_default_result(f"è¯­éŸ³è½¬å½•åˆ†æå¤±è´¥: {str(e)}")
    
    def _targeted_audio_fallback_analysis(self, video_path: str, visual_result: Dict[str, Any], tag_language: str = "ä¸­æ–‡") -> Dict[str, Any]:
        """
        ğŸ¯ é’ˆå¯¹æ€§è¯­éŸ³è½¬å½•ä¿åº•åˆ†æ - åªè¡¥å¼ºvisualç¼ºå¤±å­—æ®µ
        """
        try:
            logger.info("ğŸ¯ å¼€å§‹é’ˆå¯¹æ€§è¯­éŸ³è½¬å½•åˆ†æ...")
            
            # æ­¥éª¤1ï¼šæå–éŸ³é¢‘å¹¶è½¬å½•
            transcription = self._extract_and_transcribe_audio(video_path)
            
            if not transcription or transcription.strip() == "":
                logger.warning("ğŸ¤ è¯­éŸ³è½¬å½•ç»“æœä¸ºç©º")
                return visual_result  # è¿”å›åŸå§‹visualç»“æœ
            
            logger.info(f"ğŸ¤ è½¬å½•æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(transcription)} å­—ç¬¦")
            
            # æ­¥éª¤2ï¼šä½¿ç”¨é’ˆå¯¹æ€§DeepSeekåˆ†æ
            audio_supplement = self._targeted_deepseek_analysis(transcription, visual_result, tag_language)
            
            if audio_supplement.get("success"):
                # æ­¥éª¤3ï¼šæ™ºèƒ½åˆå¹¶ç»“æœ
                merged_result = self._merge_targeted_results(visual_result, audio_supplement)
                merged_result["analysis_method"] = "visual_targeted_audio_fusion"
                merged_result["transcription"] = transcription
                logger.info("ğŸ¯ é’ˆå¯¹æ€§è§†è§‰+è¯­éŸ³èåˆå®Œæˆ")
                return merged_result
            else:
                logger.warning("ğŸ¤ é’ˆå¯¹æ€§DeepSeekåˆ†æå¤±è´¥ï¼Œè¿”å›visualç»“æœ")
                return visual_result
                
        except Exception as e:
            logger.error(f"é’ˆå¯¹æ€§è¯­éŸ³è½¬å½•åˆ†æå¤±è´¥: {str(e)}")
            return visual_result
    
    def _targeted_deepseek_analysis(self, transcription: str, visual_result: Dict[str, Any], tag_language: str) -> Dict[str, Any]:
        """
        ğŸ¯ é’ˆå¯¹æ€§DeepSeekåˆ†æ - åªåˆ†æç¼ºå¤±å­—æ®µ
        """
        try:
            if not self.deepseek_analyzer.is_available():
                logger.warning("ğŸ¤– DeepSeekåˆ†æå™¨ä¸å¯ç”¨")
                return {"success": False}
            
            logger.info("ğŸ¯ å¼€å§‹é’ˆå¯¹æ€§DeepSeekéŸ³é¢‘åˆ†æ...")
            
            # ç”Ÿæˆé’ˆå¯¹æ€§prompt
            targeted_prompt = self._get_targeted_analysis_prompt(transcription, visual_result)
            
            # ä½¿ç”¨DeepSeekåˆ†æå™¨
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ¯å©´äº§å“è¯­éŸ³å†…å®¹åˆ†æå¸ˆï¼Œä¸“é—¨è¡¥å……visualåˆ†æçš„ç¼ºå¤±å­—æ®µã€‚åªåˆ†ææŒ‡å®šçš„ç¼ºå¤±å­—æ®µï¼Œä¸è¦é‡å¤å·²æœ‰ç»“æœã€‚"},
                {"role": "user", "content": targeted_prompt}
            ]
            
            response = self.deepseek_analyzer._chat_completion(messages)
            
            if response and "choices" in response and response["choices"]:
                result_text = response["choices"][0].get("message", {}).get("content", "")
                logger.info(f"ğŸ¯ é’ˆå¯¹æ€§DeepSeekåˆ†æç»“æœ: {result_text}")
                
                # è§£æé’ˆå¯¹æ€§åˆ†æç»“æœ
                parsed_result = self._parse_targeted_deepseek_analysis(result_text, visual_result)
                
                if parsed_result:
                    parsed_result["success"] = True
                    return parsed_result
                else:
                    return {"success": False}
            else:
                logger.warning("ğŸ¤– é’ˆå¯¹æ€§DeepSeek APIè°ƒç”¨å¤±è´¥")
                return {"success": False}
                
        except Exception as e:
            logger.error(f"é’ˆå¯¹æ€§DeepSeekåˆ†æå¤±è´¥: {str(e)}")
            return {"success": False}
    
    def _parse_targeted_deepseek_analysis(self, analysis_text: str, visual_result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ğŸ¯ è§£æé’ˆå¯¹æ€§DeepSeekåˆ†æç»“æœ - åªå¤„ç†ç¼ºå¤±å­—æ®µ
        """
        try:
            result = {
                'object': visual_result.get('object', ''),  # ä¿ç•™visualç»“æœ
                'sence': visual_result.get('sence', ''),
                'emotion': visual_result.get('emotion', ''),
                'brand_elements': visual_result.get('brand_elements', ''),
                'confidence': visual_result.get('confidence', 0.7)
            }
            
            logger.info(f"ğŸ¯ å¼€å§‹è§£æé’ˆå¯¹æ€§åˆ†æç»“æœ:")
            logger.info(f"åŸå§‹æ–‡æœ¬: {analysis_text}")
            
            # æ¸…ç†å‡½æ•°
            def clean_field_value(value):
                if not value:
                    return ''
                cleaned = str(value).strip()
                # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰å€¼
                if cleaned in ['æ— ', 'ä¸ç¡®å®š', 'è§£æå¤±è´¥', 'N/A', 'null', 'None', '']:
                    return ''
                # ç§»é™¤æ–¹æ‹¬å·ç­‰
                cleaned = cleaned.replace('[', '').replace(']', '').replace('**', '')
                return cleaned.strip()
            
            # é€è¡Œå¤„ç†ï¼Œåªæ›´æ–°æœ‰å†…å®¹çš„å­—æ®µ
            lines = analysis_text.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                logger.info(f"å¤„ç†è¡Œ: '{line}'")
                
                # åªå¤„ç†DeepSeekæ–°åˆ†æçš„å­—æ®µï¼Œä¿ç•™visualçš„éç©ºå­—æ®µ
                if line.lower().startswith('object:') and not visual_result.get('object'):
                    raw_value = line[7:].strip()
                    new_value = clean_field_value(raw_value)
                    if new_value:  # åªæœ‰éç©ºæ—¶æ‰æ›´æ–°
                        result['object'] = new_value
                        logger.info(f"ğŸ¯ è¡¥å……object: '{new_value}'")
                    
                elif line.lower().startswith('sence:') and not visual_result.get('sence'):
                    raw_value = line[6:].strip()
                    new_value = clean_field_value(raw_value)
                    if new_value:
                        result['sence'] = new_value
                        logger.info(f"ğŸ¯ è¡¥å……sence: '{new_value}'")
                    
                elif line.lower().startswith('emotion:') and not visual_result.get('emotion'):
                    raw_value = line[8:].strip()
                    new_value = clean_field_value(raw_value)
                    if new_value:
                        result['emotion'] = new_value
                        logger.info(f"ğŸ¯ è¡¥å……emotion: '{new_value}'")
                    
                elif line.lower().startswith('brand_elements:') and not visual_result.get('brand_elements'):
                    raw_value = line[15:].strip()
                    new_value = clean_field_value(raw_value)
                    if new_value:
                        result['brand_elements'] = new_value
                        logger.info(f"ğŸ¯ è¡¥å……brand_elements: '{new_value}'")
                        
                elif line.lower().startswith('confidence:'):
                    confidence_text = line[11:].strip()
                    try:
                        import re
                        confidence_match = re.search(r'([0-9.]+)', confidence_text)
                        if confidence_match:
                            result['confidence'] = float(confidence_match.group(1))
                    except:
                        pass  # ä¿æŒåŸæœ‰confidence
            
            # æ›´æ–°all_tags
            all_tags = []
            for value in [result['object'], result['sence'], result['emotion'], result['brand_elements']]:
                if value:
                    tags = [tag.strip() for tag in value.split(',') if tag.strip()]
                    for tag in tags:
                        cleaned_tag = clean_field_value(tag)
                        if cleaned_tag and cleaned_tag not in all_tags:
                            all_tags.append(cleaned_tag)
            
            result['all_tags'] = all_tags
            
            logger.info(f"ğŸ¯ é’ˆå¯¹æ€§åˆ†æå®Œæˆ:")
            logger.info(f"   ç‰©ä½“: '{result['object']}'")
            logger.info(f"   åœºæ™¯: '{result['sence']}'")
            logger.info(f"   æƒ…ç»ª: '{result['emotion']}'")
            logger.info(f"   å“ç‰Œ: '{result['brand_elements']}'")
            
            return result
            
        except Exception as e:
            logger.error(f"è§£æé’ˆå¯¹æ€§DeepSeekç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _merge_targeted_results(self, visual_result: Dict[str, Any], audio_supplement: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ¯ åˆå¹¶é’ˆå¯¹æ€§ç»“æœ - é«˜æ•ˆèåˆ
        """
        try:
            logger.info("ğŸ¯ å¼€å§‹åˆå¹¶é’ˆå¯¹æ€§åˆ†æç»“æœ...")
            
            # ä»¥visualä¸ºåŸºç¡€ï¼Œç”¨audioè¡¥å……
            merged_result = visual_result.copy()
            
            # åªè¦†ç›–visualä¸­çš„ç©ºå­—æ®µ
            for field in ['object', 'sence', 'emotion', 'brand_elements']:
                visual_value = visual_result.get(field, '')
                audio_value = audio_supplement.get(field, '')
                
                # å¦‚æœvisualä¸ºç©ºä¸”audioæœ‰å€¼ï¼Œåˆ™ä½¿ç”¨audioçš„å€¼
                if not visual_value and audio_value:
                    merged_result[field] = audio_value
                    logger.info(f"ğŸ¯ å­—æ®µè¡¥å……: {field} = '{audio_value}'")
            
            # åˆå¹¶all_tags
            visual_tags = visual_result.get('all_tags', [])
            audio_tags = audio_supplement.get('all_tags', [])
            merged_tags = list(set(visual_tags + audio_tags))  # å»é‡
            merged_result['all_tags'] = merged_tags
            
            # æ›´æ–°è´¨é‡åˆ† - å¦‚æœæœ‰è¡¥å……ï¼Œè´¨é‡åˆ†æå‡
            supplemented_count = sum(1 for field in ['object', 'sence', 'emotion', 'brand_elements'] 
                                   if not visual_result.get(field) and audio_supplement.get(field))
            
            if supplemented_count > 0:
                original_quality = visual_result.get('quality_score', 0.5)
                boost = supplemented_count * 0.1  # æ¯è¡¥å……ä¸€ä¸ªå­—æ®µæå‡0.1
                merged_result['quality_score'] = min(1.0, original_quality + boost)
                logger.info(f"ğŸ¯ è´¨é‡åˆ†æå‡: {original_quality:.2f} â†’ {merged_result['quality_score']:.2f} (è¡¥å……äº†{supplemented_count}ä¸ªå­—æ®µ)")
            
            merged_result['success'] = True
            merged_result['targeted_supplement_count'] = supplemented_count
            
            return merged_result
            
        except Exception as e:
            logger.error(f"é’ˆå¯¹æ€§ç»“æœåˆå¹¶å¤±è´¥: {str(e)}")
            return visual_result  # å¤±è´¥æ—¶è¿”å›original visualç»“æœ
    
    def _extract_and_transcribe_audio(self, video_path: str) -> str:
        """ä½¿ç”¨DashScopeAudioAnalyzeræå–éŸ³é¢‘å¹¶è½¬å½•"""
        try:
            if not self.audio_analyzer.is_available():
                logger.warning("ğŸ¤ DashScopeéŸ³é¢‘åˆ†æå™¨ä¸å¯ç”¨")
                return ""
            
            logger.info(f"ğŸ¤ å¼€å§‹è½¬å½•è§†é¢‘éŸ³é¢‘: {video_path}")
            
            # ä½¿ç”¨éŸ³é¢‘åˆ†æå™¨è¿›è¡Œè½¬å½• - ğŸ”§ å¯ç”¨é»˜è®¤çƒ­è¯ä¼˜åŒ–
            result = self.audio_analyzer.transcribe_video(
                video_path=video_path,
                extract_audio_first=True,
                preset_vocabulary_id="default"  # ğŸ”§ ä½¿ç”¨é»˜è®¤çƒ­è¯ID
            )
            
            if result.get("success") and result.get("transcript"):
                transcription = result["transcript"]
                logger.info(f"ğŸ¤ è½¬å½•æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(transcription)} å­—ç¬¦")
                return transcription
            else:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                logger.warning(f"ğŸ¤ è½¬å½•å¤±è´¥: {error_msg}")
                return ""
                
        except Exception as e:
            logger.error(f"éŸ³é¢‘è½¬å½•å¤±è´¥: {str(e)}")
            return ""
    
    def _analyze_transcription_with_deepseek(self, transcription: str, tag_language: str) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeekAnalyzeråˆ†æè½¬å½•æ–‡æœ¬"""
        try:
            if not self.deepseek_analyzer.is_available():
                logger.warning("ğŸ¤– DeepSeekåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ–‡æœ¬åˆ†æ")
                return self._simple_text_analysis(transcription)
            
            logger.info("ğŸ¤– å¼€å§‹DeepSeekéŸ³é¢‘è½¬å½•æ–‡æœ¬åˆ†æ...")
                
            # æ„å»ºéŸ³é¢‘è½¬å½•åˆ†ææç¤ºè¯
            try:
                from streamlit_app.utils.keyword_config import sync_prompt_templates
                templates = sync_prompt_templates()
                analysis_prompt = templates.get("deepseek_audio", "").replace("[éŸ³é¢‘è½¬å½•æ–‡æœ¬]", transcription)
                
                if not analysis_prompt:
                    # ä½¿ç”¨å…œåº•prompt
                    analysis_prompt = self._get_fallback_audio_prompt(transcription)
                    
            except Exception as e:
                logger.warning(f"æ— æ³•å¯¼å…¥ç»Ÿä¸€éŸ³é¢‘promptæ¨¡æ¿ï¼Œä½¿ç”¨å…œåº•é…ç½®: {e}")
                analysis_prompt = self._get_fallback_audio_prompt(transcription)

            # ä½¿ç”¨DeepSeekåˆ†æå™¨
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ¯å©´äº§å“è¯­éŸ³å†…å®¹åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æè¯­éŸ³è½¬å½•æ–‡æœ¬ä¸­çš„äº§å“å’Œè¥é”€ä¿¡æ¯ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ]
            
            response = self.deepseek_analyzer._chat_completion(messages)
            
            if response and "choices" in response and response["choices"]:
                result_text = response["choices"][0].get("message", {}).get("content", "")
                logger.info(f"ğŸ¤– DeepSeekéŸ³é¢‘åˆ†æç»“æœ: {result_text}")
                
                # è§£æDeepSeekçš„åˆ†æç»“æœ
                parsed_result = self._parse_deepseek_analysis(result_text)
                
                if parsed_result:
                    parsed_result["success"] = True
                    return parsed_result
                else:
                    return self._simple_text_analysis(transcription)
            else:
                logger.warning("ğŸ¤– DeepSeek APIè°ƒç”¨å¤±è´¥")
                return self._simple_text_analysis(transcription)
                    
        except Exception as e:
            logger.error(f"DeepSeekæ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}")
            return self._simple_text_analysis(transcription)
    
    def _parse_deepseek_analysis(self, analysis_text: str) -> Optional[Dict[str, Any]]:
        """è§£æDeepSeekåˆ†æç»“æœ"""
        
        # ğŸ”§ é‡ç”¨Qwençš„æ ¼å¼æ¸…ç†å‡½æ•°
        def clean_field_value(value: str) -> str:
            """æ¸…ç†å­—æ®µå€¼ï¼Œç¡®ä¿è¾“å‡ºç®€æ´çš„å•è¯çŸ­è¯­"""
            if not value:
                return ''
            
            # åŸºç¡€æ¸…ç†
            cleaned = value.strip()
            
            # ğŸ”§ é‡è¦ä¿®å¤ï¼šç§»é™¤å­—æ®µæ ‡è¯†ç¬¦å¹²æ‰°
            field_markers = ['object:', 'sence:', 'emotion:', 'brand_elements:', 'confidence:']
            for marker in field_markers:
                if marker in cleaned:
                    # å¦‚æœåœ¨å¼€å¤´ï¼Œç§»é™¤å®ƒ
                    if cleaned.startswith(marker):
                        cleaned = cleaned[len(marker):].strip()
                    else:
                        # å¦‚æœåœ¨ä¸­é—´ï¼Œåªå–å‰é¢éƒ¨åˆ†
                        cleaned = cleaned.split(marker)[0].strip()
            
            # å»é™¤Markdownå’Œç‰¹æ®Šç¬¦å·
            cleaned = cleaned.replace('**', '').replace('*', '')
            cleaned = cleaned.replace('- ', '').replace('+ ', '')
            cleaned = cleaned.replace('# ', '').replace('[', '').replace(']', '')
            cleaned = cleaned.replace('(', '').replace(')', '')
            cleaned = cleaned.replace('"', '').replace("'", '')
            cleaned = cleaned.replace('ï¼š', ':').replace('ã€‚', '').replace('ï¼›', '')
            
            # å»é™¤"ç–‘ä¼¼"ç­‰è¯æ±‡
            cleaned = cleaned.replace('ç–‘ä¼¼', '').replace('å¯èƒ½', '')
            cleaned = cleaned.replace('åº”è¯¥', '').replace('ä¼¼ä¹', '')
            
            # å»é™¤é•¿å¥æè¿°ï¼ˆè¶…è¿‡20å­—çš„å†…å®¹æˆªå–å…³é”®è¯ï¼‰
            if len(cleaned) > 20:
                # æå–å…³é”®è¯ï¼ˆç®€å•å¤„ç†ï¼‰
                keywords = []
                key_terms = ['å¥¶ç²‰', 'å¥¶ç“¶', 'å¯èµ‹', 'Wyeth', 'A2', 'HMO', 'DHA', 
                           'å¨æˆ¿', 'å®¢å…', 'æ¸©é¦¨', 'ä¸“ä¸š', 'å…³çˆ±', 'å“ç‰Œ']
                for term in key_terms:
                    if term in cleaned:
                        keywords.append(term)
                cleaned = ','.join(keywords[:3]) if keywords else cleaned[:10]
            
            # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            cleaned = re.sub(r'[,ï¼Œ]+', ',', cleaned)
            cleaned = cleaned.strip(',')
            
            # è¿‡æ»¤æ— æ„ä¹‰å†…å®¹
            meaningless = ['æ— ', 'ä¸ç¡®å®š', 'ç”»é¢ä¸æ¸…æ™°', 'è§£æå¤±è´¥', 'è¯­éŸ³ä¿¡æ¯ä¸è¶³', 
                          '', 'none', 'n/a', 'ä¸æ˜ç¡®', 'è½¬å½•è¯¯å·®', 'ç›¸å…³']
            if cleaned.lower() in meaningless:
                return ''
            
            # ğŸ”§ æ–°å¢ï¼šè¿‡æ»¤çº¯æ•°å­—å†…å®¹ï¼ˆconfidenceå€¼è¯¯å…¥å…¶ä»–å­—æ®µï¼‰
            if re.match(r'^[0-9.]+$', cleaned):
                return ''
            
            # ğŸ”§ æ–°å¢ï¼šè¿‡æ»¤è¿‡çŸ­çš„æ— æ„ä¹‰å†…å®¹ï¼ˆ1-2å­—ç¬¦ï¼‰
            if len(cleaned) <= 2 and cleaned.lower() in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '.']:
                return ''
            
            return cleaned
        
        try:
            result = {
                'object': '',
                'sence': '', 
                'emotion': '',
                'brand_elements': '',
                'confidence': 0.7
            }
            
            logger.info(f"ğŸ¯ å¼€å§‹è§£æDeepSeekåˆ†ææ–‡æœ¬:")
            logger.info(f"åŸå§‹æ–‡æœ¬: {analysis_text}")
            
            # ğŸ”§ å…¨æ–°çš„è§£æç­–ç•¥ï¼šé€è¡Œå¤„ç†ï¼Œé¿å…è·¨è¡ŒåŒ¹é…é—®é¢˜
            lines = analysis_text.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                logger.info(f"å¤„ç†è¡Œ: '{line}'")
                
                # æ£€æŸ¥æ¯ä¸€è¡Œæ˜¯å¦åŒ…å«å­—æ®µæ ‡è¯†ç¬¦
                if line.lower().startswith('object:'):
                    raw_value = line[7:].strip()  # ç§»é™¤ "object:" 
                    result['object'] = clean_field_value(raw_value)
                    logger.info(f"æå–object: '{raw_value}' -> '{result['object']}'")
                    
                elif line.lower().startswith('sence:'):
                    raw_value = line[6:].strip()  # ç§»é™¤ "sence:"
                    result['sence'] = clean_field_value(raw_value)
                    logger.info(f"æå–sence: '{raw_value}' -> '{result['sence']}'")
                    
                elif line.lower().startswith('emotion:'):
                    raw_value = line[8:].strip()  # ç§»é™¤ "emotion:"
                    result['emotion'] = clean_field_value(raw_value)
                    logger.info(f"æå–emotion: '{raw_value}' -> '{result['emotion']}'")
                    
                elif line.lower().startswith('brand_elements:'):
                    raw_value = line[15:].strip()  # ç§»é™¤ "brand_elements:"
                    # ğŸ”§ ç‰¹æ®Šå¤„ç†brand_elementsï¼šè¿‡æ»¤æ•°å­—æ±¡æŸ“
                    cleaned_brand = clean_field_value(raw_value)
                    if cleaned_brand:
                        # è¿›ä¸€æ­¥æ¸…ç†ï¼šç§»é™¤çº¯æ•°å­—ç‰‡æ®µ
                        brand_parts = [part.strip() for part in cleaned_brand.split(',')]
                        clean_parts = [part for part in brand_parts if part and not re.match(r'^[0-9]+\.?[0-9]*$', part)]
                        result['brand_elements'] = ','.join(clean_parts) if clean_parts else ''
                    else:
                        result['brand_elements'] = ''
                    logger.info(f"æå–brand_elements: '{raw_value}' -> '{result['brand_elements']}'")
                    
                elif line.lower().startswith('confidence:'):
                    confidence_text = line[11:].strip()  # ç§»é™¤ "confidence:"
                    try:
                        # æå–æ•°å­—éƒ¨åˆ†
                        confidence_match = re.search(r'([0-9.]+)', confidence_text)
                        if confidence_match:
                            result['confidence'] = float(confidence_match.group(1))
                            logger.info(f"æå–confidence: '{confidence_text}' -> {result['confidence']}")
                    except:
                        result['confidence'] = 0.7
            
            # ğŸ”§ åˆ›å»ºall_tags - åªåŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹
            all_tags = []
            for value in [result['object'], result['sence'], result['emotion'], result['brand_elements']]:
                if value:  # åªè¦ä¸ä¸ºç©ºå°±å¤„ç†
                    # åˆ†å‰²é€—å·åˆ†éš”çš„æ ‡ç­¾
                    tags = [tag.strip() for tag in value.split(',') if tag.strip()]
                    for tag in tags:
                        cleaned_tag = clean_field_value(tag)
                        if cleaned_tag:  # æ¸…ç†åä¸ä¸ºç©ºå°±æ·»åŠ 
                            all_tags.append(cleaned_tag)
            
            # å»é‡å¹¶è¿‡æ»¤
            result['all_tags'] = list(set(filter(None, all_tags)))
            
            logger.info(f"ğŸ¯ DeepSeekæœ€ç»ˆè§£æç»“æœ:")
            logger.info(f"   ç‰©ä½“: '{result['object']}'")
            logger.info(f"   åœºæ™¯: '{result['sence']}'")
            logger.info(f"   æƒ…ç»ª: '{result['emotion']}'")
            logger.info(f"   å“ç‰Œ: '{result['brand_elements']}'")
            logger.info(f"   ç½®ä¿¡åº¦: {result['confidence']}")
            logger.info(f"   æ ‡ç­¾: {result['all_tags']}")
            
            return result if any(v for v in result.values() if v not in ['', 0.7, []]) else None
            
        except Exception as e:
            logger.error(f"è§£æDeepSeekç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _simple_text_analysis(self, text: str) -> Dict[str, Any]:
        """ç®€å•æ–‡æœ¬åˆ†æï¼ˆå…³é”®è¯åŒ¹é…ï¼‰"""
        object_keywords = ['å¥¶ç²‰', 'å¥¶ç“¶', 'å©´å„¿', 'å®å®', 'å¦ˆå¦ˆ', 'å­©å­', 'äº§å“']
        scene_keywords = ['å®¶åº­', 'å¨æˆ¿', 'å§å®¤', 'å®¢å…', 'è‚²å„¿']
        emotion_keywords = ['æ¸©é¦¨', 'å…³çˆ±', 'ä¸“ä¸š', 'èˆ’é€‚', 'å®‰å…¨']
        brand_keywords = ['è¥å…»', 'å“ç‰Œ', 'è´¨é‡', 'å¥åº·', 'æˆé•¿']
        
        found_objects = [kw for kw in object_keywords if kw in text]
        found_scenes = [kw for kw in scene_keywords if kw in text]
        found_emotions = [kw for kw in emotion_keywords if kw in text]
        found_brands = [kw for kw in brand_keywords if kw in text]
        
        result = {
            'object': ', '.join(found_objects) if found_objects else 'ä¸ç¡®å®š',
            'sence': ', '.join(found_scenes) if found_scenes else 'ä¸ç¡®å®š',
            'emotion': ', '.join(found_emotions) if found_emotions else 'ä¸ç¡®å®š',
            'brand_elements': ', '.join(found_brands) if found_brands else 'ä¸ç¡®å®š',
            'confidence': 0.6 if any([found_objects, found_scenes, found_emotions, found_brands]) else 0.3,
            'all_tags': found_objects + found_scenes + found_emotions + found_brands,
            'success': True
        }
        
        return result
    
    def _merge_visual_audio_results(self, visual_result: Dict[str, Any], audio_result: Dict[str, Any]) -> Dict[str, Any]:
        """èåˆè§†è§‰å’Œè¯­éŸ³åˆ†æç»“æœ"""
        try:
            logger.info("ğŸ¯ğŸ¤ å¼€å§‹èåˆè§†è§‰å’Œè¯­éŸ³åˆ†æç»“æœ...")
            
            # åŸºäºè´¨é‡åˆ†çš„èåˆç­–ç•¥
            visual_weight = visual_result.get('quality_score', 0.3)
            audio_weight = 0.7  # è¯­éŸ³è½¬å½•é€šå¸¸æ›´å¯é 
            
            # ğŸ”§ ä¿®å¤ï¼šæ ‡å‡†åŒ–å­—æ®µå€¼å¤„ç†å‡½æ•°
            def clean_and_split_value(value):
                """æ¸…ç†å¹¶åˆ†å‰²å­—æ®µå€¼"""
                if not value or value in ['æ— ', 'ä¸ç¡®å®š', 'è§£æå¤±è´¥', '']:
                    return []
                
                # æ¸…ç†å¤šä½™ç¬¦å· 
                cleaned = str(value).strip()
                # ç§»é™¤æ–¹æ‹¬å·å’Œç‰¹æ®Šç¬¦å·
                cleaned = cleaned.replace('[', '').replace(']', '').replace('**', '')
                # åˆ†å‰²å¹¶æ¸…ç†
                parts = []
                for part in cleaned.split(','):
                    part = part.strip()
                    if part and part not in ['æ— ', 'ä¸ç¡®å®š', 'è§£æå¤±è´¥']:
                        parts.append(part)
                return parts
            
            # ğŸ”§ ä¿®å¤ï¼šèåˆå„å­—æ®µ
            merged_result = {}
            
            for field in ['object', 'sence', 'emotion', 'brand_elements']:
                visual_value = visual_result.get(field, '')
                audio_value = audio_result.get(field, '')
                
                # æ¸…ç†å¹¶åˆå¹¶éç©ºå€¼
                visual_parts = clean_and_split_value(visual_value)
                audio_parts = clean_and_split_value(audio_value)
                
                # åˆå¹¶å¹¶å»é‡
                all_parts = visual_parts + audio_parts
                unique_parts = []
                for part in all_parts:
                    if part not in unique_parts:
                        unique_parts.append(part)
                
                # ğŸ”§ æ ¼å¼åŒ–è¾“å‡ºï¼šç¡®ä¿ä¸è§†è§‰åˆ†ææ ¼å¼ä¸€è‡´
                if unique_parts:
                    merged_result[field] = unique_parts[0] if len(unique_parts) == 1 else ', '.join(unique_parts)
                else:
                    merged_result[field] = ''  # ä¿æŒç©ºå€¼è€Œä¸æ˜¯"ä¸ç¡®å®š"
            
            # ğŸ”§ ä¿®å¤ï¼šèåˆè´¨é‡åˆ†å’Œç½®ä¿¡åº¦
            visual_conf = visual_result.get('confidence', 0.3)
            audio_conf = audio_result.get('confidence', 0.7)
            merged_confidence = visual_weight * visual_conf + audio_weight * audio_conf
            
            merged_result['confidence'] = round(merged_confidence, 2)
            merged_result['quality_score'] = round(merged_confidence, 2)
            
            # ğŸ”§ ä¿®å¤ï¼šèåˆall_tags - ç¡®ä¿æ ¼å¼ä¸€è‡´
            visual_tags = visual_result.get('all_tags', [])
            audio_tags = audio_result.get('all_tags', [])
            
            # æ¸…ç†å’Œåˆå¹¶æ ‡ç­¾
            all_tags_raw = visual_tags + audio_tags
            clean_tags = []
            for tag in all_tags_raw:
                if isinstance(tag, str):
                    # æ¸…ç†æ ‡ç­¾
                    clean_tag = tag.strip().replace('[', '').replace(']', '').replace('**', '')
                    if clean_tag and clean_tag not in ['æ— ', 'ä¸ç¡®å®š', 'è§£æå¤±è´¥'] and clean_tag not in clean_tags:
                        clean_tags.append(clean_tag)
            
            merged_result['all_tags'] = clean_tags
            
            # ğŸ”§ å…¶ä»–å¿…éœ€å­—æ®µ
            merged_result['success'] = True
            merged_result['analysis_method'] = 'visual_audio_fusion'
            merged_result['transcription'] = audio_result.get('transcription', '')
            
            logger.info(f"ğŸ¯ğŸ¤ è§†è§‰+è¯­éŸ³èåˆå®Œæˆ")
            logger.info(f"   ç‰©ä½“: {merged_result['object']}")
            logger.info(f"   åœºæ™¯: {merged_result['sence']}")
            logger.info(f"   æƒ…ç»ª: {merged_result['emotion']}")
            logger.info(f"   å“ç‰Œ: {merged_result['brand_elements']}")
            logger.info(f"   è´¨é‡åˆ†: {merged_result['quality_score']}")
            logger.info(f"   æ ‡ç­¾æ•°: {len(merged_result['all_tags'])}")
            
            return merged_result
            
        except Exception as e:
            logger.error(f"ç»“æœèåˆå¤±è´¥: {str(e)}")
            # å¦‚æœèåˆå¤±è´¥ï¼Œè¿”å›éŸ³é¢‘ç»“æœï¼ˆé€šå¸¸æ›´å¯é ï¼‰
            fallback_result = audio_result if audio_result.get('success') else visual_result
            fallback_result['analysis_method'] = 'fallback_audio' if audio_result.get('success') else 'fallback_visual'
            return fallback_result 
    
    def _detect_face_close_up(self, analysis_result: Dict[str, Any], video_path: str) -> bool:
        """
        ğŸ¯ æ£€æµ‹äººè„¸ç‰¹å†™ç‰‡æ®µ
        
        Args:
            analysis_result: è§†è§‰åˆ†æç»“æœ
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦ä¸ºäººè„¸ç‰¹å†™ç‰‡æ®µ
        """
        try:
            # åŠ è½½é…ç½®
            try:
                from streamlit_app.config.factory_config import VISUAL_ANALYSIS_CONFIG
                detection_config = VISUAL_ANALYSIS_CONFIG.get("face_close_up_detection", {})
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨äººè„¸ç‰¹å†™æ£€æµ‹
                if not detection_config.get("enabled", True):
                    return False
                
                face_close_up_indicators = detection_config.get("keywords", [
                    'äººè„¸', 'é¢éƒ¨', 'å¤´åƒ', 'ç‰¹å†™', 'è‚–åƒ', 'è„¸éƒ¨',
                    'çœ¼ç›', 'å˜´å”‡', 'é¼»å­', 'é¢å­”', 'å¤´éƒ¨ç‰¹å†™'
                ])
                face_area_threshold = detection_config.get("face_area_threshold", 0.3)
                
            except ImportError:
                # é…ç½®ä¸å¯ç”¨æ—¶ä½¿ç”¨é»˜è®¤å€¼
                face_close_up_indicators = [
                    'äººè„¸', 'é¢éƒ¨', 'å¤´åƒ', 'ç‰¹å†™', 'è‚–åƒ', 'è„¸éƒ¨',
                    'çœ¼ç›', 'å˜´å”‡', 'é¼»å­', 'é¢å­”', 'å¤´éƒ¨ç‰¹å†™'
                ]
                face_area_threshold = 0.3
            
            # æ–¹æ³•1: åŸºäºæ ‡ç­¾å†…å®¹æ£€æµ‹
            all_tags = analysis_result.get('all_tags', [])
            object_tags = analysis_result.get('object', '')
            scene_tags = analysis_result.get('sence', '')
            
            # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä¸»è¦åŒ…å«äººè„¸ç‰¹å†™ç›¸å…³å†…å®¹
            all_text = f"{' '.join(all_tags)} {object_tags} {scene_tags}".lower()
            
            face_indicators_count = 0
            for indicator in face_close_up_indicators:
                if indicator in all_text:
                    face_indicators_count += 1
            
            # æ£€æŸ¥åœºæ™¯ç¼ºå¤±æƒ…å†µï¼ˆäººè„¸ç‰¹å†™é€šå¸¸ç¼ºä¹åœºæ™¯ä¿¡æ¯ï¼‰
            scene_missing = not scene_tags or scene_tags in ['', 'æ— ', 'ä¸ç¡®å®š']
            
            # æ£€æŸ¥ç‰©ä½“æ ‡ç­¾æ˜¯å¦ä¸»è¦æ˜¯äººè„¸ç›¸å…³
            face_dominant = face_indicators_count >= 2 and scene_missing
            
            if face_dominant:
                logger.info(f"ğŸš« åŸºäºæ ‡ç­¾æ£€æµ‹åˆ°äººè„¸ç‰¹å†™: äººè„¸æŒ‡æ ‡{face_indicators_count}ä¸ªï¼Œåœºæ™¯ç¼ºå¤±: {scene_missing}")
                return True
            
            # æ–¹æ³•2: åŸºäºç”»é¢åˆ†ææ£€æµ‹ï¼ˆå¯å‘å¼è§„åˆ™ï¼‰
            # å¦‚æœç‰©ä½“æ ‡ç­¾å¾ˆå°‘ä¸”ä¸»è¦æ˜¯äººç›¸å…³ï¼Œè€Œåœºæ™¯ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ç‰¹å†™
            objects = object_tags.split(',') if object_tags else []
            person_related = ['äºº', 'å¦ˆå¦ˆ', 'å®å®', 'å©´å„¿', 'æ¯äº²', 'çˆ¶äº²', 'å®¶é•¿']
            
            if len(objects) <= 2 and scene_missing:
                person_objects = [obj for obj in objects if any(pr in obj for pr in person_related)]
                if len(person_objects) >= len(objects) * 0.8:  # 80%ä»¥ä¸Šæ˜¯äººç›¸å…³
                    logger.info(f"ğŸš« åŸºäºç‰©ä½“-åœºæ™¯æ¯”ä¾‹æ£€æµ‹åˆ°äººè„¸ç‰¹å†™: äººç‰©å¯¹è±¡{len(person_objects)}/{len(objects)}")
                    return True
            
            # æ–¹æ³•3: åŸºäºè§†é¢‘å¸§æ£€æµ‹ï¼ˆå¯é€‰ï¼Œéœ€è¦OpenCVï¼‰
            frame_based_detection = self._detect_face_close_up_by_frames(video_path, face_area_threshold)
            if frame_based_detection:
                logger.info(f"ğŸš« åŸºäºè§†é¢‘å¸§æ£€æµ‹åˆ°äººè„¸ç‰¹å†™")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"äººè„¸ç‰¹å†™æ£€æµ‹å¤±è´¥: {str(e)}")
            return False
    
    def _detect_face_close_up_by_frames(self, video_path: str, face_area_threshold: float = 0.3) -> bool:
        """
        ğŸ¯ åŸºäºè§†é¢‘å¸§çš„äººè„¸ç‰¹å†™æ£€æµ‹
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æ£€æµ‹åˆ°äººè„¸ç‰¹å†™
        """
        try:
            import cv2
            
            # æ‰“å¼€è§†é¢‘
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return False
            
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = frame_count / fps if fps > 0 else 0
            
            # å¦‚æœè§†é¢‘å¤ªçŸ­æˆ–å¤ªé•¿ï¼Œè·³è¿‡å¸§çº§æ£€æµ‹
            if duration < 1 or duration > 30:
                cap.release()
                return False
            
            # é‡‡æ ·å‡ å¸§è¿›è¡Œæ£€æµ‹
            sample_frames = min(3, frame_count // 10)  # æœ€å¤šé‡‡æ ·3å¸§
            frame_indices = [i * frame_count // (sample_frames + 1) for i in range(1, sample_frames + 1)]
            
            face_frames = 0
            total_frames_checked = 0
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if not ret:
                    continue
                
                total_frames_checked += 1
                
                # ç®€å•çš„äººè„¸æ£€æµ‹ï¼ˆåŸºäºé¢ç§¯å æ¯”ï¼‰
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                height, width = gray.shape
                
                try:
                    # ä½¿ç”¨OpenCVçš„äººè„¸æ£€æµ‹å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                    
                    # å¦‚æœæ£€æµ‹åˆ°äººè„¸ä¸”å æ¯”è¾ƒå¤§ï¼Œå¯èƒ½æ˜¯ç‰¹å†™
                    for (x, y, w, h) in faces:
                        face_area = w * h
                        frame_area = width * height
                        face_ratio = face_area / frame_area
                        
                        # å¦‚æœäººè„¸å ç”»é¢è¾¾åˆ°é…ç½®é˜ˆå€¼ä»¥ä¸Šï¼Œè®¤ä¸ºæ˜¯ç‰¹å†™
                        if face_ratio > face_area_threshold:
                            face_frames += 1
                            break
                            
                except Exception:
                    # å¦‚æœäººè„¸æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•
                    # æ£€æŸ¥ç”»é¢ä¸­å¤®åŒºåŸŸçš„å˜åŒ–
                    center_region = gray[height//4:3*height//4, width//4:3*width//4]
                    if center_region.std() > 50:  # ä¸­å¤®åŒºåŸŸå˜åŒ–è¾ƒå¤§ï¼Œå¯èƒ½æœ‰äººè„¸
                        face_frames += 1
            
            cap.release()
            
            # å¦‚æœä¸€åŠä»¥ä¸Šçš„å¸§éƒ½æ£€æµ‹åˆ°ç–‘ä¼¼äººè„¸ç‰¹å†™
            if total_frames_checked > 0 and face_frames / total_frames_checked >= 0.5:
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"åŸºäºå¸§çš„äººè„¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return False