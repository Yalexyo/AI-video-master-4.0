"""
è½¬å½•å·¥å…·å‡½æ•°
å°è£…è§†é¢‘è½¬å½•ç›¸å…³çš„å·¥å…·å‡½æ•°
"""

import tempfile
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path

logger = logging.getLogger(__name__)


def convert_video_to_srt(
    uploaded_video: Any,
    video_id: str,
    output_dir: str,
    use_hotwords: bool = True,
    cleanup_temp: bool = True,
    hotword_id: Optional[str] = None,
    analyze_target_audience: bool = True,
    hotword_mode: str = "use_preset",
    hotwords_text: str = "",
    preset_hotword_id: str = ""
) -> Dict[str, Any]:
    """
    å°†ä¸Šä¼ çš„è§†é¢‘è½¬æ¢ä¸ºSRTå­—å¹•æ–‡ä»¶ï¼Œå¹¶å¯é€‰è¿›è¡Œäººç¾¤åˆ†æ
    
    Args:
        uploaded_video: Streamlitä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶å¯¹è±¡
        video_id: è§†é¢‘IDï¼ˆä¸å«æ‰©å±•åï¼‰
        output_dir: è¾“å‡ºç›®å½•
        use_hotwords: æ˜¯å¦ä½¿ç”¨çƒ­è¯ä¼˜åŒ–ï¼ˆå‘åå…¼å®¹ï¼‰
        cleanup_temp: æ˜¯å¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        hotword_id: çƒ­è¯IDï¼ˆå‘åå…¼å®¹ï¼‰
        analyze_target_audience: æ˜¯å¦è¿›è¡Œäººç¾¤åˆ†æ
        hotword_mode: çƒ­è¯æ¨¡å¼ ("use_preset", "use_custom", "no_hotwords")
        hotwords_text: è‡ªå®šä¹‰çƒ­è¯æ–‡æœ¬
        preset_hotword_id: é¢„è®¾çƒ­è¯ID
        
    Returns:
        Dict: åŒ…å«è½¬æ¢ç»“æœå’Œäººç¾¤åˆ†æçš„å­—å…¸
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        abs_output_dir = Path(output_dir).resolve()
        abs_output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®SRTè¾“å‡ºè·¯å¾„
        srt_output_path = abs_output_dir / f"{video_id}.srt"
        
        logger.info(f"å¼€å§‹è½¬å½•è§†é¢‘: {video_id}")
        logger.info(f"ğŸ¯ çƒ­è¯æ¨¡å¼: {hotword_mode}")
        
        # ğŸ¯ æ ¹æ®çƒ­è¯æ¨¡å¼ç¡®å®šæœ€ç»ˆçš„çƒ­è¯å‚æ•°
        final_hotword_id = None
        final_hotwords_list = None
        
        if hotword_mode == "use_preset" and preset_hotword_id:
            final_hotword_id = preset_hotword_id
            logger.info(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾çƒ­è¯ID: {preset_hotword_id}")
        elif hotword_mode == "use_custom" and hotwords_text:
            # è§£æè‡ªå®šä¹‰çƒ­è¯
            final_hotwords_list = [
                word.strip() 
                for word in hotwords_text.replace('\n', ',').split(',') 
                if word.strip()
            ]
            logger.info(f"âœï¸ ä½¿ç”¨è‡ªå®šä¹‰çƒ­è¯ ({len(final_hotwords_list)} ä¸ª): {final_hotwords_list[:5]}...")
        elif hotword_mode == "no_hotwords":
            logger.info("ğŸš« ä¸ä½¿ç”¨çƒ­è¯ä¼˜åŒ–")
        else:
            # å‘åå…¼å®¹å¤„ç†
            if use_hotwords and hotword_id:
                final_hotword_id = hotword_id
                logger.info(f"ğŸ”„ å‘åå…¼å®¹ï¼šä½¿ç”¨çƒ­è¯ID: {hotword_id}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_video_path = Path(temp_dir) / f"{video_id}.mp4"
            
            # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
            with open(temp_video_path, "wb") as f:
                f.write(uploaded_video.getvalue())
            
            logger.info(f"ä¸´æ—¶è§†é¢‘æ–‡ä»¶å·²ä¿å­˜: {temp_video_path}")
            
            # è°ƒç”¨è½¬å½•åŠŸèƒ½
            result_srt_path = _perform_transcription(
                video_path=str(temp_video_path),
                output_dir=str(abs_output_dir),
                hotword_id=final_hotword_id,
                custom_hotwords=final_hotwords_list,
                output_srt=str(srt_output_path)
            )
            
            if result_srt_path and Path(result_srt_path).exists():
                logger.info(f"è½¬å½•æˆåŠŸå®Œæˆ: {result_srt_path}")
                
                # åŸºç¡€ç»“æœ
                result = {
                    "success": True,
                    "srt_path": result_srt_path,
                    "video_id": video_id,
                    "output_dir": output_dir,
                    "hotword_mode": hotword_mode,
                    "used_hotword_id": final_hotword_id,
                    "used_custom_hotwords": final_hotwords_list
                }
                
                # ğŸ¯ äººç¾¤åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if analyze_target_audience:
                    logger.info("ğŸ¯ å¼€å§‹è¿›è¡Œç›®æ ‡äººç¾¤åˆ†æ...")
                    target_audience_result = _analyze_target_audience_from_srt(result_srt_path)
                    result.update(target_audience_result)
                
                return result
            else:
                logger.error("è½¬å½•å¤±è´¥ï¼šæ— æ³•ç”Ÿæˆç²¾ç¡®çš„SRTæ–‡ä»¶")
                return {
                    "success": False,
                    "error": "è½¬å½•æœåŠ¡æœªè¿”å›æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆç²¾ç¡®çš„SRTå­—å¹•æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è½¬å½•æœåŠ¡é…ç½®æˆ–é‡è¯•è½¬å½•ã€‚",
                    "error_type": "no_timestamps",
                    "hotword_mode": hotword_mode,
                    "used_hotword_id": final_hotword_id
                }
                
    except ImportError as e:
        logger.error(f"è½¬å½•æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"è½¬å½•æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}"
        }
    except Exception as e:
        logger.error(f"è½¬å½•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return {
            "success": False,
            "error": str(e)
        }


def _perform_transcription(
    video_path: str,
    output_dir: str,
    hotword_id: Optional[str] = None,
    custom_hotwords: Optional[list] = None,
    output_srt: Optional[str] = None
) -> Optional[str]:
    """
    æ‰§è¡Œè§†é¢‘è½¬å½•
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        hotword_id: çƒ­è¯ID
        custom_hotwords: è‡ªå®šä¹‰çƒ­è¯åˆ—è¡¨
        output_srt: è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„
        
    Returns:
        str: ç”Ÿæˆçš„SRTæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # ğŸ¯ ä¼˜å…ˆå°è¯•ä½¿ç”¨æ–°çš„DashScopeåˆ†æå™¨
        try:
            from streamlit_app.modules.ai_analyzers.dashscope_audio_analyzer import DashScopeAudioAnalyzer
            
            analyzer = DashScopeAudioAnalyzer()
            if analyzer.is_available():
                logger.info("ğŸ¤– ä½¿ç”¨DashScopeéŸ³é¢‘åˆ†æå™¨è¿›è¡Œè½¬å½•")
                
                # è½¬å½•è§†é¢‘
                result = analyzer.transcribe_video(
            video_path=video_path,
                    hotwords=custom_hotwords,  # ä¼ é€’è‡ªå®šä¹‰çƒ­è¯åˆ—è¡¨
                    professional_terms=custom_hotwords,  # ä¹Ÿç”¨ä½œä¸“ä¸šè¯æ±‡
                    extract_audio_first=True,
                    preset_vocabulary_id=hotword_id  # ä¼ é€’é¢„è®¾çƒ­è¯ID
                )
                
                if result.get("success") and result.get("transcript"):
                    try:
                        # ğŸ¯ åˆ›å»ºSRTæ–‡ä»¶ï¼Œä¼ é€’æ—¶é—´æˆ³ç‰‡æ®µ
                        segments = result.get("segments", [])
                        srt_content = _create_srt_from_transcript(
                            result.get("transcript", ""), 
                            segments
                        )
                        
                        srt_path = output_srt or f"{output_dir}/{Path(video_path).stem}.srt"
                        with open(srt_path, 'w', encoding='utf-8') as f:
                            f.write(srt_content)
                        
                        logger.info(f"DashScopeè½¬å½•å®Œæˆï¼ŒSRTæ–‡ä»¶å·²ç”Ÿæˆ: {srt_path}")
                        logger.info(f"ğŸ¯ ä½¿ç”¨çš„çƒ­è¯æ¨¡å¼: {result.get('hotword_mode', 'unknown')}")
                        logger.info(f"ğŸ“‹ è¯æ±‡è¡¨ID: {result.get('vocabulary_id', 'none')}")
                        logger.info(f"â° æ—¶é—´æˆ³ç‰‡æ®µæ•°: {len(segments)}")
                        return srt_path
        
                    except ValueError as ve:
                        logger.error(f"âŒ SRTç”Ÿæˆå¤±è´¥: {str(ve)}")
                        # ä¸ä½¿ç”¨å…œåº•æ–¹æ¡ˆï¼Œç›´æ¥è¿”å›Noneè®©ä¸Šå±‚å¤„ç†é”™è¯¯
                        return None
                else:
                    logger.warning(f"DashScopeè½¬å½•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                logger.warning("DashScopeåˆ†æå™¨ä¸å¯ç”¨")
                
        except ImportError:
            logger.warning("DashScopeåˆ†æå™¨æ¨¡å—ä¸å¯ç”¨")
        except Exception as e:
            logger.warning(f"DashScopeè½¬å½•å¤±è´¥: {str(e)}")
        
        # âŒ ä¸å†ä½¿ç”¨å…œåº•æ–¹æ¡ˆï¼Œç›´æ¥è¿”å›None
        logger.error("âŒ æ‰€æœ‰è½¬å½•æ–¹æ³•éƒ½å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆç²¾ç¡®çš„SRTæ–‡ä»¶")
        return None
        
    except Exception as e:
        logger.error(f"è½¬å½•è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        return None


def _create_srt_from_transcript(transcript: str, segments: List[Dict[str, Any]] = None) -> str:
    """
    ä»è½¬å½•æ–‡æœ¬å’Œæ—¶é—´æˆ³ç‰‡æ®µåˆ›å»ºSRTæ ¼å¼å†…å®¹
    
    Args:
        transcript: è½¬å½•æ–‡æœ¬
        segments: åŒ…å«æ—¶é—´æˆ³çš„ç‰‡æ®µåˆ—è¡¨
        
    Returns:
        str: SRTæ ¼å¼çš„å­—å¹•å†…å®¹
        
    Raises:
        ValueError: å½“æ²¡æœ‰æ—¶é—´æˆ³ç‰‡æ®µæ—¶æŠ›å‡ºé”™è¯¯
    """
    if not transcript.strip():
        raise ValueError("è½¬å½•æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆSRTæ–‡ä»¶")
    
    # ğŸ¯ å¿…é¡»æœ‰æ—¶é—´æˆ³ç‰‡æ®µæ‰èƒ½ç”Ÿæˆç²¾ç¡®çš„SRT
    if not segments or len(segments) == 0:
        logger.error("âŒ æ²¡æœ‰æ—¶é—´æˆ³ç‰‡æ®µï¼Œæ— æ³•ç”Ÿæˆç²¾ç¡®çš„SRTæ–‡ä»¶")
        raise ValueError("è½¬å½•æœåŠ¡æœªè¿”å›æ—¶é—´æˆ³ä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆç²¾ç¡®çš„SRTå­—å¹•æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è½¬å½•æœåŠ¡é…ç½®æˆ–é‡è¯•è½¬å½•ã€‚")
    
    logger.info(f"ğŸ¯ ä½¿ç”¨ {len(segments)} ä¸ªæ—¶é—´æˆ³ç‰‡æ®µç”ŸæˆSRT")
    
    srt_content = []
    valid_segments = 0
    
    for i, segment in enumerate(segments):
        if segment.get('text', '').strip():
            # è½¬æ¢æ¯«ç§’ä¸ºSRTæ ¼å¼æ—¶é—´æˆ³
            start_ms = segment.get('start_time', 0)
            end_ms = segment.get('end_time', start_ms + 3000)
            
            # éªŒè¯æ—¶é—´æˆ³æœ‰æ•ˆæ€§
            if start_ms < 0 or end_ms <= start_ms:
                logger.warning(f"ç‰‡æ®µ {i+1} æ—¶é—´æˆ³æ— æ•ˆ: {start_ms}ms -> {end_ms}msï¼Œè·³è¿‡")
                continue
            
            srt_content.append(f"{valid_segments + 1}")
            srt_content.append(f"{_format_timestamp_ms(start_ms)} --> {_format_timestamp_ms(end_ms)}")
            srt_content.append(segment['text'].strip())
            srt_content.append("")
            valid_segments += 1
    
    if valid_segments == 0:
        raise ValueError("æ‰€æœ‰æ—¶é—´æˆ³ç‰‡æ®µéƒ½æ— æ•ˆï¼Œæ— æ³•ç”ŸæˆSRTæ–‡ä»¶")
    
    logger.info(f"âœ… æˆåŠŸç”ŸæˆåŒ…å« {valid_segments} ä¸ªæœ‰æ•ˆç‰‡æ®µçš„ç²¾ç¡®SRTæ–‡ä»¶")
    return "\n".join(srt_content)


def _format_timestamp(seconds: int) -> str:
    """
    æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºSRTæ ¼å¼ï¼ˆç§’ï¼‰
    
    Args:
        seconds: ç§’æ•°
        
    Returns:
        str: SRTæ ¼å¼çš„æ—¶é—´æˆ³
    """
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    secs = seconds % 60
    
    return f"{hours:02d}:{minutes:02d}:{secs:02d},000"


def _format_timestamp_ms(milliseconds: int) -> str:
    """
    æ ¼å¼åŒ–æ¯«ç§’æ—¶é—´æˆ³ä¸ºSRTæ ¼å¼
    
    Args:
        milliseconds: æ¯«ç§’æ•°
        
    Returns:
        str: SRTæ ¼å¼çš„æ—¶é—´æˆ³ (HH:MM:SS,mmm)
    """
    # è½¬æ¢æ¯«ç§’ä¸ºç§’å’Œæ¯«ç§’éƒ¨åˆ†
    total_seconds = milliseconds // 1000
    ms = milliseconds % 1000
    
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60
    
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{ms:03d}"


def _create_mock_srt(output_path: str) -> str:
    """
    åˆ›å»ºæ¨¡æ‹ŸSRTæ–‡ä»¶ç”¨äºæµ‹è¯•
    
    Args:
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        str: ç”Ÿæˆçš„SRTæ–‡ä»¶è·¯å¾„
    """
    mock_srt_content = """1
00:00:01,000 --> 00:00:03,500
è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å­—å¹•æ–‡ä»¶

2
00:00:03,500 --> 00:00:06,000
ä¸“ä¸ºæ¼”ç¤ºé›¶ä»¶å·¥å‚åŠŸèƒ½è€Œåˆ›å»º

3
00:00:06,000 --> 00:00:08,500
è¯·æ›¿æ¢ä¸ºçœŸå®çš„è½¬å½•æ¨¡å—
"""
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(mock_srt_content)
        
        logger.info(f"æ¨¡æ‹ŸSRTæ–‡ä»¶å·²åˆ›å»º: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"åˆ›å»ºæ¨¡æ‹ŸSRTæ–‡ä»¶å¤±è´¥: {e}")
        return None


def _analyze_target_audience_from_srt(srt_path: str) -> Dict[str, Any]:
    """
    ğŸ¯ åŸºäºSRTæ–‡ä»¶å†…å®¹åˆ†æç›®æ ‡äººç¾¤
    
    Args:
        srt_path: SRTæ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict: äººç¾¤åˆ†æç»“æœ
    """
    try:
        # è¯»å–SRTæ–‡ä»¶å†…å®¹
        full_transcript = _extract_text_from_srt(srt_path)
        
        if not full_transcript.strip():
            logger.warning("SRTæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œäººç¾¤åˆ†æ")
            return {
                "target_audience_analysis": {
                    "success": False,
                    "error": "SRTæ–‡ä»¶å†…å®¹ä¸ºç©º"
                }
            }
        
        logger.info(f"ğŸ“ ä»SRTæå–åˆ° {len(full_transcript)} å­—ç¬¦çš„è½¬å½•æ–‡æœ¬")
        
        # ğŸ¤– ä½¿ç”¨DeepSeekè¿›è¡Œäººç¾¤åˆ†æ
        target_audience_result = _perform_deepseek_audience_analysis(full_transcript)
        
        # ğŸ“Š ç”Ÿæˆäººç¾¤åˆ†ææŠ¥å‘Š
        analysis_report = _generate_audience_analysis_report(
            target_audience_result, 
            full_transcript,
            srt_path
        )
        
        return {
            "target_audience_analysis": {
                "success": True,
                "target_audience": target_audience_result.get("target_audience", "æœªè¯†åˆ«"),
                "confidence": target_audience_result.get("confidence", 0.0),
                "analysis_method": "deepseek_srt_analysis",
                "transcript_length": len(full_transcript),
                "srt_source": srt_path,
                "report": analysis_report
            }
        }
        
    except Exception as e:
        logger.error(f"äººç¾¤åˆ†æå¤±è´¥: {e}")
        return {
            "target_audience_analysis": {
                "success": False,
                "error": str(e)
            }
        }


def _extract_text_from_srt(srt_path: str) -> str:
    """ä»SRTæ–‡ä»¶æå–çº¯æ–‡æœ¬å†…å®¹"""
    try:
        with open(srt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£æSRTæ ¼å¼ï¼Œæå–æ–‡æœ¬
        lines = content.strip().split('\n')
        text_lines = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # è·³è¿‡åºå·è¡Œ
            if line.isdigit():
                i += 1
                continue
            
            # è·³è¿‡æ—¶é—´æˆ³è¡Œ
            if '-->' in line:
                i += 1
                continue
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                i += 1
                continue
            
            # æ”¶é›†æ–‡æœ¬è¡Œ
            text_lines.append(line)
            i += 1
        
        full_text = ' '.join(text_lines)
        logger.info(f"ä»SRTæ–‡ä»¶æå–åˆ° {len(text_lines)} è¡Œæ–‡æœ¬ï¼Œæ€»è®¡ {len(full_text)} å­—ç¬¦")
        
        return full_text
        
    except Exception as e:
        logger.error(f"ä»SRTæ–‡ä»¶æå–æ–‡æœ¬å¤±è´¥: {e}")
        return ""


def _perform_deepseek_audience_analysis(transcript: str) -> Dict[str, Any]:
    """ä½¿ç”¨DeepSeekè¿›è¡Œç›®æ ‡äººç¾¤åˆ†æ"""
    try:
        # å¯¼å…¥DeepSeekåˆ†æå™¨
        from streamlit_app.modules.ai_analyzers.deepseek_analyzer import DeepSeekAnalyzer
        
        analyzer = DeepSeekAnalyzer()
        
        # ğŸ” è¯¦ç»†æ£€æŸ¥åˆ†æå™¨çŠ¶æ€
        if not analyzer.is_available():
            logger.warning("DeepSeekåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å…œåº•åˆ†æ")
            logger.warning(f"APIå¯†é’¥çŠ¶æ€: {bool(analyzer.api_key)}")
            return _fallback_audience_analysis(transcript)
        
        logger.info("ğŸ¤– å¼€å§‹DeepSeekç›®æ ‡äººç¾¤åˆ†æ")
        logger.info(f"ğŸ“ è½¬å½•æ–‡æœ¬é•¿åº¦: {len(transcript)} å­—ç¬¦")
        logger.info(f"ğŸ“ è½¬å½•æ–‡æœ¬é¢„è§ˆ: {transcript[:200]}...")
        
        # è°ƒç”¨äººç¾¤åˆ†æ
        result = analyzer.analyze_video_summary(transcript)
        
        # ğŸ” è¯¦ç»†è®°å½•åˆ†æç»“æœ
        logger.info(f"ğŸ” DeepSeekåŸå§‹è¿”å›ç»“æœ: {result}")
        
        if result.get("error"):
            logger.error(f"âŒ DeepSeekåˆ†æè¿”å›é”™è¯¯: {result['error']}")
            return _fallback_audience_analysis(transcript)
        
        if result.get("target_audience"):
            target_audience = result["target_audience"].strip()
            if target_audience:
                logger.info(f"ğŸ¯ äººç¾¤åˆ†ææˆåŠŸ: {target_audience}")
                return {
                    "target_audience": target_audience,
                    "confidence": 0.85,  # DeepSeekåˆ†æç½®ä¿¡åº¦
                    "method": "deepseek_api",
                    "api_response": result  # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
                }
            else:
                logger.warning("âš ï¸ DeepSeekè¿”å›ç©ºçš„target_audienceå­—æ®µ")
        else:
            logger.warning("âš ï¸ DeepSeekå“åº”ä¸­ç¼ºå°‘target_audienceå­—æ®µ")
        
        logger.warning("DeepSeekåˆ†æè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨å…œåº•åˆ†æ")
        return _fallback_audience_analysis(transcript)
            
    except ImportError as e:
        logger.error(f"DeepSeekåˆ†æå™¨æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return _fallback_audience_analysis(transcript)
    except Exception as e:
        logger.error(f"DeepSeekäººç¾¤åˆ†æå¼‚å¸¸: {e}")
        logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        return _fallback_audience_analysis(transcript)


def _fallback_audience_analysis(transcript: str) -> Dict[str, Any]:
    """å…œåº•äººç¾¤åˆ†æï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
    try:
        # ğŸ¯ ä¼˜åŒ–åçš„äººç¾¤å…³é”®è¯æ˜ å°„
        audience_keywords = {
            "å­•æœŸå¦ˆå¦ˆ": [
                # å­•æœŸç›¸å…³
                "å­•æœŸ", "èƒå„¿", "å¶é…¸", "å­•å¦‡", "æ€€å­•", "äº§æ£€", "èƒæ•™", "å­•æ—©æœŸ", "å­•ä¸­æœŸ", "å­•æ™šæœŸ",
                # è¥å…»ç›¸å…³  
                "å­•æœŸè¥å…»", "DHA", "é’™ç‰‡", "ç»´ç”Ÿç´ ", "èƒå„¿å‘è‚²",
        
            ],
            "æ–°æ‰‹çˆ¸å¦ˆ": [
                # æ–°æ‰‹æ ‡è¯†
                "æ–°ç”Ÿå„¿", "ç¬¬ä¸€æ¬¡", "æ–°æ‰‹", "ä¸çŸ¥é“", "æ€ä¹ˆåŠ", "åˆä¸ºäººæ¯", "åˆä¸ºäººçˆ¶", 
                "åˆšå‡ºç”Ÿ", "æ–°ç”Ÿ", "0-6ä¸ªæœˆ", "ç¬¬ä¸€èƒ", "å¤´èƒ",
                # å›°æƒ‘è¡¨è¾¾
                "ä¸æ‡‚", "ä¸ä¼š", "å­¦ä¹ ", "è¯·æ•™", "æ–°æ‰‹å¦ˆå¦ˆ", "æ–°æ‰‹çˆ¸çˆ¸",
                # å©´å„¿æŠ¤ç†
                "æ€ä¹ˆå–‚", "å¦‚ä½•å†²å¥¶", "åƒå¤šå°‘", "ç¡çœ ", "å“­é—¹"
            ],
            "äºŒèƒå¦ˆå¦ˆ": [
                # äºŒèƒæ ‡è¯†
                "äºŒèƒ", "äºŒå®", "è€äºŒ", "ç¬¬äºŒä¸ª", "åˆæ€€äº†", "å†æ¬¡", 
                # ç»éªŒç›¸å…³
                "ç»éªŒ", "ç†Ÿæ‚‰", "è½»è½¦ç†Ÿè·¯", "ä¸Šæ¬¡", "è€å¤§", "å¤§å®",
                # æ—¶é—´ç®¡ç†
                "å¿™ç¢Œ", "çœå¿ƒ", "çœæ—¶", "æ–¹ä¾¿", "ç®€å•", "å¿«æ·"
            ],
            "æ··å…»å¦ˆå¦ˆ": [
                # æ··åˆå–‚å…»
                "æ··å–‚", "æ··åˆå–‚å…»", "æ¯ä¹³ä¸å¤Ÿ", "å¥¶ç²‰è¡¥å……", "æ­é…", "ç»“åˆ", 
                "æ¯ä¹³+å¥¶ç²‰", "ç™½å¤©æ¯ä¹³", "æ™šä¸Šå¥¶ç²‰", "è¡¥å……å–‚å…»",
                # è¥å…»å…³æ³¨
                "è¥å…»å‡è¡¡", "è¥å…»è¡¥å……", "è¥å…»ä¸è¶³", "å¢é‡", "å‘è‚²"
            ],
            "è´µå¦‡å¦ˆå¦ˆ": [
                # é«˜ç«¯å“è´¨
                "æœ‰æœº", "è¿›å£", "é«˜ç«¯", "å“è´¨", "ç²¾é€‰", "ä¼˜è´¨", "å¥¢å", "é¡¶çº§",
                # ä»·æ ¼ä¸æ•æ„Ÿ
                "è´µä¸€ç‚¹", "å€¼å¾—", "æŠ•èµ„", "æœ€å¥½çš„", "é«˜æ¡£", "ä¸“ä¸š",
                # å“ç‰Œåå¥½
                "æ¬§æ´²", "åŸè£…è¿›å£", "å›½é™…å“ç‰Œ"
            ]
        }
        
        # ğŸ”§ æ”¹è¿›çš„ç½®ä¿¡åº¦è®¡ç®—
        audience_scores = {}
        transcript_lower = transcript.lower()
        total_chars = len(transcript)
        
        for audience, keywords in audience_keywords.items():
            # è®¡ç®—å…³é”®è¯å‘½ä¸­æƒ…å†µ
            matched_keywords = []
            for keyword in keywords:
                if keyword in transcript_lower:
                    matched_keywords.append(keyword)
            
            if matched_keywords:
                # åŸºç¡€å¾—åˆ†ï¼šåŒ¹é…å…³é”®è¯æ¯”ä¾‹
                base_score = len(matched_keywords) / len(keywords)
                
                # ğŸ¯ æƒé‡è°ƒæ•´ï¼šè€ƒè™‘æ–‡æœ¬é•¿åº¦å’Œå…³é”®è¯é‡è¦æ€§
                length_bonus = min(total_chars / 500, 1.0)  # æ–‡æœ¬è¶Šé•¿ï¼Œç½®ä¿¡åº¦ç•¥å¾®æå‡
                keyword_weight = len(matched_keywords) * 0.1  # å¤šä¸ªå…³é”®è¯å¢åŠ æƒé‡
                
                # æœ€ç»ˆå¾—åˆ†
                final_score = min(base_score + length_bonus * 0.1 + keyword_weight, 1.0)
                
                audience_scores[audience] = {
                    "score": final_score,
                    "matched_keywords": matched_keywords,
                    "match_ratio": f"{len(matched_keywords)}/{len(keywords)}"
                }
        
        if audience_scores:
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„äººç¾¤
            best_audience = max(audience_scores, key=lambda x: audience_scores[x]["score"])
            best_result = audience_scores[best_audience]
            confidence = best_result["score"]
            
            logger.info(f"ğŸ¯ å…œåº•åˆ†æç»“æœ: {best_audience} (ç½®ä¿¡åº¦: {confidence:.2f})")
            logger.info(f"ğŸ“‹ åŒ¹é…å…³é”®è¯: {best_result['matched_keywords'][:3]}... ({best_result['match_ratio']})")
            
            return {
                "target_audience": best_audience,
                "confidence": confidence,
                "method": "keyword_fallback",
                "matched_keywords": best_result["matched_keywords"],
                "match_details": audience_scores
            }
        else:
            logger.info("ğŸ¯ æœªåŒ¹é…åˆ°æ˜ç¡®äººç¾¤ï¼Œè¿”å›é€šç”¨äººç¾¤")
            return {
                "target_audience": "é€šç”¨å¦ˆå¦ˆç¾¤ä½“", 
                "confidence": 0.3,
                "method": "default_fallback"
            }
            
    except Exception as e:
        logger.error(f"å…œåº•äººç¾¤åˆ†æå¤±è´¥: {e}")
        return {
            "target_audience": "æœªè¯†åˆ«",
            "confidence": 0.0,
            "method": "error",
            "error": str(e)
        }


def _generate_audience_analysis_report(
    analysis_result: Dict[str, Any], 
    transcript: str,
    srt_path: str
) -> Dict[str, Any]:
    """ç”Ÿæˆäººç¾¤åˆ†ææŠ¥å‘Š"""
    return {
        "analysis_timestamp": Path(srt_path).stat().st_mtime,
        "transcript_stats": {
            "total_length": len(transcript),
            "word_count": len(transcript.split()),
            "has_content": bool(transcript.strip())
        },
        "analysis_method": analysis_result.get("method", "unknown"),
        "confidence_level": "é«˜" if analysis_result.get("confidence", 0) > 0.7 else 
                          "ä¸­" if analysis_result.get("confidence", 0) > 0.4 else "ä½",
        "recommendation": _get_audience_recommendation(analysis_result.get("target_audience", ""))
    }


def _get_audience_recommendation(target_audience: str) -> str:
    """è·å–é’ˆå¯¹ç›®æ ‡äººç¾¤çš„è¥é”€å»ºè®®"""
    recommendations = {
        "å­•æœŸå¦ˆå¦ˆ": "å¼ºè°ƒè¥å…»å®‰å…¨ã€èƒå„¿å‘è‚²æ”¯æŒï¼Œçªå‡ºä¸“ä¸šåŒ»å­¦èƒŒæ™¯",
        "æ–°æ‰‹çˆ¸å¦ˆ": "æä¾›è¯¦ç»†æŒ‡å¯¼ã€è§£ç­”å¸¸è§ç–‘é—®ï¼Œå¼ºè°ƒä¸“ä¸šæ”¯æŒå’Œå®‰å¿ƒæ„Ÿ",
        "äºŒèƒå¦ˆå¦ˆ": "çªå‡ºä¾¿åˆ©æ€§ã€æ—¶é—´èŠ‚çœï¼Œä½“ç°å®ç”¨ä»·å€¼å’Œæ€§ä»·æ¯”",
        "æ··å…»å¦ˆå¦ˆ": "å¼ºè°ƒè¥å…»å‡è¡¡ã€ç§‘å­¦é…æ¯”ï¼Œæä¾›ä¸“ä¸šçš„æ··åˆå–‚å…»å»ºè®®",
        "è´µå¦‡å¦ˆå¦ˆ": "çªå‡ºé«˜ç«¯å“è´¨ã€è¿›å£å“ç‰Œï¼Œå¼ºè°ƒå“è´¨ç”Ÿæ´»å’Œä¼˜è¶Šé€‰æ‹©",
        "é€šç”¨å¦ˆå¦ˆç¾¤ä½“": "å¹³è¡¡å„æ–¹é¢éœ€æ±‚ï¼Œçªå‡ºäº§å“ç»¼åˆä¼˜åŠ¿"
    }
    
    return recommendations.get(target_audience, "æ ¹æ®ç›®æ ‡äººç¾¤ç‰¹ç‚¹å®šåˆ¶è¥é”€ç­–ç•¥")


def validate_transcription_dependencies() -> Dict[str, bool]:
    """
    éªŒè¯è½¬å½•åŠŸèƒ½çš„ä¾èµ–
    
    Returns:
        Dict: éªŒè¯ç»“æœ
    """
    checks = {
        "transcribe_core_available": False,
        "deepseek_analyzer_available": False,
        "ffmpeg_available": False
    }
    
    # æ£€æŸ¥è½¬å½•æ ¸å¿ƒæ¨¡å—
    try:
        from src.core.transcribe_core import create_srt_from_video
        checks["transcribe_core_available"] = True
    except ImportError:
        logger.warning("è½¬å½•æ ¸å¿ƒæ¨¡å—ä¸å¯ç”¨")
    
    # æ£€æŸ¥DeepSeekåˆ†æå™¨
    try:
        from streamlit_app.modules.ai_analyzers.deepseek_analyzer import DeepSeekAnalyzer
        analyzer = DeepSeekAnalyzer()
        checks["deepseek_analyzer_available"] = analyzer.is_available()
    except ImportError:
        logger.warning("DeepSeekåˆ†æå™¨ä¸å¯ç”¨")
    
    # æ£€æŸ¥ffmpeg
    import subprocess
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
        checks["ffmpeg_available"] = True
    except (subprocess.CalledProcessError, FileNotFoundError):
        logger.warning("ffmpegä¸å¯ç”¨")
    
    return checks 