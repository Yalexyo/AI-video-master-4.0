"""
è§†é¢‘åˆ†æå·¥å…·å‡½æ•°
å°è£…è§†é¢‘åˆ†æç›¸å…³çš„å·¥å…·å‡½æ•°
"""

import logging
import time
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path

logger = logging.getLogger(__name__)

# ğŸš€ å¯¼å…¥ä¼˜åŒ–åˆ†æå™¨
try:
    from streamlit_app.utils.factory.optimized_video_analysis import analyze_segments_with_high_efficiency
except ImportError:
    logger.warning("ä¼˜åŒ–åˆ†æå™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†ç‰ˆæœ¬")
    analyze_segments_with_high_efficiency = None


def analyze_video_with_google_cloud(
    video_path: Optional[str] = None,
    video_uri: Optional[str] = None,
    features: List[str] = None,
    auto_cleanup: bool = True,
    progress_callback: Optional[Callable] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨Google Cloud Video Intelligenceåˆ†æè§†é¢‘
    
    Args:
        video_path: æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„
        video_uri: äº‘ç«¯è§†é¢‘URI
        features: åˆ†æåŠŸèƒ½åˆ—è¡¨
        auto_cleanup: æ˜¯å¦è‡ªåŠ¨æ¸…ç†äº‘ç«¯æ–‡ä»¶
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        Dict: åˆ†æç»“æœ
    """
    try:
        from streamlit_app.modules.ai_analyzers import GoogleVideoAnalyzer
        
        analyzer = GoogleVideoAnalyzer()
        
        # æ£€æŸ¥å‡­æ®
        has_credentials, cred_path = analyzer.check_credentials()
        if not has_credentials:
            return {
                "success": False,
                "error": "Google Cloudå‡­æ®æœªè®¾ç½®æˆ–æ— æ•ˆ"
            }
        
        # è®¾ç½®é»˜è®¤åŠŸèƒ½
        if not features:
            features = ["shot_detection", "label_detection"]
        
        # åˆ†æè§†é¢‘
        if video_uri:
            result = analyzer.analyze_video(
                video_uri=video_uri,
                features=features,
                progress_callback=progress_callback,
                auto_cleanup_storage=False  # URIé€šå¸¸ä¸éœ€è¦æ¸…ç†
            )
        else:
            result = analyzer.analyze_video(
                video_path=video_path,
                features=features,
                progress_callback=progress_callback,
                auto_cleanup_storage=auto_cleanup
            )
        
        return result
        
    except ImportError:
        logger.warning("Google Cloudåˆ†æå™¨ä¸å¯ç”¨")
        return {
            "success": False,
            "error": "Google Cloudåˆ†æå™¨æ¨¡å—ä¸å¯ç”¨"
        }
    except Exception as e:
        logger.error(f"Google Cloudåˆ†æå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e)
        }


def analyze_segments_with_qwen(
    segment_files: List[Path],
    video_id: str,
    batch_size: int = 2,
    progress_callback: Optional[Callable] = None,
    use_optimized: bool = True  # ğŸš€ æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨Qwenæ¨¡å‹åˆ†æè§†é¢‘ç‰‡æ®µ
    
    Args:
        segment_files: ç‰‡æ®µæ–‡ä»¶åˆ—è¡¨
        video_id: è§†é¢‘ID
        batch_size: æ‰¹å¤„ç†å¤§å°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        use_optimized: æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        List: åˆ†æç»“æœåˆ—è¡¨
    """
    # ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    if use_optimized and analyze_segments_with_high_efficiency:
        logger.info("ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬Qwenåˆ†æå™¨")
        
        try:
            analysis_result = analyze_segments_with_high_efficiency(
                segment_files=segment_files,
                video_id=video_id,
                strategy="qwen_only",
                max_workers=min(3, batch_size),  # æ ¹æ®batch_sizeè°ƒæ•´å¹¶è¡Œæ•°
                progress_callback=progress_callback
            )
            
            if analysis_result.get("success"):
                results = analysis_result["results"]
                
                # æ˜¾ç¤ºæ•ˆç‡æŠ¥å‘Šï¼ˆå¦‚æœæœ‰å›è°ƒçš„è¯ï¼‰
                if progress_callback:
                    efficiency_report = analysis_result["efficiency_report"]
                    progress_callback(f"âš¡ ä¼˜åŒ–åˆ†æå®Œæˆ: {efficiency_report['cache_hit_rate']} ç¼“å­˜å‘½ä¸­ç‡ï¼Œ"
                                    f"æ•ˆç‡åˆ†æ•° {efficiency_report['efficiency_score']:.1f}/100")
                
                return results
            else:
                logger.warning("ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆæœ¬")
                # å›é€€åˆ°æ ‡å‡†ç‰ˆæœ¬
                return _analyze_segments_with_qwen_standard(segment_files, video_id, batch_size, progress_callback)
        except Exception as e:
            logger.warning(f"ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬åˆ†æå¼‚å¸¸ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆæœ¬: {e}")
            return _analyze_segments_with_qwen_standard(segment_files, video_id, batch_size, progress_callback)
    
    # ğŸ”„ æ ‡å‡†ç‰ˆæœ¬ï¼ˆå½“ä¼˜åŒ–ç‰ˆæœ¬ä¸å¯ç”¨æˆ–è¢«ç¦ç”¨æ—¶ï¼‰
    return _analyze_segments_with_qwen_standard(segment_files, video_id, batch_size, progress_callback)


def _analyze_segments_with_qwen_standard(
    segment_files: List[Path],
    video_id: str,
    batch_size: int = 2,
    progress_callback: Optional[Callable] = None
) -> List[Dict[str, Any]]:
    """
    ğŸ”„ æ ‡å‡†ç‰ˆæœ¬çš„Qwenåˆ†æï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    """
    try:
        from streamlit_app.modules.ai_analyzers import QwenVideoAnalyzer
        
        analyzer = QwenVideoAnalyzer()
        if not analyzer.is_available():
            logger.error("Qwenåˆ†æå™¨ä¸å¯ç”¨")
            return []
        
        results = []
        total_segments = len(segment_files)
        
        for i, segment_file in enumerate(segment_files):
            try:
                segment_name = segment_file.name
                
                if progress_callback:
                    progress_callback(f"Qwenåˆ†æ {i+1}/{total_segments}: {segment_name}")
                
                # åˆ†æè§†é¢‘ç‰‡æ®µ
                analysis_result = analyzer.analyze_video_segment(
                    video_path=str(segment_file),
                    tag_language="ä¸­æ–‡",
                    frame_rate=2.0
                )
                
                if analysis_result and analysis_result.get("success"):
                    segment_analysis = {
                        'file_name': segment_name,
                        'file_path': str(segment_file),
                        'file_size': segment_file.stat().st_size / (1024*1024),
                        'model': 'Qwen2.5',
                        'object': analysis_result.get('object', 'æ— '),
                        'sence': analysis_result.get('sence', 'æ— '),
                        'emotion': analysis_result.get('emotion', 'æ— '),
                        'brand_elements': analysis_result.get('brand_elements', 'æ— '),
                        'confidence': analysis_result.get('confidence', 0.0),
                        'success': True
                    }
                    results.append(segment_analysis)
                    logger.info(f"Qwenåˆ†ææˆåŠŸ: {segment_name}")
                else:
                    logger.warning(f"Qwenåˆ†æå¤±è´¥: {segment_name}")
                
                # APIé™æµ
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"åˆ†æç‰‡æ®µ {segment_file.name} æ—¶å‡ºé”™: {e}")
                continue
        
        return results
        
    except ImportError:
        logger.warning("Qwenåˆ†æå™¨ä¸å¯ç”¨")
        return []
    except Exception as e:
        logger.error(f"Qwenæ‰¹é‡åˆ†æå¤±è´¥: {e}")
        return []


def analyze_segments_with_intelligent_strategy(
    segment_files: List[Path],
    video_id: str,
    batch_size: int = 2,
    min_empty_tags: int = 2,
    auto_merge_results: bool = True,
    progress_callback: Optional[Callable] = None,
    use_optimized: bool = True  # ğŸš€ æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
) -> List[Dict[str, Any]]:
    """
    æ™ºèƒ½åˆ†æç­–ç•¥ï¼šä¸€çº§Qwen + äºŒçº§DeepSeekå…œåº•
    
    Args:
        segment_files: ç‰‡æ®µæ–‡ä»¶åˆ—è¡¨
        video_id: è§†é¢‘ID
        batch_size: æ‰¹å¤„ç†å¤§å°
        min_empty_tags: è§¦å‘DeepSeekçš„ç©ºæ ‡ç­¾æ•°é‡é˜ˆå€¼
        auto_merge_results: æ˜¯å¦è‡ªåŠ¨åˆå¹¶ç»“æœ
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        use_optimized: æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        List: æœ€ç»ˆåˆ†æç»“æœ
    """
    # ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    if use_optimized and analyze_segments_with_high_efficiency:
        logger.info("ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬æ™ºèƒ½ç­–ç•¥åˆ†æå™¨")
        
        try:
            analysis_result = analyze_segments_with_high_efficiency(
                segment_files=segment_files,
                video_id=video_id,
                strategy="intelligent",
                max_workers=min(3, batch_size),  # æ ¹æ®batch_sizeè°ƒæ•´å¹¶è¡Œæ•°
                progress_callback=progress_callback
            )
            
            if analysis_result.get("success"):
                results = analysis_result["results"]
                
                # æ˜¾ç¤ºæ•ˆç‡æŠ¥å‘Šï¼ˆå¦‚æœæœ‰å›è°ƒçš„è¯ï¼‰
                if progress_callback:
                    efficiency_report = analysis_result["efficiency_report"]
                    progress_callback(f"âš¡ æ™ºèƒ½ç­–ç•¥å®Œæˆ: {efficiency_report['cache_hit_rate']} ç¼“å­˜å‘½ä¸­ç‡ï¼Œ"
                                    f"æ•ˆç‡åˆ†æ•° {efficiency_report['efficiency_score']:.1f}/100")
                
                return results
            else:
                logger.warning("ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬æ™ºèƒ½ç­–ç•¥å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆæœ¬")
                # å›é€€åˆ°æ ‡å‡†ç‰ˆæœ¬
                return _analyze_segments_with_intelligent_strategy_standard(
                    segment_files, video_id, batch_size, min_empty_tags, auto_merge_results, progress_callback
                )
        except Exception as e:
            logger.warning(f"ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬æ™ºèƒ½ç­–ç•¥å¼‚å¸¸ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆæœ¬: {e}")
            return _analyze_segments_with_intelligent_strategy_standard(
                segment_files, video_id, batch_size, min_empty_tags, auto_merge_results, progress_callback
            )
    
    # ğŸ”„ æ ‡å‡†ç‰ˆæœ¬ï¼ˆå½“ä¼˜åŒ–ç‰ˆæœ¬ä¸å¯ç”¨æˆ–è¢«ç¦ç”¨æ—¶ï¼‰
    return _analyze_segments_with_intelligent_strategy_standard(
        segment_files, video_id, batch_size, min_empty_tags, auto_merge_results, progress_callback
    )


def _analyze_segments_with_intelligent_strategy_standard(
    segment_files: List[Path],
    video_id: str,
    batch_size: int = 2,
    min_empty_tags: int = 2,
    auto_merge_results: bool = True,
    progress_callback: Optional[Callable] = None
) -> List[Dict[str, Any]]:
    """
    ğŸ”„ æ ‡å‡†ç‰ˆæœ¬çš„æ™ºèƒ½ç­–ç•¥åˆ†æï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    """
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šQwenåˆ†æ
        if progress_callback:
            progress_callback("ç¬¬ä¸€é˜¶æ®µï¼šQwenè§†è§‰åˆ†æ")
        
        qwen_results = _analyze_segments_with_qwen_standard(
            segment_files, video_id, batch_size, progress_callback
        )
        
        # è¯†åˆ«éœ€è¦DeepSeekå…œåº•çš„ç‰‡æ®µ
        deepseek_needed = []
        for i, result in enumerate(qwen_results):
            empty_count = count_empty_tags(result)
            if empty_count >= min_empty_tags:
                deepseek_needed.append((segment_files[i], result))
        
        # ç¬¬äºŒé˜¶æ®µï¼šDeepSeekå…œåº•
        deepseek_results = []
        if deepseek_needed:
            if progress_callback:
                progress_callback(f"ç¬¬äºŒé˜¶æ®µï¼šDeepSeekå…œåº•åˆ†æ ({len(deepseek_needed)}ä¸ªç‰‡æ®µ)")
            
            deepseek_results = analyze_segments_with_deepseek(
                [item[0] for item in deepseek_needed],
                video_id,
                batch_size,
                progress_callback
            )
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šç»“æœåˆå¹¶
        if auto_merge_results:
            final_results = merge_analysis_results(qwen_results, deepseek_results, deepseek_needed)
            return final_results
        else:
            # è¿”å›åŸå§‹ç»“æœ
            return qwen_results + deepseek_results
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½åˆ†æç­–ç•¥å¤±è´¥: {e}")
        return []


def analyze_segments_with_deepseek(
    segment_files: List[Path],
    video_id: str,
    batch_size: int = 2,
    progress_callback: Optional[Callable] = None
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨DeepSeekæ¨¡å‹åˆ†æè§†é¢‘ç‰‡æ®µï¼ˆå…œåº•ï¼‰
    
    Args:
        segment_files: ç‰‡æ®µæ–‡ä»¶åˆ—è¡¨
        video_id: è§†é¢‘ID
        batch_size: æ‰¹å¤„ç†å¤§å°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        List: åˆ†æç»“æœåˆ—è¡¨
    """
    try:
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœï¼Œå› ä¸ºå®é™…çš„DeepSeekè§†é¢‘åˆ†æå¯èƒ½éœ€è¦ç‰¹æ®Šé…ç½®
        results = []
        
        for i, segment_file in enumerate(segment_files):
            segment_name = segment_file.name
            
            if progress_callback:
                progress_callback(f"DeepSeekå…œåº•åˆ†æ {i+1}/{len(segment_files)}: {segment_name}")
            
            # ç”Ÿæˆæ¨¡æ‹Ÿçš„DeepSeekç»“æœ
            mock_result = generate_mock_deepseek_result(segment_name)
            
            segment_analysis = {
                'file_name': segment_name,
                'file_path': str(segment_file),
                'file_size': segment_file.stat().st_size / (1024*1024),
                'model': 'DeepSeek-V3',
                'object': mock_result.get('object', 'å•†å“å±•ç¤º'),
                'sence': mock_result.get('sence', 'å®¤å†…åœºæ™¯'),
                'emotion': mock_result.get('emotion', 'ç§¯æ'),
                'brand_elements': mock_result.get('brand_elements', 'å“ç‰Œæ ‡è¯†'),
                'confidence': mock_result.get('confidence', 0.8),
                'success': True,
                'phase': 'deepseek'
            }
            
            results.append(segment_analysis)
            
            # APIé™æµ
            time.sleep(0.3)
        
        return results
        
    except Exception as e:
        logger.error(f"DeepSeekåˆ†æå¤±è´¥: {e}")
        return []


def count_empty_tags(analysis_result: Dict[str, Any]) -> int:
    """
    è®¡ç®—åˆ†æç»“æœä¸­çš„ç©ºæ ‡ç­¾æ•°é‡
    
    Args:
        analysis_result: åˆ†æç»“æœå­—å…¸
        
    Returns:
        int: ç©ºæ ‡ç­¾æ•°é‡
    """
    empty_count = 0
    check_fields = ['object', 'sence', 'emotion', 'brand_elements']
    
    for field in check_fields:
        value = analysis_result.get(field, '')
        if not value or value in ['æ— ', 'N/A', '', 'null', 'None']:
            empty_count += 1
    
    return empty_count


def generate_mock_deepseek_result(segment_name: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„DeepSeekåˆ†æç»“æœ
    
    Args:
        segment_name: ç‰‡æ®µæ–‡ä»¶å
        
    Returns:
        Dict: æ¨¡æ‹Ÿç»“æœ
    """
    # åŸºäºæ–‡ä»¶åç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿç»“æœ
    import hashlib
    
    hash_obj = hashlib.md5(segment_name.encode())
    hash_int = int(hash_obj.hexdigest()[:8], 16)
    
    objects = ['äº§å“ç‰¹å†™', 'äººç‰©è®²è§£', 'åœºæ™¯å±•ç¤º', 'å“ç‰Œæ ‡è¯†', 'ä½¿ç”¨æ¼”ç¤º']
    scenes = ['å®¤å†…ç¯å¢ƒ', 'æˆ·å¤–åœºæ™¯', 'å·¥ä½œç©ºé—´', 'ç”Ÿæ´»åœºæ™¯', 'å•†ä¸šç¯å¢ƒ']
    emotions = ['ç§¯æ', 'ä¸“ä¸š', 'æ¸©é¦¨', 'æ´»åŠ›', 'å¯é ']
    brands = ['å“ç‰Œæ ‡è¯†', 'Logoå±•ç¤º', 'äº§å“åŒ…è£…', 'ä¼ä¸šæ ‡è¯†', 'å“ç‰Œå…ƒç´ ']
    
    return {
        'object': objects[hash_int % len(objects)],
        'sence': scenes[hash_int % len(scenes)],
        'emotion': emotions[hash_int % len(emotions)],
        'brand_elements': brands[hash_int % len(brands)],
        'confidence': 0.75 + (hash_int % 25) / 100  # 0.75-0.99ä¹‹é—´çš„ç½®ä¿¡åº¦
    }


def merge_analysis_results(
    qwen_results: List[Dict[str, Any]],
    deepseek_results: List[Dict[str, Any]],
    deepseek_needed: List[tuple]
) -> List[Dict[str, Any]]:
    """
    åˆå¹¶Qwenå’ŒDeepSeekåˆ†æç»“æœ
    
    Args:
        qwen_results: Qwenåˆ†æç»“æœ
        deepseek_results: DeepSeekåˆ†æç»“æœ
        deepseek_needed: éœ€è¦DeepSeekå…œåº•çš„ç‰‡æ®µä¿¡æ¯
        
    Returns:
        List: åˆå¹¶åçš„æœ€ç»ˆç»“æœ
    """
    final_results = []
    
    # åˆ›å»ºDeepSeekç»“æœçš„æ˜ å°„
    deepseek_map = {}
    for result in deepseek_results:
        deepseek_map[result['file_name']] = result
    
    # å¤„ç†Qwenç»“æœ
    for qwen_result in qwen_results:
        file_name = qwen_result['file_name']
        empty_count = count_empty_tags(qwen_result)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰DeepSeekå…œåº•ç»“æœ
        if file_name in deepseek_map:
            # ä½¿ç”¨DeepSeekç»“æœæ›¿æ¢
            deepseek_result = deepseek_map[file_name]
            deepseek_result['analysis_strategy'] = 'Qwen + DeepSeekå…œåº•'
            deepseek_result['original_qwen_result'] = qwen_result
            final_results.append(deepseek_result)
        else:
            # ä½¿ç”¨Qwenç»“æœ
            qwen_result['analysis_strategy'] = 'Qwenå®Œæ•´åˆ†æ'
            qwen_result['empty_tags_count'] = empty_count
            final_results.append(qwen_result)
    
    return final_results


def create_video_segments(
    video_path: str,
    segments_data: List[Dict],
    video_id: str,
    is_clustered: bool = False,
    progress_callback: Optional[Callable] = None
) -> List[str]:
    """
    æ ¹æ®åˆ†æç»“æœåˆ›å»ºè§†é¢‘ç‰‡æ®µ
    
    Args:
        video_path: åŸå§‹è§†é¢‘è·¯å¾„
        segments_data: ç‰‡æ®µæ•°æ®åˆ—è¡¨
        video_id: è§†é¢‘ID
        is_clustered: æ˜¯å¦ä¸ºèšç±»åçš„åœºæ™¯åˆ‡åˆ†
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        List: æˆåŠŸåˆ›å»ºçš„ç‰‡æ®µæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    try:
        if not video_path or not Path(video_path).exists():
            logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        root_dir = Path(__file__).parent.parent.parent.parent
        
        if is_clustered:
            output_dir = root_dir / "data" / "results" / f"{video_id}_merge"
        else:
            output_dir = root_dir / "data" / "output" / "google_video" / video_id
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # å¯¼å…¥è§†é¢‘å¤„ç†å™¨
            from src.core.utils.video_processor import VideoProcessor
            processor = VideoProcessor()
            
            created_segments = []
            total_segments = len(segments_data)
            
            for i, segment_data in enumerate(segments_data):
                if progress_callback:
                    progress_callback(f"åˆ›å»ºç‰‡æ®µ {i+1}/{total_segments}")
                
                # æå–æ—¶é—´ä¿¡æ¯
                start_time = segment_data.get('start_time_seconds', 0)
                end_time = segment_data.get('end_time_seconds', start_time + 5)
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                segment_filename = f"segment_{i+1:03d}_{start_time:.1f}s-{end_time:.1f}s.mp4"
                output_path = output_dir / segment_filename
                
                # åˆ‡åˆ†è§†é¢‘
                success = processor.extract_segment(
                    input_path=video_path,
                    output_path=str(output_path),
                    start_time=start_time,
                    end_time=end_time
                )
                
                if success:
                    created_segments.append(str(output_path))
                    logger.info(f"æˆåŠŸåˆ›å»ºç‰‡æ®µ: {segment_filename}")
                else:
                    logger.warning(f"åˆ›å»ºç‰‡æ®µå¤±è´¥: {segment_filename}")
            
            return created_segments
            
        except ImportError:
            logger.warning("è§†é¢‘å¤„ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ›å»º")
            return _create_mock_segments(output_dir, segments_data)
            
    except Exception as e:
        logger.error(f"åˆ›å»ºè§†é¢‘ç‰‡æ®µå¤±è´¥: {e}")
        return []


def _create_mock_segments(output_dir: Path, segments_data: List[Dict]) -> List[str]:
    """
    åˆ›å»ºæ¨¡æ‹Ÿçš„è§†é¢‘ç‰‡æ®µæ–‡ä»¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        segments_data: ç‰‡æ®µæ•°æ®
        
    Returns:
        List: æ¨¡æ‹Ÿç‰‡æ®µæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    mock_segments = []
    
    for i, segment_data in enumerate(segments_data):
        start_time = segment_data.get('start_time_seconds', 0)
        end_time = segment_data.get('end_time_seconds', start_time + 5)
        
        segment_filename = f"mock_segment_{i+1:03d}_{start_time:.1f}s-{end_time:.1f}s.mp4"
        mock_path = output_dir / segment_filename
        
        # åˆ›å»ºç©ºæ–‡ä»¶ä½œä¸ºå ä½ç¬¦
        try:
            mock_path.touch()
            mock_segments.append(str(mock_path))
            logger.info(f"åˆ›å»ºæ¨¡æ‹Ÿç‰‡æ®µ: {segment_filename}")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¨¡æ‹Ÿç‰‡æ®µå¤±è´¥: {e}")
    
    return mock_segments


def validate_analysis_dependencies() -> Dict[str, bool]:
    """
    éªŒè¯åˆ†æåŠŸèƒ½çš„ä¾èµ–
    
    Returns:
        Dict: éªŒè¯ç»“æœ
    """
    checks = {
        "google_cloud_available": False,
        "qwen_available": False,
        "deepseek_available": False,
        "video_processor_available": False
    }
    
    # æ£€æŸ¥Google Cloudåˆ†æå™¨
    try:
        from streamlit_app.modules.ai_analyzers import GoogleVideoAnalyzer
        analyzer = GoogleVideoAnalyzer()
        has_creds, _ = analyzer.check_credentials()
        checks["google_cloud_available"] = has_creds
    except ImportError:
        logger.warning("Google Cloudåˆ†æå™¨ä¸å¯ç”¨")
    
    # æ£€æŸ¥Qwenåˆ†æå™¨
    try:
        from streamlit_app.modules.ai_analyzers import QwenVideoAnalyzer
        analyzer = QwenVideoAnalyzer()
        checks["qwen_available"] = analyzer.is_available()
    except ImportError:
        logger.warning("Qwenåˆ†æå™¨ä¸å¯ç”¨")
    
    # æ£€æŸ¥DeepSeek (é€šå¸¸é€šè¿‡APIé…ç½®æ£€æŸ¥)
    import os
    checks["deepseek_available"] = bool(os.getenv("DEEPSEEK_API_KEY"))
    
    # æ£€æŸ¥è§†é¢‘å¤„ç†å™¨
    try:
        from src.core.utils.video_processor import VideoProcessor
        checks["video_processor_available"] = True
    except ImportError:
        logger.warning("è§†é¢‘å¤„ç†å™¨ä¸å¯ç”¨")
    
    return checks 