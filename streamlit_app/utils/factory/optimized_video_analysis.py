#!/usr/bin/env python3
"""
ğŸš€ ä¼˜åŒ–è§†é¢‘åˆ†ææœºåˆ¶
å¤§å¹…æå‡åˆ†ææ•ˆç‡ï¼ˆç”¨æ—¶/tokenï¼‰çš„æ™ºèƒ½åˆ†æç³»ç»Ÿ
"""

import asyncio
import hashlib
import json
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Tuple
import threading

logger = logging.getLogger(__name__)


class HighEfficiencyVideoAnalyzer:
    """é«˜æ•ˆè§†é¢‘åˆ†æå™¨"""
    
    def __init__(self):
        self.cache = {}  # åˆ†æç»“æœç¼“å­˜
        self.cache_lock = threading.Lock()
        self.api_limiter = APIRateLimiter()
        
        # æ•ˆç‡ç»Ÿè®¡
        self.stats = {
            "total_segments": 0,
            "cache_hits": 0,
            "api_calls": 0,
            "total_time": 0,
            "total_tokens": 0
        }
    
    def analyze_segments_optimized(
        self,
        segment_files: List[Path],
        video_id: str,
        strategy: str = "intelligent",
        max_workers: int = 3,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        ğŸš€ é«˜æ•ˆæ‰¹é‡åˆ†æè§†é¢‘ç‰‡æ®µ
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ™ºèƒ½é¢„åˆ†æåˆ†ç»„
        2. å¹¶è¡Œå¤„ç†
        3. ç¼“å­˜å¤ç”¨
        4. åŠ¨æ€å¸§ç‡è°ƒæ•´
        5. æ™ºèƒ½APIé™æµ
        """
        start_time = time.time()
        self.stats["total_segments"] = len(segment_files)
        
        if progress_callback:
            progress_callback("ğŸ” é¢„åˆ†æç‰‡æ®µç‰¹å¾...")
        
        # ğŸ”§ ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½é¢„åˆ†æ - åˆ†ç»„ç›¸ä¼¼ç‰‡æ®µ
        segment_groups = self._preanalyze_and_group_segments(segment_files)
        
        if progress_callback:
            progress_callback(f"ğŸ“Š åˆ†ç»„å®Œæˆï¼Œ{len(segment_groups)}ä¸ªæ‰¹æ¬¡")
        
        # ğŸ”§ ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ç¼“å­˜
        cached_results, uncached_segments = self._check_cache(segment_files)
        
        if cached_results:
            self.stats["cache_hits"] = len(cached_results)
            if progress_callback:
                progress_callback(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­ {len(cached_results)} ä¸ªç‰‡æ®µ")
        
        # ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šå¹¶è¡Œåˆ†ææœªç¼“å­˜çš„ç‰‡æ®µ
        analysis_results = []
        if uncached_segments:
            if progress_callback:
                progress_callback(f"ğŸš€ å¹¶è¡Œåˆ†æ {len(uncached_segments)} ä¸ªç‰‡æ®µ...")
            
            analysis_results = self._parallel_analyze_segments(
                uncached_segments, segment_groups, strategy, max_workers, progress_callback
            )
        
        # ğŸ”§ ç¬¬å››æ­¥ï¼šåˆå¹¶ç»“æœ
        final_results = cached_results + analysis_results
        
        # ğŸ”§ ç¬¬äº”æ­¥ï¼šæ›´æ–°ç¼“å­˜
        self._update_cache(analysis_results)
        
        # ğŸ”§ ç¬¬å…­æ­¥ï¼šç”Ÿæˆæ•ˆç‡æŠ¥å‘Š
        total_time = time.time() - start_time
        self.stats["total_time"] = total_time
        
        efficiency_report = self._generate_efficiency_report()
        
        return {
            "results": final_results,
            "efficiency_report": efficiency_report,
            "success": True
        }
    
    def _preanalyze_and_group_segments(self, segment_files: List[Path]) -> Dict[str, List[Path]]:
        """
        ğŸ” æ™ºèƒ½é¢„åˆ†æï¼šåŸºäºæ–‡ä»¶ç‰¹å¾åˆ†ç»„ç›¸ä¼¼ç‰‡æ®µ
        ç›¸ä¼¼ç‰‡æ®µå¯ä»¥ä½¿ç”¨ç›¸åŒçš„åˆ†æå‚æ•°ï¼Œå‡å°‘é‡å¤è®¡ç®—
        """
        groups = {
            "short": [],      # <5ç§’çŸ­ç‰‡æ®µ - é«˜å¸§ç‡
            "medium": [],     # 5-15ç§’ä¸­ç­‰ç‰‡æ®µ - æ ‡å‡†å¸§ç‡  
            "long": [],       # >15ç§’é•¿ç‰‡æ®µ - ä½å¸§ç‡
            "similar": {}     # æŒ‰å†…å®¹ç›¸ä¼¼æ€§åˆ†ç»„
        }
        
        for segment_file in segment_files:
            try:
                # åŸºäºæ–‡ä»¶å¤§å°å’Œåç§°æ¨æµ‹æ—¶é•¿
                file_size_mb = segment_file.stat().st_size / (1024 * 1024)
                estimated_duration = self._estimate_duration_from_size(file_size_mb)
                
                if estimated_duration < 5:
                    groups["short"].append(segment_file)
                elif estimated_duration < 15:
                    groups["medium"].append(segment_file)
                else:
                    groups["long"].append(segment_file)
                    
            except Exception as e:
                logger.warning(f"é¢„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç»„: {e}")
                groups["medium"].append(segment_file)
        
        logger.info(f"ğŸ” é¢„åˆ†æå®Œæˆï¼šçŸ­ç‰‡æ®µ{len(groups['short'])}ä¸ªï¼Œä¸­ç­‰{len(groups['medium'])}ä¸ªï¼Œé•¿ç‰‡æ®µ{len(groups['long'])}ä¸ª")
        return groups
    
    def _estimate_duration_from_size(self, size_mb: float) -> float:
        """æ ¹æ®æ–‡ä»¶å¤§å°ä¼°ç®—è§†é¢‘æ—¶é•¿"""
        # ç»éªŒå…¬å¼ï¼š1MB â‰ˆ 30ç§’çš„ä½è´¨é‡è§†é¢‘ æˆ– 5ç§’çš„é«˜è´¨é‡è§†é¢‘
        # å–ä¸­é—´å€¼ä½œä¸ºä¼°ç®—
        return size_mb * 10  # å‡è®¾1MBâ‰ˆ10ç§’
    
    def _check_cache(self, segment_files: List[Path]) -> Tuple[List[Dict], List[Path]]:
        """æ£€æŸ¥ç¼“å­˜ï¼Œè¿”å›å·²ç¼“å­˜ç»“æœå’Œéœ€è¦åˆ†æçš„ç‰‡æ®µ"""
        cached_results = []
        uncached_segments = []
        
        with self.cache_lock:
            for segment_file in segment_files:
                cache_key = self._get_cache_key(segment_file)
                
                if cache_key in self.cache:
                    cached_result = self.cache[cache_key].copy()
                    cached_result["from_cache"] = True
                    cached_results.append(cached_result)
                else:
                    uncached_segments.append(segment_file)
        
        return cached_results, uncached_segments
    
    def _get_cache_key(self, segment_file: Path) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼šåŸºäºæ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´"""
        try:
            mtime = segment_file.stat().st_mtime
            content = f"{segment_file}_{mtime}"
            return hashlib.md5(content.encode()).hexdigest()
        except Exception:
            return hashlib.md5(str(segment_file).encode()).hexdigest()
    
    def _parallel_analyze_segments(
        self,
        segments: List[Path],
        groups: Dict[str, List[Path]],
        strategy: str,
        max_workers: int,
        progress_callback: Optional[Callable]
    ) -> List[Dict[str, Any]]:
        """ğŸš€ å¹¶è¡Œåˆ†æç‰‡æ®µ"""
        results = []
        completed_count = 0
        total_count = len(segments)
        
        # æ ¹æ®åˆ†ç»„ç¡®å®šæœ€ä¼˜åˆ†æå‚æ•°
        analysis_params = self._get_optimal_analysis_params(groups, segments)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_segment = {
                executor.submit(
                    self._analyze_single_segment_optimized,
                    segment,
                    analysis_params.get(segment, self._get_default_params()),
                    strategy
                ): segment
                for segment in segments
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_segment):
                segment = future_to_segment[future]
                completed_count += 1
                
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                        self.stats["api_calls"] += 1
                        
                    if progress_callback:
                        progress = (completed_count / total_count) * 100
                        progress_callback(f"ğŸ”„ å·²å®Œæˆ {completed_count}/{total_count} ({progress:.1f}%)")
                        
                except Exception as e:
                    logger.error(f"åˆ†æç‰‡æ®µ {segment.name} å¤±è´¥: {e}")
        
        return results
    
    def _get_optimal_analysis_params(self, groups: Dict, segments: List[Path]) -> Dict[Path, Dict]:
        """ğŸ¯ ä¸ºæ¯ä¸ªç‰‡æ®µç¡®å®šæœ€ä¼˜åˆ†æå‚æ•°"""
        params_map = {}
        
        # ğŸ¯ NEW: å¯¼å…¥çŸ­è§†é¢‘ä¼˜åŒ–å™¨è¿›è¡Œæ–‡ä»¶è¿‡æ»¤
        try:
            from utils.short_video_optimizer import ShortVideoOptimizer
            optimizer = ShortVideoOptimizer()
        except ImportError:
            optimizer = None
        
        for segment in segments:
            # ğŸ¯ NEW: è¿‡æ»¤è¿‡å°çš„æ–‡ä»¶
            if optimizer and not optimizer.should_process_video(str(segment)):
                continue  # è·³è¿‡è¿‡å°çš„æ–‡ä»¶
            
            # ğŸ¯ NEW: è·å–æ–‡ä»¶å¤§å°è¿›è¡Œæ›´ç²¾ç»†çš„ä¼˜åŒ–
            try:
                file_size_mb = segment.stat().st_size / (1024 * 1024)
            except:
                file_size_mb = 0
            
            if segment in groups.get("short", []):
                # ğŸ¯ çŸ­è§†é¢‘ï¼šæ ¹æ®æ–‡ä»¶å¤§å°è¿›è¡Œä¼˜åŒ–
                params_map[segment] = {
                    "frame_rate": 4.0,
                    "quality_threshold": 0.45,  # é€‚åº¦é™ä½è´¨é‡é˜ˆå€¼
                    "retry_count": 2
                }
            elif segment in groups.get("long", []):
                # é•¿ç‰‡æ®µï¼šä½å¸§ç‡ï¼ŒèŠ‚çœtoken
                params_map[segment] = {
                    "frame_rate": 1.5,
                    "quality_threshold": 0.6,
                    "retry_count": 1
                }
            else:
                # ä¸­ç­‰ç‰‡æ®µï¼šæ ‡å‡†å‚æ•°
                params_map[segment] = self._get_default_params()
        
        return params_map
    
    def _get_default_params(self) -> Dict:
        """é»˜è®¤åˆ†æå‚æ•°"""
        return {
            "frame_rate": 2.0,
            "quality_threshold": 0.65,
            "retry_count": 1
        }
    
    def _analyze_single_segment_optimized(
        self,
        segment_file: Path,
        params: Dict[str, Any],
        strategy: str
    ) -> Optional[Dict[str, Any]]:
        """ğŸ¯ ä¼˜åŒ–çš„å•ç‰‡æ®µåˆ†æ"""
        
        # æ™ºèƒ½APIé™æµ
        with self.api_limiter:
            try:
                if strategy == "qwen_only":
                    return self._analyze_with_qwen_optimized(segment_file, params)
                elif strategy == "intelligent":
                    return self._analyze_with_intelligent_strategy_optimized(segment_file, params)
                else:
                    return self._analyze_with_qwen_optimized(segment_file, params)
                    
            except Exception as e:
                logger.error(f"ä¼˜åŒ–åˆ†æå¤±è´¥ {segment_file.name}: {e}")
                return None
    
    def _analyze_with_qwen_optimized(
        self,
        segment_file: Path,
        params: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ğŸ¯ ä¼˜åŒ–çš„Qwenåˆ†æ - æ”¯æŒåŒæ¨¡å‹åˆ†å·¥å’Œå“ç‰Œæ£€æµ‹"""
        try:
            from modules.ai_analyzers import QwenVideoAnalyzer
            
            analyzer = QwenVideoAnalyzer()
            if not analyzer.is_available():
                return None
            
            # ğŸ¯ ä½¿ç”¨æ–°ç‰ˆæœ¬çš„åŒæ¨¡å‹åˆ†å·¥æœºåˆ¶
            result = analyzer.analyze_video_segment(
                video_path=str(segment_file),
                tag_language="ä¸­æ–‡",
                frame_rate=params["frame_rate"]
            )
            
            if result and result.get("success"):
                # ä¼°ç®—tokenä½¿ç”¨
                estimated_tokens = self._estimate_tokens_used(segment_file, params)
                self.stats["total_tokens"] += estimated_tokens
                
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å­—æ®µæ˜ å°„ï¼Œä¿æŒä¸æ–°ç‰ˆæœ¬ä¸€è‡´
                return {
                    'file_name': segment_file.name,
                    'file_path': str(segment_file),
                    'file_size': segment_file.stat().st_size / (1024*1024),
                    'model': 'Qwen-VL-Max-Latest',
                    # ğŸ¯ æ”¯æŒæ–°çš„å­—æ®µç»“æ„
                    'object': result.get('object', result.get('interaction', '')),  # å…¼å®¹æ–°çš„interactionå­—æ®µ
                    'scene': result.get('scene', ''),
                    'emotion': result.get('emotion', ''),
                    'brand_elements': result.get('brand_elements', ''),  # ğŸ”§ å…³é”®ï¼šä¸ä½¿ç”¨é»˜è®¤å€¼'æ— '
                    'confidence': result.get('confidence', 0.0),
                    'success': True,
                    'analysis_params': params,
                    'estimated_tokens': estimated_tokens,
                    # ğŸ¯ ä¿ç•™åˆ†ææ–¹æ³•ä¿¡æ¯
                    'analysis_strategy': result.get('analysis_method', 'Qwenå®Œæ•´åˆ†æ')
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Qwenä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            return None
    
    def _analyze_with_intelligent_strategy_optimized(
        self,
        segment_file: Path,
        params: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ğŸ§  ä¼˜åŒ–çš„æ™ºèƒ½ç­–ç•¥åˆ†æ - ä½¿ç”¨æ–°ç‰ˆæœ¬åŒæ¨¡å‹åˆ†å·¥æœºåˆ¶"""
        
        # ğŸ¯ ç›´æ¥ä½¿ç”¨QwenVideoAnalyzerçš„æ™ºèƒ½ç­–ç•¥ï¼Œå®ƒå·²ç»åŒ…å«äº†åŒæ¨¡å‹åˆ†å·¥å’Œå“ç‰Œæ£€æµ‹
        try:
            from modules.ai_analyzers import QwenVideoAnalyzer
            
            analyzer = QwenVideoAnalyzer()
            if not analyzer.is_available():
                return None
            
            # ğŸ¯ ä½¿ç”¨æ–°ç‰ˆæœ¬çš„æ™ºèƒ½ç­–ç•¥ï¼ˆåŒ…å«åŒæ¨¡å‹åˆ†å·¥å’Œå“ç‰Œæ£€æµ‹ï¼‰
            result = analyzer.analyze_video_segment(
                video_path=str(segment_file),
                tag_language="ä¸­æ–‡",
                frame_rate=params["frame_rate"]
            )
            
            if result and result.get("success"):
                # ä¼°ç®—tokenä½¿ç”¨
                estimated_tokens = self._estimate_tokens_used(segment_file, params)
                self.stats["total_tokens"] += estimated_tokens
                
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å­—æ®µæ˜ å°„ï¼Œä¿æŒä¸æ–°ç‰ˆæœ¬ä¸€è‡´
                return {
                    'file_name': segment_file.name,
                    'file_path': str(segment_file),
                    'file_size': segment_file.stat().st_size / (1024*1024),
                    'model': 'Qwen-VL-Max-Latest',
                    # ğŸ¯ æ”¯æŒæ–°çš„å­—æ®µç»“æ„
                    'object': result.get('object', result.get('interaction', '')),  # å…¼å®¹æ–°çš„interactionå­—æ®µ
                    'scene': result.get('scene', ''),
                    'emotion': result.get('emotion', ''),
                    'brand_elements': result.get('brand_elements', ''),  # ğŸ”§ å…³é”®ï¼šä¸ä½¿ç”¨é»˜è®¤å€¼'æ— '
                    'confidence': result.get('confidence', 0.0),
                    'success': True,
                    'analysis_params': params,
                    'estimated_tokens': estimated_tokens,
                    # ğŸ¯ ä¿ç•™åˆ†ææ–¹æ³•ä¿¡æ¯
                    'analysis_strategy': result.get('analysis_method', 'Qwenå®Œæ•´åˆ†æ')
                }
            
            return None
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½ç­–ç•¥ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            return None
    
    def _is_result_sufficient(self, result: Dict[str, Any]) -> bool:
        """åˆ¤æ–­åˆ†æç»“æœæ˜¯å¦å……åˆ†"""
        empty_count = 0
        check_fields = ['object', 'scene', 'emotion', 'brand_elements']
        
        for field in check_fields:
            value = result.get(field, '')
            if not value or value in ['æ— ', 'N/A', '', 'null']:
                empty_count += 1
        
        return empty_count < 2  # å°‘äº2ä¸ªç©ºå­—æ®µè§†ä¸ºå……åˆ†
    
    def _enhance_insufficient_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºä¸å……åˆ†çš„ç»“æœ - ä¿æŒç©ºå­—æ®µä¸ºç©ºï¼Œä¸å¡«å……æ— æ„ä¹‰å ä½ç¬¦"""
        enhanced = result.copy()
        
        # ğŸ”§ é‡è¦ä¿®å¤ï¼šä¸å†å¡«å……æ— æ„ä¹‰çš„å ä½ç¬¦
        # ä¿æŒç©ºå­—æ®µä¸ºç©ºï¼Œè¿™æ ·åœ¨åç»­åˆ†æä¸­æ›´å®¹æ˜“è¯†åˆ«å’Œå¤„ç†
        # å ä½ç¬¦å¯¹åç»­åˆ†ææ²¡æœ‰ä»»ä½•ä½œç”¨ï¼Œåè€Œä¼šå¹²æ‰°ç»“æœ
        
        # åªæ ‡è®°è¿›è¡Œäº†å¢å¼ºï¼Œä½†ä¸æ”¹å˜å†…å®¹
        enhanced['enhancement_applied'] = True
        
        # å¦‚æœç¡®å®éœ€è¦æ¨æ–­ï¼Œå¯ä»¥åŸºäºæ–‡ä»¶åã€å¤§å°ç­‰è¿›è¡Œç®€å•æ¨æ–­
        # ä½†ç›®å‰æš‚æ—¶ä¿æŒä¸ºç©ºæ›´å®‰å…¨
        logger.info("ğŸ”§ ä¿æŒä¸å……åˆ†å­—æ®µä¸ºç©ºï¼Œé¿å…æ— æ„ä¹‰å ä½ç¬¦å¹²æ‰°åˆ†æ")
        return enhanced
    
    def _estimate_tokens_used(self, segment_file: Path, params: Dict) -> int:
        """ä¼°ç®—tokenä½¿ç”¨é‡"""
        # åŸºäºå¸§ç‡å’Œæ–‡ä»¶å¤§å°ä¼°ç®—
        frame_rate = params.get("frame_rate", 2.0)
        file_size_mb = segment_file.stat().st_size / (1024 * 1024)
        estimated_duration = self._estimate_duration_from_size(file_size_mb)
        
        # æ¯å¸§å¤§çº¦æ¶ˆè€—100-200ä¸ªtoken
        estimated_frames = frame_rate * estimated_duration
        estimated_tokens = int(estimated_frames * 150)  # å¹³å‡æ¯å¸§150 token
        
        return estimated_tokens
    
    def _update_cache(self, results: List[Dict[str, Any]]):
        """æ›´æ–°åˆ†æç»“æœç¼“å­˜"""
        with self.cache_lock:
            for result in results:
                if result and result.get('file_path'):
                    segment_file = Path(result['file_path'])
                    cache_key = self._get_cache_key(segment_file)
                    
                    # ç§»é™¤ä¸´æ—¶å­—æ®µåç¼“å­˜
                    cache_result = result.copy()
                    cache_result.pop('analysis_params', None)
                    cache_result.pop('estimated_tokens', None)
                    
                    self.cache[cache_key] = cache_result
    
    def _generate_efficiency_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•ˆç‡æŠ¥å‘Š"""
        cache_hit_rate = (self.stats["cache_hits"] / self.stats["total_segments"]) * 100 if self.stats["total_segments"] > 0 else 0
        avg_time_per_segment = self.stats["total_time"] / self.stats["total_segments"] if self.stats["total_segments"] > 0 else 0
        avg_tokens_per_segment = self.stats["total_tokens"] / self.stats["api_calls"] if self.stats["api_calls"] > 0 else 0
        
        return {
            "total_segments": self.stats["total_segments"],
            "cache_hits": self.stats["cache_hits"],
            "cache_hit_rate": f"{cache_hit_rate:.1f}%",
            "api_calls": self.stats["api_calls"],
            "total_time": f"{self.stats['total_time']:.2f}s",
            "avg_time_per_segment": f"{avg_time_per_segment:.2f}s",
            "total_tokens_estimated": self.stats["total_tokens"],
            "avg_tokens_per_segment": f"{avg_tokens_per_segment:.0f}",
            "efficiency_score": self._calculate_efficiency_score()
        }
    
    def _calculate_efficiency_score(self) -> float:
        """è®¡ç®—æ•ˆç‡åˆ†æ•° (0-100)"""
        # åŸºäºç¼“å­˜å‘½ä¸­ç‡å’Œå¤„ç†é€Ÿåº¦è®¡ç®—
        cache_factor = (self.stats["cache_hits"] / self.stats["total_segments"]) * 40 if self.stats["total_segments"] > 0 else 0
        speed_factor = min(60, (1.0 / (self.stats["total_time"] / self.stats["total_segments"])) * 10) if self.stats["total_segments"] > 0 else 0
        
        return min(100, cache_factor + speed_factor)


class APIRateLimiter:
    """æ™ºèƒ½APIé™æµå™¨"""
    
    def __init__(self, initial_delay: float = 0.2):
        self.delay = initial_delay
        self.last_call_time = 0
        self.consecutive_successes = 0
        self.consecutive_failures = 0
        self.lock = threading.Lock()
    
    def __enter__(self):
        with self.lock:
            current_time = time.time()
            time_since_last_call = current_time - self.last_call_time
            
            if time_since_last_call < self.delay:
                sleep_time = self.delay - time_since_last_call
                time.sleep(sleep_time)
            
            self.last_call_time = time.time()
            return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ ¹æ®æˆåŠŸ/å¤±è´¥è°ƒæ•´å»¶æ—¶
        if exc_type is None:
            self.consecutive_successes += 1
            self.consecutive_failures = 0
            
            # è¿ç»­æˆåŠŸæ—¶å‡å°‘å»¶æ—¶
            if self.consecutive_successes > 5:
                self.delay = max(0.1, self.delay * 0.9)
        else:
            self.consecutive_failures += 1
            self.consecutive_successes = 0
            
            # è¿ç»­å¤±è´¥æ—¶å¢åŠ å»¶æ—¶
            if self.consecutive_failures > 2:
                self.delay = min(2.0, self.delay * 1.5)


# ä¾¿æ·æ¥å£å‡½æ•°
def analyze_segments_with_high_efficiency(
    segment_files: List[Path],
    video_id: str,
    strategy: str = "intelligent",
    max_workers: int = 3,
    progress_callback: Optional[Callable] = None
) -> Dict[str, Any]:
    """
    ğŸš€ é«˜æ•ˆè§†é¢‘ç‰‡æ®µåˆ†æå…¥å£å‡½æ•°
    
    Args:
        segment_files: ç‰‡æ®µæ–‡ä»¶åˆ—è¡¨
        video_id: è§†é¢‘ID
        strategy: åˆ†æç­–ç•¥ ("qwen_only", "intelligent")
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
    Returns:
        åŒ…å«åˆ†æç»“æœå’Œæ•ˆç‡æŠ¥å‘Šçš„å­—å…¸
    """
    analyzer = HighEfficiencyVideoAnalyzer()
    return analyzer.analyze_segments_optimized(
        segment_files=segment_files,
        video_id=video_id,
        strategy=strategy,
        max_workers=max_workers,
        progress_callback=progress_callback
    ) 