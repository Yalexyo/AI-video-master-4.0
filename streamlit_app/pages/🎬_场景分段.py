#!/usr/bin/env python3
"""
è§†é¢‘åœºæ™¯åˆ†æ®µé¡µé¢
å®ç°è§†é¢‘åœºæ™¯è¾¹ç•Œæ£€æµ‹å’Œæ‰‹åŠ¨è¯­ä¹‰æ ‡ç­¾åŒ¹é…åŠŸèƒ½
"""

import streamlit as st
import os
import sys
import json
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = Path(__file__).parent.parent.parent
sys.path.append(str(ROOT_DIR))

from src.core.utils.scene_detector import SceneDetector
from src.core.utils.video_processor import VideoProcessor
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¯­ä¹‰ç±»å‹é€‰é¡¹
SEMANTIC_TYPES = [
    "å¹¿å‘Šå¼€åœº", "é—®é¢˜é™ˆè¿°", "äº§å“ä»‹ç»", "äº§å“ä¼˜åŠ¿", "ç”¨æˆ·åé¦ˆ", 
    "ä¸“å®¶èƒŒä¹¦", "å“ç‰Œç†å¿µ", "è¡ŒåŠ¨å·å¬", "æ€»ç»“æ”¶å°¾", "å…¶ä»–"
]

def main():
    st.set_page_config(
        page_title="è§†é¢‘åœºæ™¯åˆ†æ®µ",
        page_icon="ğŸ¬",
        layout="wide"
    )
    
    st.title("ğŸ¬ è§†é¢‘åœºæ™¯åˆ†æ®µ")
    st.markdown("---")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ æ£€æµ‹é…ç½®")
        
        # åœºæ™¯æ£€æµ‹å‚æ•°
        threshold = st.slider(
            "åœºæ™¯å˜åŒ–é˜ˆå€¼", 
            min_value=0.1, 
            max_value=0.8, 
            value=0.3, 
            step=0.05,
            help="é˜ˆå€¼è¶Šä½ï¼Œæ£€æµ‹åˆ°çš„åœºæ™¯å˜åŒ–è¶Šå¤š"
        )
        
        min_scene_length = st.slider(
            "æœ€å°åœºæ™¯é•¿åº¦(ç§’)", 
            min_value=0.5, 
            max_value=5.0, 
            value=1.0, 
            step=0.5,
            help="è¿‡æ»¤æ‰å¤ªçŸ­çš„åœºæ™¯"
        )
        
        # æ·»åŠ æ£€æµ‹ç²¾åº¦è®¾ç½®
        detection_precision = st.selectbox(
            "æ£€æµ‹ç²¾åº¦",
            ["é«˜ç²¾åº¦ (0.1ç§’)", "è¶…é«˜ç²¾åº¦ (0.05ç§’)", "æ ‡å‡†ç²¾åº¦ (0.2ç§’)"],
            index=0,
            help="ğŸ¯ é«˜ç²¾åº¦: æ¯0.1ç§’æ£€æµ‹ä¸€æ¬¡ï¼Œå¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½\\nâš¡ è¶…é«˜ç²¾åº¦: æ¯0.05ç§’æ£€æµ‹ä¸€æ¬¡ï¼Œæœ€é«˜ç²¾åº¦ä½†è¾ƒæ…¢\\nğŸ“Š æ ‡å‡†ç²¾åº¦: æ¯0.2ç§’æ£€æµ‹ä¸€æ¬¡ï¼Œé€Ÿåº¦å¿«ä½†ç²¾åº¦è¾ƒä½"
        )
        
        # è§£æç²¾åº¦è®¾ç½®
        precision_map = {
            "é«˜ç²¾åº¦ (0.1ç§’)": 0.1,
            "è¶…é«˜ç²¾åº¦ (0.05ç§’)": 0.05,
            "æ ‡å‡†ç²¾åº¦ (0.2ç§’)": 0.2
        }
        precision_interval = precision_map[detection_precision]
        
        # æ˜¾ç¤ºç²¾åº¦è¯´æ˜
        if detection_precision == "è¶…é«˜ç²¾åº¦ (0.05ç§’)":
            st.warning("âš¡ è¶…é«˜ç²¾åº¦æ¨¡å¼å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œé€‚åˆå¯¹ç²¾åº¦è¦æ±‚æé«˜çš„åœºæ™¯")
        elif detection_precision == "é«˜ç²¾åº¦ (0.1ç§’)":
            st.info("ğŸ¯ æ¨èè®¾ç½®ï¼Œå¹³è¡¡ç²¾åº¦å’Œå¤„ç†é€Ÿåº¦")
        else:
            st.success("ğŸ“Š å¿«é€Ÿæ¨¡å¼ï¼Œé€‚åˆå¿«é€Ÿé¢„è§ˆå’Œå¤§æ–‡ä»¶å¤„ç†")
        
        detection_method = st.selectbox(
            "æ£€æµ‹æ–¹æ³•",
            ["ffmpeg", "content", "histogram"],
            index=0,
            help="ğŸ”¥ æ¨è: ffmpeg (ä¸“ä¸šçº§æ£€æµ‹ï¼Œç²¾åº¦é«˜)\nğŸ“Š content: åŸºäºå†…å®¹å˜åŒ–\nğŸ¨ histogram: åŸºäºé¢œè‰²ç›´æ–¹å›¾"
        )
        
        # æ·»åŠ æ–¹æ³•è¯´æ˜
        if detection_method == "ffmpeg":
            st.success("âš¡ **FFmpegä¸“ä¸šæ»¤é•œ**\n- ç²¾åº¦æœ€é«˜\n- æ€§èƒ½ä¼˜åŒ–\n- ä¸“ä¸šçº§ç®—æ³•")
            
            # FFmpegé«˜çº§é€‰é¡¹
            with st.expander("ğŸ”§ FFmpegé«˜çº§é€‰é¡¹"):
                use_adaptive_threshold = st.checkbox(
                    "è‡ªé€‚åº”é˜ˆå€¼", 
                    value=False,
                    help="æ ¹æ®è§†é¢‘å†…å®¹è‡ªåŠ¨è°ƒæ•´æ£€æµ‹é˜ˆå€¼"
                )
                
                enable_motion_detection = st.checkbox(
                    "è¿åŠ¨æ£€æµ‹å¢å¼º", 
                    value=True,
                    help="ç»“åˆè¿åŠ¨æ£€æµ‹æé«˜åœºæ™¯å˜åŒ–è¯†åˆ«ç²¾åº¦"
                )
                
                scene_detection_sensitivity = st.selectbox(
                    "æ£€æµ‹æ•æ„Ÿåº¦",
                    ["ä½", "ä¸­", "é«˜", "æé«˜"],
                    index=1,
                    help="è°ƒæ•´åœºæ™¯æ£€æµ‹çš„æ•æ„Ÿç¨‹åº¦"
                )
                
        elif detection_method == "content":
            st.info("ğŸ“Š **å†…å®¹åˆ†æ**\n- æ£€æµ‹ç”»é¢å˜åŒ–\n- é€‚åˆé•œå¤´åˆ‡æ¢\n- ä¸­ç­‰ç²¾åº¦")
        else:
            st.info("ğŸ¨ **ç›´æ–¹å›¾åˆ†æ**\n- æ£€æµ‹è‰²å½©å˜åŒ–\n- é€‚åˆç¯å¢ƒå˜åŒ–\n- å¯¹å…‰ç…§æ•æ„Ÿ")
        
        # æ·»åŠ æ€§èƒ½ç›‘æ§
        st.markdown("---")
        st.subheader("ğŸ“Š æ€§èƒ½ç›‘æ§")
        if st.button("ğŸ” æµ‹è¯•æ£€æµ‹æ€§èƒ½"):
            test_detection_performance()
    
    # ä¸»ç•Œé¢ - æ”¹ä¸ºå•åˆ—å¸ƒå±€
    st.header("ğŸ“ è§†é¢‘é€‰æ‹©")
    
    # è§†é¢‘æ–‡ä»¶é€‰æ‹©
    video_files = get_available_videos()
    
    if not video_files:
        st.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è§†é¢‘æ–‡ä»¶")
        st.info("è¯·å°†è§†é¢‘æ–‡ä»¶ä¸Šä¼ åˆ° `data/temp/uploads/` ç›®å½•")
        return
    
    selected_video = st.selectbox(
        "é€‰æ‹©è§†é¢‘æ–‡ä»¶",
        video_files,
        format_func=lambda x: os.path.basename(x)
    )
    
    if st.button("ğŸ” å¼€å§‹åœºæ™¯æ£€æµ‹", type="primary"):
        with st.spinner("æ­£åœ¨æ£€æµ‹è§†é¢‘åœºæ™¯..."):
            # æ”¶é›†é«˜çº§é€‰é¡¹å‚æ•°
            advanced_options = {}
            if detection_method == "ffmpeg":
                # ä»session stateè·å–FFmpegé«˜çº§é€‰é¡¹
                advanced_options = {
                    'use_adaptive_threshold': st.session_state.get('use_adaptive_threshold', False),
                    'enable_motion_detection': st.session_state.get('enable_motion_detection', True),
                    'scene_detection_sensitivity': st.session_state.get('scene_detection_sensitivity', 'ä¸­')
                }
            
            scenes = detect_video_scenes_advanced(
                selected_video, 
                threshold, 
                min_scene_length, 
                detection_method,
                precision_interval,
                **advanced_options
            )
            
            if scenes:
                st.session_state.scenes = scenes
                st.session_state.selected_video = selected_video
                st.session_state.detection_method = detection_method
                st.session_state.detection_params = {
                    'threshold': threshold,
                    'min_scene_length': min_scene_length,
                    'detection_interval': precision_interval,
                    **advanced_options
                }
                st.success(f"æ£€æµ‹å®Œæˆï¼å‘ç° {len(scenes)} ä¸ªåœºæ™¯")
                
                # æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
                if detection_method == "ffmpeg":
                    st.info(f"âš¡ ä½¿ç”¨FFmpegä¸“ä¸šæ£€æµ‹ | é˜ˆå€¼: {threshold} | æ•æ„Ÿåº¦: {advanced_options.get('scene_detection_sensitivity', 'ä¸­')}")
                else:
                    st.info(f"ğŸ“Š ä½¿ç”¨{detection_method}æ£€æµ‹ | é˜ˆå€¼: {threshold}")
            else:
                st.error("åœºæ™¯æ£€æµ‹å¤±è´¥")

    # åœºæ™¯åˆ†æ®µç»“æœåŒºåŸŸ
    st.markdown("---")
    st.header("ğŸ¯ åœºæ™¯åˆ†æ®µç»“æœ")
    
    if 'scenes' in st.session_state and st.session_state.scenes:
        display_scene_editor()
    else:
        st.info("è¯·å…ˆé€‰æ‹©è§†é¢‘å¹¶è¿›è¡Œåœºæ™¯æ£€æµ‹")

def get_available_videos():
    """è·å–å¯ç”¨çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨"""
    video_dirs = [
        "data/temp/uploads",
        "data/input/test_videos"
    ]
    
    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
    video_files = []
    
    for video_dir in video_dirs:
        if os.path.exists(video_dir):
            for file in os.listdir(video_dir):
                if any(file.lower().endswith(ext) for ext in video_extensions):
                    video_files.append(os.path.join(video_dir, file))
    
    return video_files

def detect_video_scenes(video_path, threshold, min_scene_length, method, detection_interval=0.1):
    """æ£€æµ‹è§†é¢‘åœºæ™¯"""
    try:
        detector = SceneDetector(
            threshold=threshold, 
            min_scene_length=min_scene_length,
            detection_interval=detection_interval
        )
        scenes = detector.detect_scenes(video_path, method=method)
        
        # ä¸ºæ¯ä¸ªåœºæ™¯æ·»åŠ é»˜è®¤çš„è¯­ä¹‰æ ‡ç­¾
        for i, scene in enumerate(scenes):
            scene['scene_id'] = i + 1
            scene['semantic_type'] = "å…¶ä»–"  # é»˜è®¤æ ‡ç­¾
            scene['manual_adjusted'] = False
        
        return scenes
        
    except Exception as e:
        logger.error(f"åœºæ™¯æ£€æµ‹å¤±è´¥: {e}")
        st.error(f"åœºæ™¯æ£€æµ‹å¤±è´¥: {e}")
        return []

def display_scene_editor():
    """æ˜¾ç¤ºåœºæ™¯ç¼–è¾‘å™¨"""
    scenes = st.session_state.scenes
    video_path = st.session_state.selected_video
    detection_method = st.session_state.get('detection_method', 'unknown')
    
    # æ˜¾ç¤ºè§†é¢‘ä¿¡æ¯å’Œæ£€æµ‹ç»Ÿè®¡
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        st.info(f"ğŸ“¹ è§†é¢‘: {os.path.basename(video_path)} | ğŸ¬ åœºæ™¯æ•°: {len(scenes)}")
    with col2:
        st.metric("æ£€æµ‹æ–¹æ³•", detection_method.upper())
    with col3:
        total_duration = sum(scene['end_time'] - scene['start_time'] for scene in scenes)
        st.metric("æ€»æ—¶é•¿", f"{total_duration:.1f}s")
    
    # æ‰¹é‡æ“ä½œå·¥å…·æ 
    st.markdown("---")
    st.subheader("ğŸ› ï¸ æ‰¹é‡æ“ä½œ")
    
    col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
    
    with col1:
        if st.button("ğŸ¯ æ™ºèƒ½è¯­ä¹‰å»ºè®®"):
            apply_smart_semantic_suggestions(scenes)
    
    with col2:
        batch_semantic_type = st.selectbox(
            "æ‰¹é‡è®¾ç½®ç±»å‹",
            SEMANTIC_TYPES,
            key="batch_semantic"
        )
        if st.button("ğŸ“ æ‰¹é‡åº”ç”¨"):
            apply_batch_semantic_type(scenes, batch_semantic_type)
    
    with col3:
        if st.button("ğŸ”„ é‡ç½®æ‰€æœ‰æ ‡ç­¾"):
            reset_all_semantic_types(scenes)
    
    with col4:
        if st.button("ğŸ“Š åœºæ™¯ç»Ÿè®¡"):
            show_scene_statistics(scenes)
    
    # æ˜¾ç¤ºåœºæ™¯åˆ—è¡¨
    st.markdown("---")
    st.subheader("ğŸ¬ åœºæ™¯åˆ—è¡¨")
    
    # åˆ›å»ºç¼–è¾‘è¡¨æ ¼
    edited_scenes = []
    
    for i, scene in enumerate(scenes):
        col1, col2, col3, col4, col5 = st.columns([0.5, 2, 2, 0.5, 0.5])
        
        with col1:
            # æ–‡ä»¶çŠ¶æ€ï¼ˆå¯¹åº”çº¢æ¡†ä¸­çš„"æ–‡ä»¶"åˆ—ï¼‰
            if scene.get('manual_adjusted', False):
                st.success("âœ…")
            else:
                st.warning("âš ï¸")
        
        with col2:
            # æ—¶é—´èŒƒå›´ï¼ˆå¯¹åº”çº¢æ¡†ä¸­çš„"æ—¶é—´"åˆ—ï¼‰
            time_range = f"{format_time(scene['start_time'])} - {format_time(scene['end_time'])}"
            duration = scene['end_time'] - scene['start_time']
            st.text(f"{time_range}")
            st.caption(f"æ—¶é•¿: {duration:.2f}s | ç½®ä¿¡åº¦: {scene['confidence']:.3f}")
        
        with col3:
            # è¯­ä¹‰ç±»å‹é€‰æ‹©ï¼ˆå¯¹åº”çº¢æ¡†ä¸­çš„"è¯­ä¹‰ç±»å‹"åˆ—ï¼‰
            semantic_type = st.selectbox(
                f"åœºæ™¯ {i+1} è¯­ä¹‰ç±»å‹",
                SEMANTIC_TYPES,
                index=SEMANTIC_TYPES.index(scene['semantic_type']) if scene['semantic_type'] in SEMANTIC_TYPES else 0,
                key=f"semantic_{i}",
                label_visibility="collapsed"
            )
            
            # æ›´æ–°åœºæ™¯çš„è¯­ä¹‰ç±»å‹
            if semantic_type != scene['semantic_type']:
                scene['semantic_type'] = semantic_type
                scene['manual_adjusted'] = True
        
        with col4:
            # é¢„è§ˆæŒ‰é’®
            if st.button("ğŸ‘ï¸", key=f"preview_{i}", help="é¢„è§ˆåœºæ™¯"):
                preview_scene(video_path, scene)
        
        with col5:
            # åˆ é™¤æŒ‰é’®
            if st.button("ğŸ—‘ï¸", key=f"delete_{i}", help="åˆ é™¤åœºæ™¯"):
                if st.session_state.get(f"confirm_delete_{i}", False):
                    scenes.pop(i)
                    st.rerun()
                else:
                    st.session_state[f"confirm_delete_{i}"] = True
                    st.warning("å†æ¬¡ç‚¹å‡»ç¡®è®¤åˆ é™¤")
        
        edited_scenes.append(scene)
    
    # æ›´æ–°session state
    st.session_state.scenes = edited_scenes
    
    # æ“ä½œæŒ‰é’®
    st.markdown("---")
    col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
    
    with col1:
        if st.button("ğŸ’¾ ä¿å­˜åˆ†æ®µç»“æœ", type="primary"):
            save_scene_segments(video_path, edited_scenes)
    
    with col2:
        if st.button("ğŸ¬ ç”Ÿæˆè§†é¢‘ç‰‡æ®µ"):
            generate_video_segments(video_path, edited_scenes)
    
    with col3:
        if st.button("ğŸ“Š å¯¼å‡ºæ•°æ®"):
            export_scene_data(edited_scenes)
    
    with col4:
        if st.button("ğŸ”„ é‡æ–°æ£€æµ‹"):
            # æ¸…é™¤å½“å‰ç»“æœï¼Œé‡æ–°æ£€æµ‹
            for key in ['scenes', 'selected_video', 'detection_method']:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:06.3f}"

def preview_scene(video_path, scene):
    """é¢„è§ˆåœºæ™¯"""
    st.info(f"ğŸ¬ åœºæ™¯é¢„è§ˆ")
    st.write(f"**æ—¶é—´èŒƒå›´**: {format_time(scene['start_time'])} - {format_time(scene['end_time'])}")
    st.write(f"**æ—¶é•¿**: {scene['end_time'] - scene['start_time']:.2f} ç§’")
    st.write(f"**è¯­ä¹‰ç±»å‹**: {scene['semantic_type']}")
    st.write(f"**ç½®ä¿¡åº¦**: {scene['confidence']:.3f}")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ è§†é¢‘é¢„è§ˆåŠŸèƒ½
    # ç”±äºStreamlitçš„é™åˆ¶ï¼Œæš‚æ—¶æ˜¾ç¤ºä¿¡æ¯
    st.info("ğŸ’¡ æç¤º: è§†é¢‘é¢„è§ˆåŠŸèƒ½å°†åœ¨åç»­ç‰ˆæœ¬ä¸­å®ç°")

def save_scene_segments(video_path, scenes):
    """ä¿å­˜åœºæ™¯åˆ†æ®µç»“æœ"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "data/output/scene_segments"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜åœºæ™¯æ•°æ®
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_file = os.path.join(output_dir, f"{video_name}_scenes.json")
        
        scene_data = {
            "video_path": video_path,
            "video_name": video_name,
            "total_scenes": len(scenes),
            "scenes": scenes
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(scene_data, f, ensure_ascii=False, indent=2)
        
        st.success(f"âœ… åœºæ™¯åˆ†æ®µç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜å¤±è´¥: {e}")
        st.error(f"ä¿å­˜å¤±è´¥: {e}")

def generate_video_segments(video_path, scenes):
    """ç”Ÿæˆè§†é¢‘ç‰‡æ®µ"""
    try:
        processor = VideoProcessor()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_base_dir = "data/output/scene_segments"
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        generated_files = []
        
        for i, scene in enumerate(scenes):
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(scenes)
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨ç”Ÿæˆåœºæ™¯ {i+1}/{len(scenes)}: {scene['semantic_type']}")
            
            # åˆ›å»ºè¯­ä¹‰ç±»å‹ç›®å½•
            semantic_dir = os.path.join(output_base_dir, scene['semantic_type'])
            os.makedirs(semantic_dir, exist_ok=True)
            
            # ç”Ÿæˆç‰‡æ®µ
            segment_file = processor.extract_segment(
                video_path=video_path,
                start_time=scene['start_time'],
                end_time=scene['end_time'],
                segment_index=scene['scene_id'],
                semantic_type=scene['semantic_type'],
                video_id=video_name,
                output_dir=semantic_dir
            )
            
            if segment_file:
                generated_files.append(segment_file)
                logger.info(f"ç”Ÿæˆåœºæ™¯ç‰‡æ®µ: {segment_file}")
        
        progress_bar.progress(1.0)
        status_text.text("âœ… æ‰€æœ‰åœºæ™¯ç‰‡æ®µç”Ÿæˆå®Œæˆï¼")
        
        st.success(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {len(generated_files)} ä¸ªè§†é¢‘ç‰‡æ®µ")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        with st.expander("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨"):
            for file_path in generated_files:
                st.text(file_path)
        
    except Exception as e:
        logger.error(f"è§†é¢‘ç‰‡æ®µç”Ÿæˆå¤±è´¥: {e}")
        st.error(f"è§†é¢‘ç‰‡æ®µç”Ÿæˆå¤±è´¥: {e}")

def export_scene_data(scenes):
    """å¯¼å‡ºåœºæ™¯æ•°æ®"""
    try:
        # åˆ›å»ºDataFrame
        export_data = []
        for scene in scenes:
            export_data.append({
                "åœºæ™¯ID": scene['scene_id'],
                "å¼€å§‹æ—¶é—´": format_time(scene['start_time']),
                "ç»“æŸæ—¶é—´": format_time(scene['end_time']),
                "æ—¶é•¿(ç§’)": round(scene['end_time'] - scene['start_time'], 3),
                "è¯­ä¹‰ç±»å‹": scene['semantic_type'],
                "ç½®ä¿¡åº¦": round(scene['confidence'], 3),
                "æ£€æµ‹æ–¹æ³•": scene.get('method', 'unknown'),
                "æ‰‹åŠ¨è°ƒæ•´": "æ˜¯" if scene.get('manual_adjusted', False) else "å¦"
            })
        
        df = pd.DataFrame(export_data)
        
        # è½¬æ¢ä¸ºCSV
        csv = df.to_csv(index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½CSVæ–‡ä»¶",
            data=csv,
            file_name="scene_segments.csv",
            mime="text/csv"
        )
        
        st.success("âœ… æ•°æ®å¯¼å‡ºå‡†å¤‡å®Œæˆï¼Œç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä¸‹è½½")
        
    except Exception as e:
        logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        st.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")

def test_detection_performance():
    """æµ‹è¯•ä¸åŒæ£€æµ‹æ–¹æ³•çš„æ€§èƒ½"""
    video_files = get_available_videos()
    if not video_files:
        st.warning("æ²¡æœ‰å¯ç”¨çš„è§†é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•")
        return
    
    test_video = video_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘è¿›è¡Œæµ‹è¯•
    
    st.info(f"ğŸ¯ æ­£åœ¨æµ‹è¯•è§†é¢‘: {os.path.basename(test_video)}")
    
    methods = ["ffmpeg", "content", "histogram"]
    results = {}
    
    progress_bar = st.progress(0)
    
    for i, method in enumerate(methods):
        with st.spinner(f"æµ‹è¯• {method} æ–¹æ³•..."):
            import time
            start_time = time.time()
            
            try:
                detector = SceneDetector(threshold=0.3, min_scene_length=1.0)
                scenes = detector.detect_scenes(test_video, method=method)
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                results[method] = {
                    "scenes_count": len(scenes),
                    "processing_time": processing_time,
                    "success": True
                }
                
            except Exception as e:
                results[method] = {
                    "scenes_count": 0,
                    "processing_time": 0,
                    "success": False,
                    "error": str(e)
                }
        
        progress_bar.progress((i + 1) / len(methods))
    
    # æ˜¾ç¤ºç»“æœ
    st.subheader("ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ")
    
    for method, result in results.items():
        if result["success"]:
            st.success(f"**{method.upper()}**: {result['scenes_count']} ä¸ªåœºæ™¯, {result['processing_time']:.2f}ç§’")
        else:
            st.error(f"**{method.upper()}**: å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

def detect_video_scenes_advanced(video_path, threshold, min_scene_length, method, detection_interval=0.1, **kwargs):
    """é«˜çº§åœºæ™¯æ£€æµ‹ï¼Œæ”¯æŒæ›´å¤šé…ç½®é€‰é¡¹"""
    try:
        detector = SceneDetector(
            threshold=threshold, 
            min_scene_length=min_scene_length,
            detection_interval=detection_interval
        )
        
        # æ ¹æ®æ–¹æ³•å’Œé«˜çº§é€‰é¡¹è°ƒæ•´å‚æ•°
        if method == "ffmpeg":
            # å¤„ç†FFmpegé«˜çº§é€‰é¡¹
            use_adaptive_threshold = kwargs.get('use_adaptive_threshold', False)
            enable_motion_detection = kwargs.get('enable_motion_detection', True)
            sensitivity = kwargs.get('scene_detection_sensitivity', 'ä¸­')
            
            # æ ¹æ®æ•æ„Ÿåº¦è°ƒæ•´é˜ˆå€¼
            sensitivity_map = {
                "ä½": threshold * 1.5,
                "ä¸­": threshold,
                "é«˜": threshold * 0.7,
                "æé«˜": threshold * 0.4
            }
            
            adjusted_threshold = sensitivity_map.get(sensitivity, threshold)
            detector.threshold = adjusted_threshold
            
            logger.info(f"FFmpegé«˜çº§æ£€æµ‹: é˜ˆå€¼={adjusted_threshold:.3f}, è‡ªé€‚åº”={use_adaptive_threshold}, è¿åŠ¨æ£€æµ‹={enable_motion_detection}, ç²¾åº¦={detection_interval}s")
        
        scenes = detector.detect_scenes(video_path, method=method)
        
        # ä¸ºæ¯ä¸ªåœºæ™¯æ·»åŠ é»˜è®¤çš„è¯­ä¹‰æ ‡ç­¾
        for i, scene in enumerate(scenes):
            scene['scene_id'] = i + 1
            scene['semantic_type'] = "å…¶ä»–"  # é»˜è®¤æ ‡ç­¾
            scene['manual_adjusted'] = False
        
        return scenes
        
    except Exception as e:
        logger.error(f"é«˜çº§åœºæ™¯æ£€æµ‹å¤±è´¥: {e}")
        st.error(f"é«˜çº§åœºæ™¯æ£€æµ‹å¤±è´¥: {e}")
        return []

def apply_smart_semantic_suggestions(scenes):
    """åº”ç”¨æ™ºèƒ½è¯­ä¹‰å»ºè®®"""
    # åŸºäºåœºæ™¯æ—¶é•¿å’Œä½ç½®çš„ç®€å•å¯å‘å¼è§„åˆ™
    total_scenes = len(scenes)
    
    for i, scene in enumerate(scenes):
        duration = scene['end_time'] - scene['start_time']
        position_ratio = i / total_scenes
        
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        if i == 0:
            scene['semantic_type'] = "å¹¿å‘Šå¼€åœº"
        elif i == total_scenes - 1:
            scene['semantic_type'] = "æ€»ç»“æ”¶å°¾"
        elif duration < 3:
            scene['semantic_type'] = "å…¶ä»–"
        elif position_ratio < 0.3:
            scene['semantic_type'] = "é—®é¢˜é™ˆè¿°"
        elif position_ratio < 0.7:
            scene['semantic_type'] = "äº§å“ä»‹ç»"
        else:
            scene['semantic_type'] = "è¡ŒåŠ¨å·å¬"
        
        scene['manual_adjusted'] = True
    
    st.success("âœ… å·²åº”ç”¨æ™ºèƒ½è¯­ä¹‰å»ºè®®")

def apply_batch_semantic_type(scenes, semantic_type):
    """æ‰¹é‡åº”ç”¨è¯­ä¹‰ç±»å‹"""
    for scene in scenes:
        scene['semantic_type'] = semantic_type
        scene['manual_adjusted'] = True
    
    st.success(f"âœ… å·²å°†æ‰€æœ‰åœºæ™¯è®¾ç½®ä¸º: {semantic_type}")

def reset_all_semantic_types(scenes):
    """é‡ç½®æ‰€æœ‰è¯­ä¹‰ç±»å‹"""
    for scene in scenes:
        scene['semantic_type'] = "å…¶ä»–"
        scene['manual_adjusted'] = False
    
    st.success("âœ… å·²é‡ç½®æ‰€æœ‰è¯­ä¹‰æ ‡ç­¾")

def show_scene_statistics(scenes):
    """æ˜¾ç¤ºåœºæ™¯ç»Ÿè®¡ä¿¡æ¯"""
    st.subheader("ğŸ“ˆ åœºæ™¯ç»Ÿè®¡")
    
    # ç»Ÿè®¡å„è¯­ä¹‰ç±»å‹çš„æ•°é‡
    semantic_counts = {}
    total_duration = 0
    
    for scene in scenes:
        semantic_type = scene['semantic_type']
        duration = scene['end_time'] - scene['start_time']
        
        if semantic_type not in semantic_counts:
            semantic_counts[semantic_type] = {'count': 0, 'duration': 0}
        
        semantic_counts[semantic_type]['count'] += 1
        semantic_counts[semantic_type]['duration'] += duration
        total_duration += duration
    
    # æ˜¾ç¤ºç»Ÿè®¡è¡¨æ ¼
    stats_data = []
    for semantic_type, stats in semantic_counts.items():
        percentage = (stats['duration'] / total_duration) * 100 if total_duration > 0 else 0
        stats_data.append({
            "è¯­ä¹‰ç±»å‹": semantic_type,
            "åœºæ™¯æ•°é‡": stats['count'],
            "æ€»æ—¶é•¿(ç§’)": f"{stats['duration']:.2f}",
            "å æ¯”(%)": f"{percentage:.1f}%"
        })
    
    df = pd.DataFrame(stats_data)
    st.dataframe(df, use_container_width=True)

if __name__ == "__main__":
    main() 