import streamlit as st
import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import pandas as pd
import sys
from pathlib import Path
import yaml
import time
from copy import deepcopy

from utils.config_manager import get_config_manager, CONFIG_PATH

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.absolute()
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ› è°ƒè¯•å·¥å‚ - AIè§†é¢‘æ··å‰ªç³»ç»Ÿ",
    page_icon="ğŸ›",
    layout="wide"
)

def main():
    """è°ƒè¯•å·¥å‚ä¸»ç•Œé¢"""
    
    st.title("ğŸ› è°ƒè¯•å·¥å‚")
    st.markdown("**å®æ—¶æŸ¥çœ‹æ˜ å°„æœºåˆ¶ã€è¿‡æ»¤è§„åˆ™å’Œé€‰ç‰‡è¿‡ç¨‹çš„è¯¦ç»†ä¿¡æ¯**")
    
    # ä¾§è¾¹æ  - è°ƒè¯•é€‰é¡¹
    with st.sidebar:
        st.header("ğŸ”§ è°ƒè¯•é€‰é¡¹")
        
        # åªä¿ç•™ä¸¤ä¸ªè°ƒè¯•æ¨¡å¼
        debug_mode = st.selectbox(
            "é€‰æ‹©è°ƒè¯•æ¨¡å¼",
            ["é€‚é…åˆ†ç±»æœºåˆ¶", "è¯æ±‡ç®¡ç†ä¸­å¿ƒ"],
            index=0,
            help="é€‰æ‹©è¦ä½¿ç”¨çš„è°ƒè¯•åŠŸèƒ½"
        )
        
        st.markdown("---")
        
        if debug_mode == "é€‚é…åˆ†ç±»æœºåˆ¶":
            st.info("ğŸ”§ å½“å‰æ¨¡å¼ï¼šé€‚é…åˆ†ç±»æœºåˆ¶")
        elif debug_mode == "è¯æ±‡ç®¡ç†ä¸­å¿ƒ":
            st.info("ğŸ“š å½“å‰æ¨¡å¼ï¼šè¯æ±‡ç®¡ç†ä¸­å¿ƒ")
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    if debug_mode == "é€‚é…åˆ†ç±»æœºåˆ¶":
        render_debug_classification()
    elif debug_mode == "è¯æ±‡ç®¡ç†ä¸­å¿ƒ":
        render_vocabulary_management()

def render_debug_classification():
    """æ¸²æŸ“è°ƒè¯•åˆ†ç±»æœºåˆ¶ç•Œé¢"""
    st.header("ğŸ”§ é€‚é…åˆ†ç±»æœºåˆ¶")
    st.markdown("**æŒ‰æ¨¡å—åˆ†ç±»ç‰‡æ®µå¹¶ä¿å­˜åˆ°å¯¹åº”æ–‡ä»¶å¤¹ï¼Œä¾¿äºè°ƒè¯•æ˜ å°„æœºåˆ¶æ˜¯å¦æ­£ç¡®**")
    
    # æ£€æŸ¥å¿…è¦çš„session stateæ•°æ®
    mapped_segments = st.session_state.get('mapped_segments', [])
    srt_entries = st.session_state.get('srt_entries', [])
    
    # ğŸ”§ NEW: æ·»åŠ ç‹¬ç«‹çš„ç‰‡æ®µæ‰«æåŠŸèƒ½
    col_scan, col_status = st.columns([1, 2])
    
    with col_scan:
        if st.button("ğŸ”„ é‡æ–°æ‰«æç‰‡æ®µ", help="ç‹¬ç«‹æ‰«ævideo_poolç›®å½•ï¼Œè·å–æœ€æ–°çš„ç‰‡æ®µæ•°æ®"):
            try:
                from modules.mapper import get_cached_mapping_results, resolve_video_pool_path
                
                # å¼ºåˆ¶æ¸…é™¤ç›¸å…³ç¼“å­˜
                st.cache_data.clear()
                
                # ğŸ”§ ä½¿ç”¨è·¨å¹³å°å…¼å®¹çš„è·¯å¾„è§£æ
                video_pool_path = "data/output/google_video/video_pool"
                resolved_path = resolve_video_pool_path(video_pool_path)
                
                with st.spinner("ğŸ”„ æ­£åœ¨æ‰«ævideo_poolç›®å½•..."):
                    logger.info(f"ğŸ”„ å¼€å§‹é‡æ–°æ‰«æç‰‡æ®µï¼Œè·¯å¾„: {resolved_path}")
                    
                    # è°ƒç”¨æ˜ å°„å‡½æ•°è·å–æœ€æ–°æ•°æ®
                    mapped_segments, stats = get_cached_mapping_results(resolved_path)
                    
                    # æ›´æ–°session state
                    st.session_state.mapped_segments = mapped_segments
                    st.session_state.mapping_stats = stats
                    
                    logger.info(f"âœ… é‡æ–°æ‰«æå®Œæˆï¼ŒåŠ è½½äº† {len(mapped_segments)} ä¸ªç‰‡æ®µ")
                    
                    st.success(f"ğŸ‰ æ‰«æå®Œæˆï¼å‘ç° {len(mapped_segments)} ä¸ªç‰‡æ®µ")
                    
                    # æ˜¾ç¤ºæ‰«æç»Ÿè®¡
                    if stats and stats.get("by_video"):
                        st.info(f"ğŸ“Š æŒ‰è§†é¢‘åˆ†å¸ƒ: {dict(list(stats['by_video'].items())[:5])}")
                    
                    # å¼ºåˆ¶åˆ·æ–°é¡µé¢
                    st.rerun()
                    
            except Exception as e:
                logger.error(f"é‡æ–°æ‰«æå¤±è´¥: {e}")
                st.error(f"âŒ æ‰«æå¤±è´¥: {e}")
    
    with col_status:
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        total_segments = len(mapped_segments)
        if total_segments > 0:
            st.success(f"âœ… å·²åŠ è½½æ˜ å°„ç‰‡æ®µ: {total_segments} ä¸ª")
            
            # æ˜¾ç¤ºè§†é¢‘åˆ†å¸ƒ
            if mapped_segments:
                video_distribution = {}
                for segment in mapped_segments:
                    video_id = segment.get('video_id', 'unknown')
                    video_distribution[video_id] = video_distribution.get(video_id, 0) + 1
                
                if len(video_distribution) > 1:
                    st.info(f"ğŸ“Š è§†é¢‘åˆ†å¸ƒ: {dict(list(video_distribution.items())[:3])}{'...' if len(video_distribution) > 3 else ''}")
        else:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æ˜ å°„ç‰‡æ®µæ•°æ®")
            st.info("ğŸ’¡ è¯·ç‚¹å‡»ä¸Šæ–¹çš„ã€Œé‡æ–°æ‰«æç‰‡æ®µã€æŒ‰é’®è·å–æœ€æ–°æ•°æ®")
    
    # SRTæ•°æ®çŠ¶æ€æ£€æŸ¥
    col1, col2 = st.columns(2)
    with col2:
        if srt_entries:
            st.success(f"âœ… å·²åŠ è½½SRTæ•°æ®: {len(srt_entries)} æ¡")
        else:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°SRTæ—¶é—´å‚è€ƒæ•°æ®")
    
    # åŠŸèƒ½è¯´æ˜
    st.info("""
    ğŸ¯ **åŠŸèƒ½è¯´æ˜**:
    1. æ ¹æ®SRTæ—¶é—´æ¯”ä¾‹å’Œæ˜ å°„æœºåˆ¶å¯¹æ‰€æœ‰ç‰‡æ®µè¿›è¡Œåˆ†ç±»
    2. å°†ç‰‡æ®µæŒ‰æ¨¡å—ä¿å­˜åˆ°ã€ç—›ç‚¹ã€‘ã€è§£å†³æ–¹æ¡ˆã€‘ã€å–ç‚¹ã€‘ã€ä¿ƒé”€ã€‘æ–‡ä»¶å¤¹ä¸­
    3. ç”Ÿæˆè¯¦ç»†çš„åˆ†ç±»ç»Ÿè®¡æŠ¥å‘Šï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
    """)
    
    # æ‰§è¡ŒæŒ‰é’®
    debug_disabled = not (mapped_segments and srt_entries)
    
    if st.button("ğŸ”§ æ‰§è¡Œè°ƒè¯•åˆ†ç±»", 
                disabled=debug_disabled, 
                type="primary",
                help="å°†æ‰€æœ‰ç‰‡æ®µæŒ‰æ¨¡å—åˆ†ç±»å¹¶ä¿å­˜åˆ°å¯¹åº”æ–‡ä»¶å¤¹"):
        
        if debug_disabled:
            st.error("âŒ éœ€è¦å…ˆæ‰«æç‰‡æ®µæ•°æ®å¹¶åŠ è½½SRTæ–‡ä»¶")
            return
            
        execute_debug_classification(mapped_segments, srt_entries)
    
    if debug_disabled:
        st.markdown("---")
        st.info("""
        ğŸ“‹ **ä½¿ç”¨æ­¥éª¤**:
        1. ç‚¹å‡»ä¸Šæ–¹ã€Œé‡æ–°æ‰«æç‰‡æ®µã€æŒ‰é’®åŠ è½½æœ€æ–°æ•°æ®
        2. å‰å¾€ ğŸ§ª æ··å‰ªå·¥å‚ åŠ è½½æ ‡æ†è§†é¢‘SRTæ–‡ä»¶
        3. è¿”å›æ­¤å¤„æ‰§è¡Œè°ƒè¯•åˆ†ç±»
        """)
    
    # ğŸ”§ NEW: æ˜¾ç¤ºå½“å‰çŠ¶æ€æ±‡æ€»
    if mapped_segments or srt_entries:
        st.markdown("---")
        st.markdown("### ğŸ“Š å½“å‰çŠ¶æ€æ±‡æ€»")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ˜ å°„ç‰‡æ®µ", len(mapped_segments))
        with col2:
            st.metric("SRTæ¡ç›®", len(srt_entries))
        with col3:
            ready_status = "âœ… å°±ç»ª" if (mapped_segments and srt_entries) else "âš ï¸ æœªå°±ç»ª"
            st.metric("è°ƒè¯•çŠ¶æ€", ready_status)

def apply_global_filters(segments: List[Dict]) -> List[Dict]:
    """åº”ç”¨å…¨å±€æ’é™¤è¿‡æ»¤è§„åˆ™"""
    
    # ğŸ”§ ä½¿ç”¨ConfigManageræ›¿ä»£ç›´æ¥è¯»å–matching_rules.json
    try:
        from utils.config_manager import get_config_manager
        config_manager = get_config_manager()
        raw_config = config_manager.get_raw_config()
        
        # ä»ç»Ÿä¸€é…ç½®ä¸­æå–å…¨å±€è®¾ç½®
        global_settings = raw_config.get("global_settings", {})
        global_exclude_keywords = global_settings.get("global_exclusion_keywords", [])
        max_segments_per_module = global_settings.get("max_segments_per_module", 3)
        max_duration = global_settings.get("max_duration_seconds", 10)
        
        logger.info(f"âœ… æˆåŠŸä»ConfigManageråŠ è½½å…¨å±€è®¾ç½®")
        
    except Exception as e:
        logger.warning(f"æ— æ³•ä»ConfigManageråŠ è½½å…¨å±€è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        global_exclude_keywords = ["ç–‘ä¼¼", "æ¨¡ç³Š", "ä¸æ¸…æ¥š"]
        max_segments_per_module = 3
        max_duration = 10
    
    # è´¨é‡é˜ˆå€¼ 
    min_quality_threshold = 0.3
    
    filtered_segments = []
    
    for segment in segments:
        # æ£€æŸ¥æ—¶é•¿
        duration = segment.get('duration', 0)
        if duration > max_duration:
            logger.info(f"ğŸ•’ æ—¶é•¿è¿‡æ»¤: {segment.get('file_name', '')} (æ—¶é•¿{duration:.1f}s > é™åˆ¶{max_duration}s)")
            continue
            
        # æ£€æŸ¥è´¨é‡åˆ†æ•°
        quality = segment.get('combined_quality', 0)
        if quality < min_quality_threshold:
            logger.info(f"ğŸ“Š è´¨é‡è¿‡æ»¤: {segment.get('file_name', '')} (è´¨é‡{quality:.2f} < é˜ˆå€¼{min_quality_threshold})")
            continue
            
        # æ£€æŸ¥å…¨å±€æ’é™¤å…³é”®è¯
        all_tags = segment.get('all_tags', [])
        transcription = segment.get('transcription', '')
        
        excluded = False
        excluding_keywords = []
        
        for keyword in global_exclude_keywords:
            # æ£€æŸ¥æ ‡ç­¾ - å¢åŠ ç±»å‹å®‰å…¨æ£€æŸ¥
            for tag in all_tags:
                if tag is None:
                    continue
                tag_str = tag if isinstance(tag, str) else str(tag)
                if keyword and isinstance(keyword, str) and keyword.lower() in tag_str.lower():
                    excluded = True
                    excluding_keywords.append(keyword)
                    break
            
            # æ£€æŸ¥è½¬å½•æ–‡æœ¬ - å¢åŠ ç±»å‹å®‰å…¨æ£€æŸ¥
            if transcription is not None and isinstance(transcription, str) and keyword and isinstance(keyword, str):
                if keyword.lower() in transcription.lower():
                    excluded = True
                    excluding_keywords.append(keyword)
                        
        if excluded:
            logger.info(f"ğŸš« å…³é”®è¯æ’é™¤: {segment.get('file_name', '')} (å…³é”®è¯: {excluding_keywords})")
            continue
        
        # é€šè¿‡æ‰€æœ‰è¿‡æ»¤å™¨
        filtered_segments.append(segment)
    
    return filtered_segments

def limit_segments_per_module(classification_result: Dict, max_per_module: int = None) -> Dict:
    """é™åˆ¶æ¯ä¸ªæ¨¡å—çš„ç‰‡æ®µæ•°é‡"""
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥é™åˆ¶æ•°é‡ï¼Œä»ConfigManagerè¯»å–
    if max_per_module is None:
        try:
            from utils.config_manager import get_config_manager
            config_manager = get_config_manager()
            raw_config = config_manager.get_raw_config()
            max_per_module = raw_config.get("global_settings", {}).get("max_segments_per_module", 3)
        except Exception as e:
            logger.warning(f"æ— æ³•ä»ConfigManagerè¯»å–æ¨¡å—æ•°é‡é™åˆ¶ï¼Œä½¿ç”¨é»˜è®¤å€¼3: {e}")
            max_per_module = 3
    
    for module_name, stats in classification_result.get("module_stats", {}).items():
        if stats["saved_segments"] > max_per_module:
            st.warning(f"âš ï¸ æ¨¡å— '{module_name}' æœ‰ {stats['saved_segments']} ä¸ªç‰‡æ®µï¼Œè¶…è¿‡é™åˆ¶ {max_per_module} ä¸ª")
            st.info(f"ğŸ’¡ å»ºè®®ï¼šè°ƒæ•´å…³é”®è¯åŒ¹é…è§„åˆ™æˆ–æé«˜è´¨é‡é˜ˆå€¼æ¥å‡å°‘ç‰‡æ®µæ•°é‡")
    
    return classification_result

def execute_debug_classification(mapped_segments: List[Dict], srt_entries: List[Dict]):
    """æ‰§è¡Œè°ƒè¯•åˆ†ç±»ï¼šæŒ‰æ¨¡å—åˆ†ç±»ç‰‡æ®µå¹¶ä¿å­˜åˆ°å¯¹åº”æ–‡ä»¶å¤¹"""
    try:
        # å¯¼å…¥è°ƒè¯•åˆ†ç±»å™¨
        from modules.debug_classifier import DebugClassifier
        
        # ä»å›¾ç‰‡æ˜¾ç¤ºçš„æ¯”ä¾‹è®¡ç®—ç›®æ ‡æ¯”ä¾‹
        target_ratios = [25, 21, 49, 4]  # ç—›ç‚¹, è§£å†³æ–¹æ¡ˆ, å–ç‚¹, ä¿ƒé”€ (æ ¹æ®ç”¨æˆ·å›¾ç‰‡)
        
        # ğŸ†• æ·»åŠ å…¨å±€æ’é™¤è¿‡æ»¤
        st.markdown("### ğŸš« åº”ç”¨å…¨å±€æ’é™¤è§„åˆ™")
        
        # é¢„è¿‡æ»¤ç‰‡æ®µ
        filtered_segments = apply_global_filters(mapped_segments)
        
        # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡
        original_count = len(mapped_segments)
        filtered_count = len(filtered_segments)
        excluded_count = original_count - filtered_count
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("åŸå§‹ç‰‡æ®µ", original_count)
        with col2:
            st.metric("é€šè¿‡è¿‡æ»¤", filtered_count)
        with col3:
            st.metric("è¢«æ’é™¤", excluded_count, delta=f"-{excluded_count}")
        
        if excluded_count > 0:
            st.warning(f"âš ï¸ æœ‰ {excluded_count} ä¸ªç‰‡æ®µè¢«å…¨å±€æ’é™¤è§„åˆ™è¿‡æ»¤")
        
        with st.spinner("ğŸ”§ æ­£åœ¨æŒ‰æ¨¡å—åˆ†ç±»ç‰‡æ®µ..."):
            # åˆ›å»ºè°ƒè¯•åˆ†ç±»å™¨
            debug_classifier = DebugClassifier()
        
            # æ‰§è¡Œåˆ†ç±»ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„ç‰‡æ®µï¼‰
            classification_result = debug_classifier.classify_and_save_segments_by_srt_timing(
                mapped_segments=filtered_segments,
                srt_entries=srt_entries,
                target_ratios=target_ratios
            )
            
            # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
            st.success("âœ… è°ƒè¯•åˆ†ç±»å®Œæˆï¼")
        
            # ğŸ†• æ£€æŸ¥æ¨¡å—æ•°é‡é™åˆ¶
            classification_result = limit_segments_per_module(classification_result)
            
            # å±•ç¤ºç»Ÿè®¡ç»“æœ
            st.markdown("### ğŸ“Š åˆ†ç±»ç»Ÿè®¡ç»“æœ")
            col1, col2, col3 = st.columns(3)
    
            with col1:
                st.metric("åŸå§‹æ€»æ•°", original_count)
    
            with col2:
                st.metric("è¿‡æ»¤åæ•°", filtered_count)
    
            with col3:
                st.metric("æœ€ç»ˆåˆ†ç±»", classification_result["classified_segments"])
    
            # åˆ†ç±»æˆåŠŸç‡
            success_rate = (classification_result["classified_segments"] / 
                          max(filtered_count, 1)) * 100
            st.metric("åˆ†ç±»æˆåŠŸç‡", f"{success_rate:.1f}%", 
                     help="åŸºäºè¿‡æ»¤åç‰‡æ®µçš„åˆ†ç±»æˆåŠŸç‡")
            
            # å„æ¨¡å—è¯¦ç»†ç»Ÿè®¡
            st.markdown("#### ğŸ“ å„æ¨¡å—åˆ†ç±»ç»“æœ")
            
            for module_name, stats in classification_result["module_stats"].items():
                folder_name = debug_classifier.module_folders.get(module_name, module_name)
                
                col1, col2, col3, col4 = st.columns(4)
    
                with col1:
                    st.markdown(f"**ğŸ“ {folder_name}**")
    
                with col2:
                    st.metric("ç‰‡æ®µæ•°", stats["saved_segments"])
                
                with col3:
                    st.metric("å®é™…æ—¶é•¿", f"{stats['actual_time']:.1f}s")
                
                with col4:
                    st.metric("ç›®æ ‡æ—¶é•¿", f"{stats['target_time']:.1f}s")
                
                # æ˜¾ç¤ºæ–‡ä»¶å¤¹è·¯å¾„
                st.code(stats["folder_path"], language=None)
            
            # æ˜¾ç¤ºSRTæ—¶é—´åˆ†é…
            st.markdown("### â±ï¸ SRTæ—¶é—´åˆ†é…å‚è€ƒ")
            st.markdown("#### ğŸ“Š åŸºäºSRTçš„æ¨¡å—æ—¶é—´åˆ†é…")
            
            for module, time_range in classification_result["srt_time_ranges"].items():
                folder_name = debug_classifier.module_folders.get(module, module)
                st.markdown(
                    f"**{folder_name}**: {time_range['start']:.1f}s - {time_range['end']:.1f}s "
                    f"(æ—¶é•¿: {time_range['duration']:.1f}s, æ¯”ä¾‹: {time_range['ratio']}%)"
                )
            
            # æ˜¾ç¤ºå…¨å±€æ’é™¤è¯¦æƒ…
            if excluded_count > 0:
                st.markdown("### ğŸš« å…¨å±€æ’é™¤è¯¦æƒ…")
                with st.expander(f"æŸ¥çœ‹è¢«æ’é™¤çš„ {excluded_count} ä¸ªç‰‡æ®µ", expanded=False):
                    excluded_segments = [seg for seg in mapped_segments if seg not in filtered_segments]
                    
                    for i, segment in enumerate(excluded_segments[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                        col1, col2 = st.columns([2, 3])
                        
                        with col1:
                            st.write(f"**{segment.get('file_name', 'Unknown')}**")
                            st.write(f"æ—¶é•¿: {segment.get('duration', 0):.1f}s")
                            st.write(f"è´¨é‡: {segment.get('combined_quality', 0):.2f}")
                        
                        with col2:
                            tags = segment.get('all_tags', [])
                            if tags:
                                st.write(f"æ ‡ç­¾: {', '.join(tags[:3])}{'...' if len(tags) > 3 else ''}")
                            transcription = segment.get('transcription', '')
                            if transcription:
                                st.write(f"è½¬å½•: {transcription[:50]}{'...' if len(transcription) > 50 else ''}")
                
                    if len(excluded_segments) > 10:
                        st.info(f"è¿˜æœ‰ {len(excluded_segments) - 10} ä¸ªç‰‡æ®µæœªæ˜¾ç¤º...")
            
            st.markdown("---")
            
            # ä¸‹ä¸€æ­¥å»ºè®®
            st.info("""
            ğŸ¯ **è°ƒè¯•å»ºè®®**:
            1. æ£€æŸ¥å„æ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘ç‰‡æ®µæ˜¯å¦ç¬¦åˆé¢„æœŸæ¨¡å—ç±»å‹
            2. æŸ¥çœ‹ç‰‡æ®µä¿¡æ¯JSONæ–‡ä»¶äº†è§£åˆ†ç±»åŸå› 
            3. å¦‚å‘ç°åˆ†ç±»é”™è¯¯ï¼Œå¯è°ƒæ•´é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®è¯
            4. å¦‚æœæŸä¸ªæ¨¡å—ç‰‡æ®µè¿‡å¤šï¼Œè€ƒè™‘æé«˜è¯¥æ¨¡å—çš„è´¨é‡é˜ˆå€¼
            5. æ£€æŸ¥å…¨å±€æ’é™¤è§„åˆ™æ˜¯å¦è¿‡äºä¸¥æ ¼
            6. é‡æ–°è¿è¡Œè°ƒè¯•åˆ†ç±»éªŒè¯ä¼˜åŒ–æ•ˆæœ
            """)
            
    except ImportError:
        st.error("âŒ è°ƒè¯•åˆ†ç±»å™¨æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
    except Exception as e:
        logger.error(f"è°ƒè¯•åˆ†ç±»æ‰§è¡Œå¤±è´¥: {e}")
        st.error(f"âŒ è°ƒè¯•åˆ†ç±»å¤±è´¥: {e}")

def auto_save_config(config, save_msg_container=None):
    """è‡ªåŠ¨ä¿å­˜é…ç½®å¹¶æ˜¾ç¤ºæˆåŠŸæé†’"""
    try:
        with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True, sort_keys=False, indent=2)
        
        if save_msg_container:
            with save_msg_container:
                st.success("âœ… ä¿å­˜æˆåŠŸï¼", icon="âœ…")
        
        # æ¸…é™¤ç¼“å­˜
        st.cache_data.clear()
        st.cache_resource.clear()
        
        return True
    except Exception as e:
        if save_msg_container:
            with save_msg_container:
                st.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return False

def handle_input_change(config, module_key, save_container):
    """å¤„ç†è¾“å…¥å˜æ›´çš„å³æ—¶ä¿å­˜"""
    auto_save_config(config, save_container)

def create_input_change_callback(config, module_key, save_container):
    """åˆ›å»ºè¾“å…¥å˜æ›´å›è°ƒå‡½æ•°"""
    def callback():
        auto_save_config(config, save_container)
    return callback

def handle_delete_item(config, module_key, category, index, save_container):
    """å¤„ç†åˆ é™¤é¡¹ç›®çš„å³æ—¶ä¿å­˜"""
    try:
        if "ai_batch" in config[module_key] and category in config[module_key]["ai_batch"]:
            items = config[module_key]["ai_batch"][category]
            if 0 <= index < len(items):
                items.pop(index)
                auto_save_config(config, save_container)
    except Exception as e:
        st.error(f"åˆ é™¤å¤±è´¥: {e}")

def handle_add_item(config, module_key, category, save_container):
    """å¤„ç†æ·»åŠ é¡¹ç›®çš„å³æ—¶ä¿å­˜"""
    try:
        if "ai_batch" not in config[module_key]:
            config[module_key]["ai_batch"] = {}
        if category not in config[module_key]["ai_batch"]:
            config[module_key]["ai_batch"][category] = []
        
        config[module_key]["ai_batch"][category].append({"word": "", "weight": 2})
        auto_save_config(config, save_container)
    except Exception as e:
        st.error(f"æ·»åŠ å¤±è´¥: {e}")

def render_vocabulary_management():
    """ğŸ“š è¯æ±‡ç®¡ç†ï¼šç¼–è¾‘ä¸šåŠ¡è“å›¾ï¼ŒåŠ¨æ€ç”ŸæˆAIé…ç½®"""
    st.subheader("ğŸ“š è¯æ±‡ç®¡ç† - ä¸šåŠ¡è“å›¾ç¼–è¾‘å™¨")
    
    # ğŸ†• æ˜¾ç¤ºå½“å‰é…ç½®æ¨¡å¼å’Œç»Ÿè®¡
    st.markdown("### ğŸ“Š å½“å‰é…ç½®çŠ¶æ€")
    
    try:
        config_manager = get_config_manager()
        vocab = config_manager.get_ai_vocabulary()
        stats = config_manager.get_ai_statistics()
        supports_batch = config_manager.supports_batch_definition()
        
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col1:
            mode_color = "ğŸŸ¢" if supports_batch else "ğŸŸ¡"
            mode_text = "æ‰¹é‡å®šä¹‰æ¨¡å¼" if supports_batch else "ä¼ ç»Ÿæ˜ å°„æ¨¡å¼"
            st.metric("é…ç½®æ¨¡å¼", mode_text, delta=mode_color)
        
        with col2:
            total_words = sum(stats.values())
            st.metric("æ€»è¯æ±‡æ•°", total_words)
        
        col_stat = st.columns(4)
        total_words = sum(stats.values())
        categories = ["object", "scene", "emotion", "brand"]
        category_names = ["Object", "Scene", "Emotion", "Brand"]
        category_colors = ["ğŸ¯", "ğŸï¸", "ğŸ’­", "ğŸ·ï¸"]
        for i, (cat, name, color) in enumerate(zip(categories, category_names, category_colors)):
            with col_stat[i]:
                ratio = stats[cat] / total_words * 100 if total_words > 0 else 0
                # ä¸šåŠ¡å»ºè®®ï¼šåœºæ™¯<20%æ˜¾ç¤ºğŸ¯ä¼˜åŒ–ç›®æ ‡ï¼Œå…¶å®ƒç±»åˆ«<20%æ˜¾ç¤ºâš ï¸å»ºè®®è¡¥å……
                if cat == "scene":
                    delta = "ğŸ¯ ä¼˜åŒ–ç›®æ ‡" if ratio < 20 else "âœ… è‰¯å¥½"
                else:
                    delta = "âš ï¸ å»ºè®®è¡¥å……" if ratio < 20 else "âœ… è‰¯å¥½"
                st.metric(f"{color} {name} å æ¯”", f"{ratio:.1f}%", delta=delta)
        # ä¿ç•™åŸæœ‰åˆ†å¸ƒè¯¦æƒ…
        st.markdown("**AIè¯æ±‡åˆ†å¸ƒè¯¦æƒ…:**")
        distribution_cols = st.columns(4)
        for i, (category, color) in enumerate(zip(categories, category_colors)):
            with distribution_cols[i]:
                count = stats[category]
                st.metric(f"{color} {category_names[i]}", count)
        
        if supports_batch:
            st.success("âœ… **æ­£åœ¨ä½¿ç”¨æ‰¹é‡å®šä¹‰**: æ‚¨å¯ä»¥ç²¾ç¡®æ§åˆ¶æ¯ä¸ªè¯æ±‡çš„AIç±»åˆ«å½’å±")
        else:
            st.warning("âš ï¸ **æ­£åœ¨ä½¿ç”¨ä¼ ç»Ÿæ˜ å°„**: ç³»ç»ŸæŒ‰é¢„è®¾è§„åˆ™è‡ªåŠ¨åˆ†é…AIç±»åˆ«")
        
        st.markdown("---")
        
    except Exception as e:
        st.error(f"âŒ è·å–é…ç½®çŠ¶æ€å¤±è´¥: {e}")
    
    st.markdown("""
    ğŸ¯ **å•ä¸€æ•°æ®æº**: è¿™é‡Œæ˜¯ä¸šåŠ¡é€»è¾‘çš„å”¯ä¸€çœŸå®æ¥æºã€‚
    - **æ‚¨ç¼–è¾‘çš„æ˜¯**: `keywords.yml` æ–‡ä»¶ä¸­çš„ä¸šåŠ¡æ¦‚å¿µã€‚
    - **ç³»ç»Ÿè‡ªåŠ¨å¤„ç†**: æ ¹æ®æ‚¨çš„å®šä¹‰ï¼ŒåŠ¨æ€ç”ŸæˆAIè¯æ±‡è¡¨å’Œåˆ†ç±»è§„åˆ™ã€‚
    - **ä¸€å¤„ä¿®æ”¹ï¼Œå…¨å±€ç”Ÿæ•ˆ**: ä¿®æ”¹å¹¶ä¿å­˜åï¼Œæ•´ä¸ªç³»ç»Ÿå°†ç«‹å³é‡‡ç”¨æ–°è§„åˆ™ã€‚

    ### ğŸ†• é…ç½®æ–¹å¼è¯´æ˜

    **ğŸ”¹ æ‰¹é‡å®šä¹‰ (ai_batch)** - æ¨èä½¿ç”¨
    - âœ… **ç²¾ç¡®æ§åˆ¶**: ç›´æ¥æŒ‡å®šæ¯ä¸ªè¯æ±‡å±äºå“ªä¸ªAIç±»åˆ«
    - âœ… **çµæ´»æ˜ å°„**: ä¸å—ä¼ ç»Ÿä¸šåŠ¡æ¦‚å¿µé™åˆ¶
    - âœ… **ä¼˜åŒ–åˆ†å¸ƒ**: å¯ä»¥æ‰‹åŠ¨å¹³è¡¡å„AIç±»åˆ«çš„è¯æ±‡æ•°é‡
    """)

    try:
        config_manager = get_config_manager()
        config = config_manager.get_raw_config()
        if not config:
            st.error("âŒ æ— æ³•åŠ è½½ä¸šåŠ¡è“å›¾ (keywords.yml)ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
            return
    except Exception as e:
        st.error(f"âŒ åŠ è½½é…ç½®æ—¶å‡ºé”™: {e}")
        return

    # åˆå§‹åŒ– session_state
    if 'vocab_config' not in st.session_state:
        st.session_state['vocab_config'] = deepcopy(config)
    vocab_config = st.session_state['vocab_config']

    # --- æ¨¡å—åŒ–ç¼–è¾‘ç•Œé¢ ---
    
    # ğŸ­ é¦–å…ˆæ˜¾ç¤º Shared æ¨¡å— (ç»Ÿä¸€é…ç½®)
    with st.expander("ğŸ­ Sharedæ¨¡å—ï¼šæƒ…ç»ªè¯åº“é…ç½® (è§†è§‰+éŸ³é¢‘AIå…±ç”¨)", expanded=True):
        shared_data = vocab_config.get("shared", {})
        st.markdown("### ğŸŒ ç»Ÿä¸€æƒ…ç»ªé…ç½®")
        st.info("**è¯´æ˜**: è¿™äº›æƒ…ç»ªè¯æ±‡å°†è¢«æ‰€æœ‰AIæ¨¡å‹å…±äº«ä½¿ç”¨ï¼ŒåŒ…æ‹¬Qwenè§†è§‰åˆ†æå’ŒDeepSeekéŸ³é¢‘åˆ†æ")
        
        ai_batch = shared_data.get("ai_batch", {})
        
        # åªæ˜¾ç¤º Emotion ç±»åˆ«ï¼Œå› ä¸ºsharedä¸»è¦ç”¨äºæƒ…ç»ªè¯æ±‡
        emotions_batch = ai_batch.get("emotion", [])
        st.markdown("#### ğŸ’­ Emotion (æƒ…ç»ªè¯æ±‡) - æ”¯æŒæƒé‡")
        
        col1, col2 = st.columns(2)
        
        # åˆ†ä¸¤åˆ—æ˜¾ç¤ºæƒ…ç»ªè¯æ±‡
        for i, item in enumerate(emotions_batch):
            word = item.get("word", "")
            weight = item.get("weight", 2)
            
            # äº¤æ›¿æ˜¾ç¤ºåœ¨ä¸¤åˆ—ä¸­
            target_col = col1 if i % 2 == 0 else col2
            
            with target_col:
                col_word, col_weight, col_del = st.columns([3,2,1])
                with col_word:
                    new_word = st.text_input(f"æƒ…ç»ªè¯æ±‡", value=word, key=f"shared_emotion_word_{i}")
                    if new_word != word:
                        emotions_batch[i]["word"] = new_word
                with col_weight:
                    new_weight = st.slider("æƒé‡", 1, 3, value=weight, key=f"shared_emotion_weight_{i}")
                    if new_weight != weight:
                        emotions_batch[i]["weight"] = new_weight
                with col_del:
                    if st.button("åˆ é™¤", key=f"shared_emotion_delete_{i}"):
                        emotions_batch.pop(i)
                        st.rerun()
        
        # æ·»åŠ æ–°æƒ…ç»ªè¯æ±‡æŒ‰é’®
        if st.button("â• æ·»åŠ æ–°æƒ…ç»ªè¯æ±‡", key="shared_emotion_add"):
            emotions_batch.append({"word": "", "weight": 2})
            st.rerun()
        
        # æ›´æ–°é…ç½®
        if "shared" not in vocab_config:
            vocab_config["shared"] = {}
        if "ai_batch" not in vocab_config["shared"]:
            vocab_config["shared"]["ai_batch"] = {}
        vocab_config["shared"]["ai_batch"]["emotion"] = emotions_batch
        
        # ä¸ºäº†ä¿æŒå…¶ä»–AIç±»åˆ«çš„ç©ºåˆ—è¡¨
        for category in ["object", "scene", "brand"]:
            if category not in vocab_config["shared"]["ai_batch"]:
                vocab_config["shared"]["ai_batch"][category] = []
        
        # æ˜¾ç¤ºç»Ÿè®¡
        valid_emotions = [item for item in emotions_batch if item.get("word", "").strip()]
        if valid_emotions:
            st.success(f"âœ… å·²é…ç½® {len(valid_emotions)} ä¸ªæƒ…ç»ªè¯æ±‡ï¼Œå°†ç”¨äºæ‰€æœ‰AIæ¨¡å‹")
        else:
            st.warning("âš ï¸ å»ºè®®è‡³å°‘é…ç½®10-20ä¸ªæƒ…ç»ªè¯æ±‡ä»¥è·å¾—æœ€ä½³AIè¯†åˆ«æ•ˆæœ")

    st.markdown("---")

    module_mapping = {
        "pain_points": "æ¨¡å—ä¸€ï¼šç—›ç‚¹ (Pain Points)",
        "solutions": "æ¨¡å—äºŒï¼šè§£å†³æ–¹æ¡ˆå¯¼å…¥ (Solutions)",
        "features_formula": "æ¨¡å—ä¸‰ï¼šå–ç‚¹Â·æˆåˆ†&é…æ–¹ (Features & Formula)",
        "promotions": "æ¨¡å—å››ï¼šä¿ƒé”€æœºåˆ¶ (Promotions)"
    }
    for key, title in module_mapping.items():
        with st.expander(title, expanded=False):
            module_data = vocab_config.get(key, {})
            st.markdown("### ğŸ†• æ‰¹é‡å®šä¹‰ (AI Batch) - ç²¾ç¡®æ§åˆ¶AIç±»åˆ«")
            ai_batch = module_data.get("ai_batch", {})
            col1, col2 = st.columns(2)
            with col1:
                # Object
                objects_batch = ai_batch.get("object", [])
                st.markdown("#### ğŸ¯ Object (ç‰©ä½“/è¡Œä¸º) - æ”¯æŒæƒé‡")
                for i, item in enumerate(objects_batch):
                    word = item.get("word", "")
                    weight = item.get("weight", 2)
                    col_word, col_weight, col_del = st.columns([3,2,1])
                    with col_word:
                        new_word = st.text_input(f"è¯æ±‡{i}", value=word, key=f"{key}_object_word_{i}")
                        if new_word != word:
                            objects_batch[i]["word"] = new_word
                    with col_weight:
                        new_weight = st.slider("æƒé‡", 1, 3, value=weight, key=f"{key}_object_weight_{i}")
                        if new_weight != weight:
                            objects_batch[i]["weight"] = new_weight
                    with col_del:
                        if st.button("åˆ é™¤", key=f"{key}_object_delete_{i}"):
                            objects_batch.pop(i)
                            st.rerun()
                if st.button("æ·»åŠ æ–°è¯", key=f"{key}_object_add"):
                    objects_batch.append({"word": "", "weight": 2})
                    st.rerun()
                if "ai_batch" not in vocab_config[key]: vocab_config[key]["ai_batch"] = {}
                vocab_config[key]["ai_batch"]["object"] = objects_batch
            with col2:
                # Emotion
                emotions_batch = ai_batch.get("emotion", [])
                st.markdown("#### ğŸ’­ Emotion (æƒ…ç»ª/ä»·å€¼) - æ”¯æŒæƒé‡")
                for i, item in enumerate(emotions_batch):
                    word = item.get("word", "")
                    weight = item.get("weight", 2)
                    col_word, col_weight, col_del = st.columns([3,2,1])
                    with col_word:
                        new_word = st.text_input(f"æƒ…ç»ªè¯æ±‡{i}", value=word, key=f"{key}_emotion_word_{i}")
                        if new_word != word:
                            emotions_batch[i]["word"] = new_word
                    with col_weight:
                        new_weight = st.slider("æƒé‡", 1, 3, value=weight, key=f"{key}_emotion_weight_{i}")
                        if new_weight != weight:
                            emotions_batch[i]["weight"] = new_weight
                    with col_del:
                        if st.button("åˆ é™¤", key=f"{key}_emotion_delete_{i}"):
                            emotions_batch.pop(i)
                            st.rerun()
                if st.button("æ·»åŠ æ–°æƒ…ç»ª", key=f"{key}_emotion_add"):
                    emotions_batch.append({"word": "", "weight": 2})
                    st.rerun()
                if "ai_batch" not in vocab_config[key]: vocab_config[key]["ai_batch"] = {}
                vocab_config[key]["ai_batch"]["emotion"] = emotions_batch
            with col1:
                # Scene
                scenes_batch = ai_batch.get("scene", [])
                st.markdown("#### ğŸï¸ Scene (åœºæ™¯/ç¯å¢ƒ) - æ”¯æŒæƒé‡")
                for i, item in enumerate(scenes_batch):
                    word = item.get("word", "")
                    weight = item.get("weight", 2)
                    col_word, col_weight, col_del = st.columns([3,2,1])
                    with col_word:
                        new_word = st.text_input(f"åœºæ™¯è¯æ±‡{i}", value=word, key=f"{key}_scene_word_{i}")
                        if new_word != word:
                            scenes_batch[i]["word"] = new_word
                    with col_weight:
                        new_weight = st.slider("æƒé‡", 1, 3, value=weight, key=f"{key}_scene_weight_{i}")
                        if new_weight != weight:
                            scenes_batch[i]["weight"] = new_weight
                    with col_del:
                        if st.button("åˆ é™¤", key=f"{key}_scene_delete_{i}"):
                            scenes_batch.pop(i)
                            st.rerun()
                if st.button("æ·»åŠ æ–°åœºæ™¯", key=f"{key}_scene_add"):
                    scenes_batch.append({"word": "", "weight": 2})
                    st.rerun()
                if "ai_batch" not in vocab_config[key]: vocab_config[key]["ai_batch"] = {}
                vocab_config[key]["ai_batch"]["scene"] = scenes_batch
            with col2:
                # Brand
                brands_batch = ai_batch.get("brand", [])
                st.markdown("#### ğŸ·ï¸ Brand (å“ç‰Œæ ‡è¯†) - æ”¯æŒæƒé‡")
                for i, item in enumerate(brands_batch):
                    word = item.get("word", "")
                    weight = item.get("weight", 2)
                    col_word, col_weight, col_del = st.columns([3,2,1])
                    with col_word:
                        new_word = st.text_input(f"å“ç‰Œè¯æ±‡{i}", value=word, key=f"{key}_brand_word_{i}")
                        if new_word != word:
                            brands_batch[i]["word"] = new_word
                    with col_weight:
                        new_weight = st.slider("æƒé‡", 1, 3, value=weight, key=f"{key}_brand_weight_{i}")
                        if new_weight != weight:
                            brands_batch[i]["weight"] = new_weight
                    with col_del:
                        if st.button("åˆ é™¤", key=f"{key}_brand_delete_{i}"):
                            brands_batch.pop(i)
                            st.rerun()
                if st.button("æ·»åŠ æ–°å“ç‰Œ", key=f"{key}_brand_add"):
                    brands_batch.append({"word": "", "weight": 2})
                    st.rerun()
                if "ai_batch" not in vocab_config[key]: vocab_config[key]["ai_batch"] = {}
                vocab_config[key]["ai_batch"]["brand"] = brands_batch
            
            # ğŸ†• Negative Keywords ç¼–è¾‘åŒºåŸŸ
            st.markdown("#### ğŸš« Negative Keywords (æ’é™¤å…³é”®è¯)")
            negative_keywords = module_data.get("negative_keywords", [])
            
            col_neg1, col_neg2 = st.columns([4, 1])
            with col_neg1:
                st.markdown("**æ’é™¤å…³é”®è¯åˆ—è¡¨** - åŒ…å«è¿™äº›è¯çš„ç‰‡æ®µå°†ä¸ä¼šè¢«åˆ†ç±»åˆ°æœ¬æ¨¡å—")
                for i, neg_word in enumerate(negative_keywords):
                    col_word, col_del = st.columns([4, 1])
                    with col_word:
                        new_neg_word = st.text_input(f"æ’é™¤è¯{i}", value=neg_word, key=f"{key}_negative_{i}")
                        if new_neg_word != neg_word:
                            negative_keywords[i] = new_neg_word
                    with col_del:
                        if st.button("åˆ é™¤", key=f"{key}_negative_delete_{i}"):
                            negative_keywords.pop(i)
                            st.rerun()
            
            with col_neg2:
                if st.button("æ·»åŠ æ’é™¤è¯", key=f"{key}_negative_add"):
                    negative_keywords.append("")
                    st.rerun()
            
            vocab_config[key]["negative_keywords"] = negative_keywords
            
            # ğŸ†• æ˜¾ç¤ºæ’é™¤å…³é”®è¯è¯´æ˜
            if negative_keywords:
                valid_negatives = [neg for neg in negative_keywords if neg.strip()]
                if valid_negatives:
                    st.success(f"âœ… å·²é…ç½® {len(valid_negatives)} ä¸ªæ’é™¤å…³é”®è¯ï¼Œå°†è¿‡æ»¤ç›¸å…³ç‰‡æ®µ")
                else:
                    st.warning("âš ï¸ æ’é™¤å…³é”®è¯åˆ—è¡¨ä¸ºç©ºï¼Œå»ºè®®æ·»åŠ ä»¥æé«˜åˆ†ç±»ç²¾åº¦")
            
            if ai_batch:
                st.info("â„¹ï¸ **ä½¿ç”¨æ‰¹é‡å®šä¹‰**: æ­¤æ¨¡å—å°†ä¼˜å…ˆä½¿ç”¨ä¸Šé¢çš„æ‰¹é‡å®šä¹‰ï¼Œä¼ ç»Ÿå­—æ®µå°†è¢«å¿½ç•¥ã€‚")
            st.markdown("---")

    # --- å…¨å±€é…ç½®ç¼–è¾‘ ---
    with st.expander("å…¨å±€é…ç½® (Global Settings)", expanded=False):
        global_settings = vocab_config.get("global_settings", {})
        st.markdown("**è§„åˆ™å¾®è°ƒ (Overrides)**")
        overrides = global_settings.get("overrides", {})
        pain_neg = overrides.get("pain_points_negatives", [])
        pain_neg_text = ", ".join(pain_neg)
        new_pain_neg_text = st.text_area("ç—›ç‚¹æ¨¡å—æ’é™¤è¯", value=pain_neg_text, key="pain_neg")
        if new_pain_neg_text != pain_neg_text:
            if "overrides" not in vocab_config["global_settings"]: vocab_config["global_settings"]["overrides"] = {}
            vocab_config["global_settings"]["overrides"]["pain_points_negatives"] = [w.strip() for w in new_pain_neg_text.split(",") if w.strip()]
        promo_neg = overrides.get("promotions_negatives", [])
        promo_neg_text = ", ".join(promo_neg)
        new_promo_neg_text = st.text_area("ä¿ƒé”€æœºåˆ¶æ¨¡å—æ’é™¤è¯", value=promo_neg_text, key="promo_neg")
        if new_promo_neg_text != promo_neg_text:
            if "overrides" not in vocab_config["global_settings"]: vocab_config["global_settings"]["overrides"] = {}
            vocab_config["global_settings"]["overrides"]["promotions_negatives"] = [w.strip() for w in new_promo_neg_text.split(",") if w.strip()]

    # --- ä¿å­˜æŒ‰é’® ---
    save_col = st.empty()
    if save_col.button("ğŸ’¾ ä¿å­˜ä¸šåŠ¡è“å›¾", type="primary"):
        try:
            with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(vocab_config, f, allow_unicode=True, sort_keys=False, indent=2)
            st.session_state['save_success'] = True
        except Exception as e:
            st.error(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    if st.session_state.get('save_success'):
        st.success("âœ… ä¿å­˜æˆåŠŸï¼", icon="âœ…")
        time.sleep(2)
        st.session_state['save_success'] = False

    st.markdown("---")
    st.info('ğŸ’¡ **æ‰€æœ‰ç¼–è¾‘æ“ä½œéƒ½åªåœ¨é¡µé¢å†…ç”Ÿæ•ˆï¼Œç‚¹å‡»"ä¿å­˜ä¸šåŠ¡è“å›¾"åæ‰ä¼šå†™å…¥é…ç½®æ–‡ä»¶ã€‚**')

    # --- å…¨å±€æ’é™¤å…³é”®è¯ç¼–è¾‘ ---
    st.markdown("---")
    with st.expander("ğŸš« å…¨å±€æ’é™¤å…³é”®è¯ (Global Exclusion)", expanded=True):
        st.markdown("##### åœ¨æ­¤å®šä¹‰çš„å…³é”®è¯å°†ä»æ‰€æœ‰æ¨¡å—ä¸­æ’é™¤ï¼Œç”¨äºè¿‡æ»¤é€šç”¨æ— å…³åœºæ™¯ï¼ˆå¦‚è·¯ç‰Œã€äº¤é€šç¯ç­‰ï¼‰")
        
        # ä» vocab_config (st.session_state) ä¸­è·å–æˆ–åˆå§‹åŒ–
        global_settings = vocab_config.get("global_settings", {})
        if "global_exclusion_keywords" not in global_settings:
            global_settings["global_exclusion_keywords"] = []
        
        exclusion_keywords = global_settings["global_exclusion_keywords"]

        # ä½¿ç”¨å¾ªç¯å’Œ key æ¥åŠ¨æ€åˆ›å»ºå’Œç®¡ç†è¾“å…¥æ¡†
        for i in range(len(exclusion_keywords)):
            col_word, col_del = st.columns([4, 1])
            with col_word:
                new_keyword = st.text_input(f"å…¨å±€æ’é™¤è¯ {i+1}", value=exclusion_keywords[i], key=f"global_exclude_{i}")
                if new_keyword != exclusion_keywords[i]:
                    exclusion_keywords[i] = new_keyword
            with col_del:
                if st.button("åˆ é™¤", key=f"global_exclude_del_{i}"):
                    exclusion_keywords.pop(i)
                    st.rerun()

        if st.button("â• æ·»åŠ å…¨å±€æ’é™¤è¯", key="global_exclude_add"):
            exclusion_keywords.append("")
            st.rerun()
            
        # å°†ä¿®æ”¹å†™å› vocab_config
        vocab_config["global_settings"] = global_settings
        
        if exclusion_keywords:
            valid_exclusions = [kw for kw in exclusion_keywords if kw.strip()]
            if valid_exclusions:
                st.success(f"âœ… å·²é…ç½® {len(valid_exclusions)} ä¸ªå…¨å±€æ’é™¤å…³é”®è¯ã€‚ä¿å­˜åï¼ŒåŒ…å«è¿™äº›è¯çš„ç‰‡æ®µå°†ä¸ä¼šè¢«é€‰ç”¨ã€‚")

    # --- é¢„è§ˆAI Prompt ---
    st.markdown("---")
    st.subheader("ğŸ‘ï¸ AI Prompt é¢„è§ˆ")
    
    if st.button("ğŸ” é¢„è§ˆå½“å‰é…ç½®ç”Ÿæˆçš„AI Prompt"):
        vocab = config_manager.get_ai_vocabulary()
        st.json({k: list(v) for k, v in vocab.items()})

        # Qwenè§†è§‰åˆ†æPrompt
        from modules.ai_analyzers.qwen_video_analyzer import QwenVideoAnalyzer
        analyzer = QwenVideoAnalyzer()
        prompt = analyzer._get_fallback_visual_prompt() # è°ƒç”¨é‡æ„åçš„æ–¹æ³•
        st.text_area("Qwenè§†è§‰åˆ†æPrompté¢„è§ˆ", prompt, height=600)

        # DeepSeekéŸ³é¢‘åˆ†æPromptï¼ˆæ ‡ç­¾ç”Ÿæˆå…œåº•ï¼‰
        from utils.keyword_config import get_deepseek_audio_prompt_for_labeling, get_deepseek_audio_prompt_for_mapping
        deepseek_labeling_prompt = get_deepseek_audio_prompt_for_labeling()
        st.text_area("DeepSeekéŸ³é¢‘åˆ†æPromptï¼ˆæ ‡ç­¾ç”Ÿæˆå…œåº•ï¼‰", deepseek_labeling_prompt, height=300)

        # DeepSeekéŸ³é¢‘åˆ†æPromptï¼ˆä¸šåŠ¡å½’ç±»å…œåº•ï¼‰
        deepseek_mapping_prompt = get_deepseek_audio_prompt_for_mapping()
        st.text_area("DeepSeekéŸ³é¢‘åˆ†æPromptï¼ˆä¸šåŠ¡å½’ç±»å…œåº•ï¼‰", deepseek_mapping_prompt, height=600)

if __name__ == "__main__":
    main() 